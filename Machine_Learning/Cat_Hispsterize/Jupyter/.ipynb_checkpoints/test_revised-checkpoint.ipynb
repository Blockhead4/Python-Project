{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras, sys, cv2, os\n",
    "from keras.models import Model, load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import atan2, degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0811 18:06:49.616598  4604 deprecation_wrapper.py:119] From C:\\Users\\Jwp\\Anaconda3\\envs\\py_ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0811 18:06:49.666078  4604 deprecation_wrapper.py:119] From C:\\Users\\Jwp\\Anaconda3\\envs\\py_ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0811 18:06:49.689037  4604 deprecation_wrapper.py:119] From C:\\Users\\Jwp\\Anaconda3\\envs\\py_ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0811 18:06:49.690043  4604 deprecation_wrapper.py:119] From C:\\Users\\Jwp\\Anaconda3\\envs\\py_ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0811 18:06:49.691033  4604 deprecation_wrapper.py:119] From C:\\Users\\Jwp\\Anaconda3\\envs\\py_ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0811 18:06:49.731182  4604 deprecation_wrapper.py:119] From C:\\Users\\Jwp\\Anaconda3\\envs\\py_ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0811 18:07:02.862896  4604 deprecation_wrapper.py:119] From C:\\Users\\Jwp\\Anaconda3\\envs\\py_ml\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img_size = 224\n",
    "base_path = 'samples'\n",
    "file_list = sorted(os.listdir(base_path))\n",
    "\n",
    "# this is most important thing\n",
    "# glasses = cv2.imread('images/2.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "model_path = 'models'\n",
    "bbs_model_name = os.path.join(model_path, 'bbs_1.h5')\n",
    "lmks_model_name = os.path.join(model_path, 'lmks_1.h5')\n",
    "bbs_model = load_model(bbs_model_name)\n",
    "lmks_model = load_model(lmks_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(im):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(img_size) / max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = img_size - new_size[1]\n",
    "    delta_h = img_size - new_size[0]\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "        value=[0, 0, 0])\n",
    "    return new_im, ratio, top, left\n",
    "\n",
    "# overlay function\n",
    "def overlay_transparent(background_img, img_to_overlay_t, x, y, overlay_size=None):\n",
    "    bg_img = background_img.copy()\n",
    "    # convert 3 channels to 4 channels\n",
    "    if bg_img.shape[2] == 3:\n",
    "        bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "    if overlay_size is not None:\n",
    "        img_to_overlay_t = cv2.resize(img_to_overlay_t.copy(), overlay_size)\n",
    "\n",
    "    b, g, r, a = cv2.split(img_to_overlay_t)\n",
    "\n",
    "    mask = cv2.medianBlur(a, 5)\n",
    "\n",
    "    h, w, _ = img_to_overlay_t.shape\n",
    "    roi = bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)]\n",
    "\n",
    "    img1_bg = cv2.bitwise_and(roi.copy(), roi.copy(), mask=cv2.bitwise_not(mask))\n",
    "    img2_fg = cv2.bitwise_and(img_to_overlay_t, img_to_overlay_t, mask=mask)\n",
    "\n",
    "    bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)] = cv2.add(img1_bg, img2_fg)\n",
    "\n",
    "    # convert 4 channels to 4 channels\n",
    "    bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "    return bg_img\n",
    "\n",
    "def angle_between(p1, p2):\n",
    "    xDiff = p2[0] - p1[0]\n",
    "    yDiff = p2[1] - p1[1]\n",
    "    return degrees(atan2(yDiff, xDiff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "for f in file_list:\n",
    "    if '.jpg' not in f:\n",
    "        continue\n",
    "\n",
    "    img = cv2.imread(os.path.join(base_path, f))\n",
    "    ori_img = img.copy()\n",
    "    result_img = img.copy()\n",
    "\n",
    "    # predict bounding box\n",
    "    img, ratio, top, left = resize_img(img)\n",
    "\n",
    "    inputs = (img.astype('float32') / 255).reshape((1, img_size, img_size, 3))\n",
    "    pred_bb = bbs_model.predict(inputs)[0].reshape((-1, 2))\n",
    "\n",
    "    # compute bounding box of original image\n",
    "    ori_bb = ((pred_bb - np.array([left, top])) / ratio).astype(np.int)\n",
    "\n",
    "    # compute lazy bounding box for detecting landmarks\n",
    "    center = np.mean(ori_bb, axis=0)\n",
    "    face_size = max(np.abs(ori_bb[1] - ori_bb[0]))\n",
    "    new_bb = np.array([\n",
    "        center - face_size * 0.6,\n",
    "        center + face_size * 0.6\n",
    "    ]).astype(np.int)\n",
    "    new_bb = np.clip(new_bb, 0, 99999)\n",
    "\n",
    "    # predict landmarks\n",
    "    face_img = ori_img[new_bb[0][1]:new_bb[1][1], new_bb[0][0]:new_bb[1][0]]\n",
    "    face_img, face_ratio, face_top, face_left = resize_img(face_img)\n",
    "\n",
    "    face_inputs = (face_img.astype('float32') / 255).reshape((1, img_size, img_size, 3))\n",
    "\n",
    "    pred_lmks = lmks_model.predict(face_inputs)[0].reshape((-1, 2))\n",
    "\n",
    "    # compute landmark of original image\n",
    "    new_lmks = ((pred_lmks - np.array([face_left, face_top])) / face_ratio).astype(np.int)\n",
    "    ori_lmks = new_lmks + new_bb[0]\n",
    "\n",
    "    # visualize\n",
    "    cv2.rectangle(ori_img, pt1=tuple(ori_bb[0]), pt2=tuple(ori_bb[1]), color=(255, 255, 255), thickness=2)\n",
    "\n",
    "    for i, l in enumerate(ori_lmks):\n",
    "        cv2.putText(ori_img, str(i), tuple(l), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.circle(ori_img, center=tuple(l), radius=1, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "    # wearing glasses\n",
    "    glasses_center = np.mean([ori_lmks[0], ori_lmks[1]], axis=0)\n",
    "    glasses_size = np.linalg.norm(ori_lmks[0] - ori_lmks[1]) * 2\n",
    "    \n",
    "    angle = -angle_between(ori_lmks[0], ori_lmks[1])\n",
    "    M = cv2.getRotationMatrix2D((glasses.shape[1] / 2, glasses.shape[0] / 2), angle, 1)\n",
    "    rotated_glasses = cv2.warpAffine(glasses, M, (glasses.shape[1], glasses.shape[0]))\n",
    "\n",
    "    # try:\n",
    "    result_img = overlay_transparent(result_img, rotated_glasses, glasses_center[0], glasses_center[1], overlay_size=(int(glasses_size), int(glasses.shape[0] * glasses_size / glasses.shape[1])))\n",
    "    # except:\n",
    "    #     print('failed overlay image')\n",
    "\n",
    "    cv2.imshow('img', ori_img)\n",
    "    cv2.imshow('result', result_img)\n",
    "    filename, ext = os.path.splitext(f)\n",
    "    cv2.imwrite('result/%s_lmks%s' % (filename, ext), ori_img)\n",
    "    cv2.imwrite('result/%s_result%s_2' % (filename, ext), result_img)\n",
    "\n",
    "    if cv2.waitKey(0) == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'002.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = file_list[1]\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join(base_path, f))\n",
    "ori_img = img.copy()\n",
    "result_img = img.copy()\n",
    "\n",
    "# predict bounding box\n",
    "img, ratio, top, left = resize_img(img)\n",
    "\n",
    "inputs = (img.astype('float32') / 255).reshape((1, img_size, img_size, 3))\n",
    "pred_bb = bbs_model.predict(inputs)[0].reshape((-1, 2))\n",
    "\n",
    "# compute bounding box of original image\n",
    "ori_bb = ((pred_bb - np.array([left, top])) / ratio).astype(np.int)\n",
    "\n",
    "# compute lazy bounding box for detecting landmarks\n",
    "center = np.mean(ori_bb, axis=0)\n",
    "face_size = max(np.abs(ori_bb[1] - ori_bb[0]))\n",
    "new_bb = np.array([\n",
    "    center - face_size * 0.6,\n",
    "    center + face_size * 0.6\n",
    "]).astype(np.int)\n",
    "new_bb = np.clip(new_bb, 0, 99999)\n",
    "\n",
    "# predict landmarks\n",
    "face_img = ori_img[new_bb[0][1]:new_bb[1][1], new_bb[0][0]:new_bb[1][0]]\n",
    "face_img, face_ratio, face_top, face_left = resize_img(face_img)\n",
    "\n",
    "face_inputs = (face_img.astype('float32') / 255).reshape((1, img_size, img_size, 3))\n",
    "\n",
    "pred_lmks = lmks_model.predict(face_inputs)[0].reshape((-1, 2))\n",
    "\n",
    "# compute landmark of original image\n",
    "new_lmks = ((pred_lmks - np.array([face_left, face_top])) / face_ratio).astype(np.int)\n",
    "ori_lmks = new_lmks + new_bb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = ori_lmks[0]\n",
    "\n",
    "o0 = np.array([\n",
    "    o - face_size * 0.12,\n",
    "    o + face_size * 0.12\n",
    "]).astype(np.int)\n",
    "o0 = np.clip(o0, 0, 99999)\n",
    "\n",
    "cv2.rectangle(ori_img, pt1=tuple(o0[0]), pt2=tuple(o0[1]), color=(255, 255, 255), thickness=2)\n",
    "\n",
    "cv2.imshow('img', ori_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = ori_lmks[1]\n",
    "\n",
    "o1 = np.array([\n",
    "    o - face_size * 0.12,\n",
    "    o + face_size * 0.12\n",
    "]).astype(np.int)\n",
    "o1 = np.clip(o1, 0, 99999)\n",
    "\n",
    "cv2.rectangle(ori_img, pt1=tuple(o1[0]), pt2=tuple(o1[1]), color=(255, 255, 255), thickness=2)\n",
    "\n",
    "cv2.imshow('img', ori_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = ori_lmks[2]\n",
    "\n",
    "o2 = np.array([\n",
    "    o - face_size * 0.15,\n",
    "    o + face_size * 0.15\n",
    "]).astype(np.int)\n",
    "o2 = np.clip(o2, 0, 99999)\n",
    "\n",
    "cv2.rectangle(ori_img, pt1=tuple(o2[0]), pt2=tuple(o2[1]), color=(255, 255, 255), thickness=2)\n",
    "\n",
    "cv2.imshow('img', ori_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.85"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_size * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[313,   0],\n",
       "       [466, 202]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = ori_lmks[3:6]\n",
    "\n",
    "e1 = np.array([\n",
    "    np.min(e, axis=0) - face_size * 0.05,\n",
    "    np.max(e, axis=0) + face_size * 0.05\n",
    "]).astype(np.int)\n",
    "e1 = np.clip(e1, 0, 99999)\n",
    "\n",
    "cv2.rectangle(ori_img, pt1=tuple(e1[0]), pt2=tuple(e1[1]), color=(255, 255, 255), thickness=2)\n",
    "\n",
    "cv2.imshow('img', ori_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[568, 131],\n",
       "       [697,  37],\n",
       "       [671, 217]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_lmks[6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[549,  18],\n",
       "       [715, 235]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = ori_lmks[6:9]\n",
    "\n",
    "e2 = np.array([\n",
    "    np.min(e, axis=0) - face_size * 0.05,\n",
    "    np.max(e, axis=0) + face_size * 0.05\n",
    "]).astype(np.int)\n",
    "e2 = np.clip(e2, 0, 99999)\n",
    "e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.rectangle(ori_img, pt1=tuple(e2[0]), pt2=tuple(e2[1]), color=(255, 255, 255), thickness=2)\n",
    "\n",
    "cv2.imshow('img', ori_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_ML",
   "language": "python",
   "name": "py_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
