{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import random_normal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, kernel_initializer=random_normal(), input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(1024, kernel_initializer=random_normal()))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones(batch_size)\n",
    "        fake = np.zeros(batch_size)\n",
    "\n",
    "        for epoch in range(epochs+1):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((128, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.552685, acc.: 50.78%] [G loss: 0.525490]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jwp\\Anaconda3\\envs\\py_ml\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.502631, acc.: 55.86%] [G loss: 0.608233]\n",
      "2 [D loss: 0.378939, acc.: 72.66%] [G loss: 0.782753]\n",
      "3 [D loss: 0.314956, acc.: 83.98%] [G loss: 0.929062]\n",
      "4 [D loss: 0.266350, acc.: 93.36%] [G loss: 1.078284]\n",
      "5 [D loss: 0.219549, acc.: 97.27%] [G loss: 1.225582]\n",
      "6 [D loss: 0.193964, acc.: 98.44%] [G loss: 1.366628]\n",
      "7 [D loss: 0.174427, acc.: 98.44%] [G loss: 1.511957]\n",
      "8 [D loss: 0.146550, acc.: 99.61%] [G loss: 1.632651]\n",
      "9 [D loss: 0.143761, acc.: 99.61%] [G loss: 1.704270]\n",
      "10 [D loss: 0.125060, acc.: 99.61%] [G loss: 1.790219]\n",
      "11 [D loss: 0.117557, acc.: 99.61%] [G loss: 1.878601]\n",
      "12 [D loss: 0.104076, acc.: 100.00%] [G loss: 1.948069]\n",
      "13 [D loss: 0.094227, acc.: 100.00%] [G loss: 2.061252]\n",
      "14 [D loss: 0.085229, acc.: 100.00%] [G loss: 2.141508]\n",
      "15 [D loss: 0.070948, acc.: 100.00%] [G loss: 2.227463]\n",
      "16 [D loss: 0.072642, acc.: 100.00%] [G loss: 2.292915]\n",
      "17 [D loss: 0.066963, acc.: 100.00%] [G loss: 2.346740]\n",
      "18 [D loss: 0.062119, acc.: 100.00%] [G loss: 2.418702]\n",
      "19 [D loss: 0.061986, acc.: 100.00%] [G loss: 2.439375]\n",
      "20 [D loss: 0.054520, acc.: 100.00%] [G loss: 2.575773]\n",
      "21 [D loss: 0.044974, acc.: 100.00%] [G loss: 2.580123]\n",
      "22 [D loss: 0.052255, acc.: 100.00%] [G loss: 2.611986]\n",
      "23 [D loss: 0.042490, acc.: 100.00%] [G loss: 2.738525]\n",
      "24 [D loss: 0.042496, acc.: 100.00%] [G loss: 2.806637]\n",
      "25 [D loss: 0.041656, acc.: 100.00%] [G loss: 2.822800]\n",
      "26 [D loss: 0.038986, acc.: 100.00%] [G loss: 2.882761]\n",
      "27 [D loss: 0.039981, acc.: 100.00%] [G loss: 2.948958]\n",
      "28 [D loss: 0.039143, acc.: 100.00%] [G loss: 2.998744]\n",
      "29 [D loss: 0.031793, acc.: 100.00%] [G loss: 3.086278]\n",
      "30 [D loss: 0.031575, acc.: 100.00%] [G loss: 3.102322]\n",
      "31 [D loss: 0.029776, acc.: 100.00%] [G loss: 3.104997]\n",
      "32 [D loss: 0.034818, acc.: 100.00%] [G loss: 3.165338]\n",
      "33 [D loss: 0.029634, acc.: 100.00%] [G loss: 3.233599]\n",
      "34 [D loss: 0.027328, acc.: 100.00%] [G loss: 3.292215]\n",
      "35 [D loss: 0.027834, acc.: 100.00%] [G loss: 3.263720]\n",
      "36 [D loss: 0.025628, acc.: 100.00%] [G loss: 3.363588]\n",
      "37 [D loss: 0.025650, acc.: 100.00%] [G loss: 3.362255]\n",
      "38 [D loss: 0.026864, acc.: 100.00%] [G loss: 3.368966]\n",
      "39 [D loss: 0.026729, acc.: 100.00%] [G loss: 3.441476]\n",
      "40 [D loss: 0.024512, acc.: 100.00%] [G loss: 3.485596]\n",
      "41 [D loss: 0.025447, acc.: 100.00%] [G loss: 3.501424]\n",
      "42 [D loss: 0.025131, acc.: 100.00%] [G loss: 3.549476]\n",
      "43 [D loss: 0.022007, acc.: 100.00%] [G loss: 3.567067]\n",
      "44 [D loss: 0.022091, acc.: 100.00%] [G loss: 3.573424]\n",
      "45 [D loss: 0.025623, acc.: 100.00%] [G loss: 3.684950]\n",
      "46 [D loss: 0.021857, acc.: 100.00%] [G loss: 3.608957]\n",
      "47 [D loss: 0.025237, acc.: 100.00%] [G loss: 3.684994]\n",
      "48 [D loss: 0.021786, acc.: 100.00%] [G loss: 3.723216]\n",
      "49 [D loss: 0.021164, acc.: 100.00%] [G loss: 3.707666]\n",
      "50 [D loss: 0.024446, acc.: 100.00%] [G loss: 3.820087]\n",
      "51 [D loss: 0.023432, acc.: 100.00%] [G loss: 3.776831]\n",
      "52 [D loss: 0.022515, acc.: 100.00%] [G loss: 3.819517]\n",
      "53 [D loss: 0.020191, acc.: 100.00%] [G loss: 3.840447]\n",
      "54 [D loss: 0.022738, acc.: 100.00%] [G loss: 3.826008]\n",
      "55 [D loss: 0.021144, acc.: 100.00%] [G loss: 3.839846]\n",
      "56 [D loss: 0.018133, acc.: 100.00%] [G loss: 3.842606]\n",
      "57 [D loss: 0.022860, acc.: 100.00%] [G loss: 3.939248]\n",
      "58 [D loss: 0.017840, acc.: 100.00%] [G loss: 3.801705]\n",
      "59 [D loss: 0.023577, acc.: 100.00%] [G loss: 3.905439]\n",
      "60 [D loss: 0.020728, acc.: 100.00%] [G loss: 3.885050]\n",
      "61 [D loss: 0.024345, acc.: 100.00%] [G loss: 3.978275]\n",
      "62 [D loss: 0.022363, acc.: 100.00%] [G loss: 3.976507]\n",
      "63 [D loss: 0.025986, acc.: 100.00%] [G loss: 3.953658]\n",
      "64 [D loss: 0.024951, acc.: 100.00%] [G loss: 3.977073]\n",
      "65 [D loss: 0.024680, acc.: 100.00%] [G loss: 3.975621]\n",
      "66 [D loss: 0.020626, acc.: 100.00%] [G loss: 4.038953]\n",
      "67 [D loss: 0.024636, acc.: 100.00%] [G loss: 4.001499]\n",
      "68 [D loss: 0.024505, acc.: 100.00%] [G loss: 4.007498]\n",
      "69 [D loss: 0.026534, acc.: 100.00%] [G loss: 3.987639]\n",
      "70 [D loss: 0.031953, acc.: 100.00%] [G loss: 4.064009]\n",
      "71 [D loss: 0.023658, acc.: 100.00%] [G loss: 4.049451]\n",
      "72 [D loss: 0.029149, acc.: 100.00%] [G loss: 4.174396]\n",
      "73 [D loss: 0.029545, acc.: 100.00%] [G loss: 4.036039]\n",
      "74 [D loss: 0.024009, acc.: 100.00%] [G loss: 4.055513]\n",
      "75 [D loss: 0.029894, acc.: 100.00%] [G loss: 3.977242]\n",
      "76 [D loss: 0.031013, acc.: 100.00%] [G loss: 4.139815]\n",
      "77 [D loss: 0.041320, acc.: 99.61%] [G loss: 3.983381]\n",
      "78 [D loss: 0.044807, acc.: 98.83%] [G loss: 4.125922]\n",
      "79 [D loss: 0.107289, acc.: 96.88%] [G loss: 4.380145]\n",
      "80 [D loss: 0.116937, acc.: 96.88%] [G loss: 3.942258]\n",
      "81 [D loss: 0.039716, acc.: 99.61%] [G loss: 4.236199]\n",
      "82 [D loss: 0.104016, acc.: 96.48%] [G loss: 4.036575]\n",
      "83 [D loss: 0.040409, acc.: 99.61%] [G loss: 4.344503]\n",
      "84 [D loss: 0.310698, acc.: 89.06%] [G loss: 3.820242]\n",
      "85 [D loss: 0.038869, acc.: 100.00%] [G loss: 4.346179]\n",
      "86 [D loss: 0.217815, acc.: 90.23%] [G loss: 3.847967]\n",
      "87 [D loss: 0.065417, acc.: 98.44%] [G loss: 4.382199]\n",
      "88 [D loss: 0.829907, acc.: 71.88%] [G loss: 3.164519]\n",
      "89 [D loss: 0.282144, acc.: 83.98%] [G loss: 3.025894]\n",
      "90 [D loss: 0.119048, acc.: 96.48%] [G loss: 3.381238]\n",
      "91 [D loss: 0.070632, acc.: 98.44%] [G loss: 3.781227]\n",
      "92 [D loss: 0.042791, acc.: 99.61%] [G loss: 3.859728]\n",
      "93 [D loss: 0.041103, acc.: 100.00%] [G loss: 3.675658]\n",
      "94 [D loss: 0.057634, acc.: 98.83%] [G loss: 3.689948]\n",
      "95 [D loss: 0.090606, acc.: 98.44%] [G loss: 3.545334]\n",
      "96 [D loss: 0.089017, acc.: 98.44%] [G loss: 3.248775]\n",
      "97 [D loss: 0.094235, acc.: 98.05%] [G loss: 3.470224]\n",
      "98 [D loss: 0.149780, acc.: 95.70%] [G loss: 3.076037]\n",
      "99 [D loss: 0.099876, acc.: 96.88%] [G loss: 3.331749]\n",
      "100 [D loss: 0.225189, acc.: 90.23%] [G loss: 3.375350]\n",
      "101 [D loss: 0.137898, acc.: 95.70%] [G loss: 3.230637]\n",
      "102 [D loss: 0.160151, acc.: 94.14%] [G loss: 3.335735]\n",
      "103 [D loss: 0.268169, acc.: 89.45%] [G loss: 3.287827]\n",
      "104 [D loss: 0.409639, acc.: 80.47%] [G loss: 2.856162]\n",
      "105 [D loss: 0.087239, acc.: 99.22%] [G loss: 3.497633]\n",
      "106 [D loss: 0.835729, acc.: 71.48%] [G loss: 2.540880]\n",
      "107 [D loss: 0.563081, acc.: 75.39%] [G loss: 3.425045]\n",
      "108 [D loss: 0.121260, acc.: 94.53%] [G loss: 3.310773]\n",
      "109 [D loss: 0.078117, acc.: 98.83%] [G loss: 3.980177]\n",
      "110 [D loss: 0.076669, acc.: 99.61%] [G loss: 3.573610]\n",
      "111 [D loss: 0.129262, acc.: 97.66%] [G loss: 3.350971]\n",
      "112 [D loss: 0.120489, acc.: 96.88%] [G loss: 3.214602]\n",
      "113 [D loss: 0.195288, acc.: 92.19%] [G loss: 3.106970]\n",
      "114 [D loss: 0.310951, acc.: 87.89%] [G loss: 2.783224]\n",
      "115 [D loss: 0.166920, acc.: 95.70%] [G loss: 3.138668]\n",
      "116 [D loss: 0.581606, acc.: 73.44%] [G loss: 2.495089]\n",
      "117 [D loss: 0.156847, acc.: 93.75%] [G loss: 3.092541]\n",
      "118 [D loss: 0.357395, acc.: 85.55%] [G loss: 2.890098]\n",
      "119 [D loss: 0.120032, acc.: 96.88%] [G loss: 2.967704]\n",
      "120 [D loss: 0.264362, acc.: 87.50%] [G loss: 3.159420]\n",
      "121 [D loss: 0.510260, acc.: 74.22%] [G loss: 2.348080]\n",
      "122 [D loss: 0.197264, acc.: 89.06%] [G loss: 2.858091]\n",
      "123 [D loss: 0.111063, acc.: 96.09%] [G loss: 3.189881]\n",
      "124 [D loss: 0.194294, acc.: 92.19%] [G loss: 3.221413]\n",
      "125 [D loss: 0.235307, acc.: 92.58%] [G loss: 2.476200]\n",
      "126 [D loss: 0.142701, acc.: 94.14%] [G loss: 2.893155]\n",
      "127 [D loss: 0.233581, acc.: 90.62%] [G loss: 3.120324]\n",
      "128 [D loss: 0.663002, acc.: 72.66%] [G loss: 2.238460]\n",
      "129 [D loss: 0.202105, acc.: 89.84%] [G loss: 3.077380]\n",
      "130 [D loss: 0.274419, acc.: 86.33%] [G loss: 2.805143]\n",
      "131 [D loss: 0.208960, acc.: 93.75%] [G loss: 2.989506]\n",
      "132 [D loss: 0.331899, acc.: 83.98%] [G loss: 2.792076]\n",
      "133 [D loss: 0.206459, acc.: 94.53%] [G loss: 2.505578]\n",
      "134 [D loss: 0.200823, acc.: 91.80%] [G loss: 2.802914]\n",
      "135 [D loss: 0.303926, acc.: 87.11%] [G loss: 2.914753]\n",
      "136 [D loss: 0.298003, acc.: 86.33%] [G loss: 2.582171]\n",
      "137 [D loss: 0.188929, acc.: 92.97%] [G loss: 2.696388]\n",
      "138 [D loss: 0.238435, acc.: 91.02%] [G loss: 2.596376]\n",
      "139 [D loss: 0.223660, acc.: 90.23%] [G loss: 3.092521]\n",
      "140 [D loss: 0.347894, acc.: 82.42%] [G loss: 2.565871]\n",
      "141 [D loss: 0.163097, acc.: 93.75%] [G loss: 2.929544]\n",
      "142 [D loss: 0.319817, acc.: 85.16%] [G loss: 2.848681]\n",
      "143 [D loss: 0.187742, acc.: 95.70%] [G loss: 2.820129]\n",
      "144 [D loss: 0.249776, acc.: 88.67%] [G loss: 3.052845]\n",
      "145 [D loss: 0.696375, acc.: 66.41%] [G loss: 2.078074]\n",
      "146 [D loss: 0.229434, acc.: 89.06%] [G loss: 3.105752]\n",
      "147 [D loss: 0.205503, acc.: 94.92%] [G loss: 3.051214]\n",
      "148 [D loss: 0.261873, acc.: 90.23%] [G loss: 2.899950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 [D loss: 0.234284, acc.: 92.19%] [G loss: 2.643116]\n",
      "150 [D loss: 0.309858, acc.: 82.81%] [G loss: 3.535574]\n",
      "151 [D loss: 1.477806, acc.: 39.45%] [G loss: 1.179516]\n",
      "152 [D loss: 1.036940, acc.: 60.94%] [G loss: 1.196026]\n",
      "153 [D loss: 0.434293, acc.: 70.31%] [G loss: 1.960806]\n",
      "154 [D loss: 0.135506, acc.: 96.09%] [G loss: 3.063986]\n",
      "155 [D loss: 0.225488, acc.: 96.88%] [G loss: 2.541502]\n",
      "156 [D loss: 0.258233, acc.: 92.19%] [G loss: 2.127388]\n",
      "157 [D loss: 0.219349, acc.: 93.75%] [G loss: 2.380096]\n",
      "158 [D loss: 0.240224, acc.: 90.23%] [G loss: 2.270355]\n",
      "159 [D loss: 0.242799, acc.: 91.80%] [G loss: 2.322944]\n",
      "160 [D loss: 0.264515, acc.: 93.75%] [G loss: 2.216620]\n",
      "161 [D loss: 0.313168, acc.: 85.55%] [G loss: 2.311587]\n",
      "162 [D loss: 0.423127, acc.: 77.34%] [G loss: 2.009403]\n",
      "163 [D loss: 0.305505, acc.: 85.94%] [G loss: 2.451000]\n",
      "164 [D loss: 0.467811, acc.: 74.22%] [G loss: 2.084091]\n",
      "165 [D loss: 0.224669, acc.: 94.14%] [G loss: 2.433859]\n",
      "166 [D loss: 0.432774, acc.: 76.56%] [G loss: 1.997096]\n",
      "167 [D loss: 0.267673, acc.: 86.72%] [G loss: 2.975152]\n",
      "168 [D loss: 0.636894, acc.: 64.06%] [G loss: 1.651305]\n",
      "169 [D loss: 0.257011, acc.: 87.89%] [G loss: 2.666240]\n",
      "170 [D loss: 0.651124, acc.: 65.23%] [G loss: 1.540697]\n",
      "171 [D loss: 0.251756, acc.: 90.23%] [G loss: 2.817080]\n",
      "172 [D loss: 0.622148, acc.: 63.67%] [G loss: 1.584143]\n",
      "173 [D loss: 0.295405, acc.: 84.77%] [G loss: 2.797080]\n",
      "174 [D loss: 0.701414, acc.: 62.11%] [G loss: 1.434386]\n",
      "175 [D loss: 0.305583, acc.: 85.55%] [G loss: 2.433315]\n",
      "176 [D loss: 0.702431, acc.: 56.64%] [G loss: 1.478605]\n",
      "177 [D loss: 0.311338, acc.: 82.81%] [G loss: 2.674908]\n",
      "178 [D loss: 0.841808, acc.: 50.78%] [G loss: 1.242476]\n",
      "179 [D loss: 0.394658, acc.: 73.05%] [G loss: 2.598462]\n",
      "180 [D loss: 0.456686, acc.: 79.30%] [G loss: 1.897739]\n",
      "181 [D loss: 0.458362, acc.: 72.27%] [G loss: 2.008204]\n",
      "182 [D loss: 0.553804, acc.: 68.75%] [G loss: 1.813613]\n",
      "183 [D loss: 0.482282, acc.: 71.48%] [G loss: 2.033013]\n",
      "184 [D loss: 0.707542, acc.: 57.42%] [G loss: 1.376580]\n",
      "185 [D loss: 0.410763, acc.: 77.73%] [G loss: 2.378234]\n",
      "186 [D loss: 0.867997, acc.: 44.14%] [G loss: 0.858910]\n",
      "187 [D loss: 0.483276, acc.: 65.62%] [G loss: 1.991626]\n",
      "188 [D loss: 0.555415, acc.: 67.97%] [G loss: 2.026616]\n",
      "189 [D loss: 0.567968, acc.: 66.80%] [G loss: 1.622183]\n",
      "190 [D loss: 0.575821, acc.: 65.62%] [G loss: 1.642773]\n",
      "191 [D loss: 0.784331, acc.: 44.53%] [G loss: 1.268905]\n",
      "192 [D loss: 0.528830, acc.: 64.45%] [G loss: 1.739900]\n",
      "193 [D loss: 0.878547, acc.: 42.58%] [G loss: 0.989887]\n",
      "194 [D loss: 0.472225, acc.: 68.75%] [G loss: 1.914606]\n",
      "195 [D loss: 0.891366, acc.: 36.33%] [G loss: 0.834920]\n",
      "196 [D loss: 0.563969, acc.: 63.67%] [G loss: 1.390015]\n",
      "197 [D loss: 0.691660, acc.: 53.52%] [G loss: 1.262366]\n",
      "198 [D loss: 0.719364, acc.: 50.39%] [G loss: 1.105275]\n",
      "199 [D loss: 0.642568, acc.: 56.64%] [G loss: 1.291217]\n",
      "200 [D loss: 0.818193, acc.: 42.58%] [G loss: 0.882448]\n",
      "201 [D loss: 0.648823, acc.: 53.12%] [G loss: 1.224240]\n",
      "202 [D loss: 0.718905, acc.: 47.27%] [G loss: 0.957532]\n",
      "203 [D loss: 0.711669, acc.: 45.70%] [G loss: 0.998124]\n",
      "204 [D loss: 0.788901, acc.: 40.23%] [G loss: 0.805967]\n",
      "205 [D loss: 0.694117, acc.: 47.66%] [G loss: 1.012020]\n",
      "206 [D loss: 0.748692, acc.: 39.84%] [G loss: 0.816718]\n",
      "207 [D loss: 0.726655, acc.: 42.97%] [G loss: 0.860031]\n",
      "208 [D loss: 0.790044, acc.: 37.11%] [G loss: 0.746068]\n",
      "209 [D loss: 0.677897, acc.: 49.61%] [G loss: 0.883263]\n",
      "210 [D loss: 0.795510, acc.: 37.50%] [G loss: 0.690320]\n",
      "211 [D loss: 0.700848, acc.: 46.09%] [G loss: 0.769379]\n",
      "212 [D loss: 0.751044, acc.: 39.84%] [G loss: 0.702677]\n",
      "213 [D loss: 0.725299, acc.: 42.19%] [G loss: 0.745255]\n",
      "214 [D loss: 0.743670, acc.: 43.75%] [G loss: 0.706361]\n",
      "215 [D loss: 0.754915, acc.: 41.41%] [G loss: 0.676775]\n",
      "216 [D loss: 0.746765, acc.: 43.75%] [G loss: 0.686630]\n",
      "217 [D loss: 0.722026, acc.: 44.92%] [G loss: 0.687728]\n",
      "218 [D loss: 0.738729, acc.: 42.97%] [G loss: 0.644079]\n",
      "219 [D loss: 0.742716, acc.: 43.36%] [G loss: 0.623692]\n",
      "220 [D loss: 0.718845, acc.: 44.92%] [G loss: 0.665835]\n",
      "221 [D loss: 0.706443, acc.: 45.31%] [G loss: 0.649787]\n",
      "222 [D loss: 0.714928, acc.: 44.92%] [G loss: 0.649270]\n",
      "223 [D loss: 0.705222, acc.: 46.09%] [G loss: 0.639163]\n",
      "224 [D loss: 0.712328, acc.: 46.09%] [G loss: 0.634314]\n",
      "225 [D loss: 0.703044, acc.: 46.48%] [G loss: 0.640220]\n",
      "226 [D loss: 0.710297, acc.: 45.70%] [G loss: 0.612191]\n",
      "227 [D loss: 0.695831, acc.: 48.44%] [G loss: 0.616100]\n",
      "228 [D loss: 0.705289, acc.: 46.09%] [G loss: 0.633057]\n",
      "229 [D loss: 0.695027, acc.: 46.48%] [G loss: 0.629443]\n",
      "230 [D loss: 0.701582, acc.: 44.53%] [G loss: 0.630093]\n",
      "231 [D loss: 0.719938, acc.: 44.14%] [G loss: 0.608606]\n",
      "232 [D loss: 0.718946, acc.: 43.36%] [G loss: 0.584788]\n",
      "233 [D loss: 0.693967, acc.: 47.66%] [G loss: 0.600630]\n",
      "234 [D loss: 0.680982, acc.: 48.83%] [G loss: 0.611010]\n",
      "235 [D loss: 0.699751, acc.: 46.48%] [G loss: 0.604868]\n",
      "236 [D loss: 0.707785, acc.: 46.48%] [G loss: 0.599067]\n",
      "237 [D loss: 0.704693, acc.: 47.27%] [G loss: 0.590008]\n",
      "238 [D loss: 0.677789, acc.: 48.44%] [G loss: 0.605527]\n",
      "239 [D loss: 0.703732, acc.: 48.05%] [G loss: 0.600257]\n",
      "240 [D loss: 0.690630, acc.: 47.66%] [G loss: 0.596970]\n",
      "241 [D loss: 0.693216, acc.: 47.66%] [G loss: 0.599748]\n",
      "242 [D loss: 0.696188, acc.: 47.27%] [G loss: 0.604164]\n",
      "243 [D loss: 0.696218, acc.: 48.05%] [G loss: 0.593917]\n",
      "244 [D loss: 0.678798, acc.: 48.05%] [G loss: 0.597565]\n",
      "245 [D loss: 0.681990, acc.: 48.83%] [G loss: 0.601941]\n",
      "246 [D loss: 0.670145, acc.: 49.61%] [G loss: 0.601739]\n",
      "247 [D loss: 0.680873, acc.: 48.44%] [G loss: 0.616206]\n",
      "248 [D loss: 0.674511, acc.: 49.61%] [G loss: 0.613619]\n",
      "249 [D loss: 0.664033, acc.: 49.61%] [G loss: 0.616309]\n",
      "250 [D loss: 0.676369, acc.: 48.44%] [G loss: 0.626992]\n",
      "251 [D loss: 0.669883, acc.: 48.05%] [G loss: 0.621846]\n",
      "252 [D loss: 0.680984, acc.: 48.83%] [G loss: 0.614177]\n",
      "253 [D loss: 0.677202, acc.: 48.83%] [G loss: 0.617083]\n",
      "254 [D loss: 0.677898, acc.: 48.83%] [G loss: 0.610612]\n",
      "255 [D loss: 0.666071, acc.: 49.22%] [G loss: 0.621630]\n",
      "256 [D loss: 0.661856, acc.: 48.83%] [G loss: 0.621537]\n",
      "257 [D loss: 0.662486, acc.: 49.22%] [G loss: 0.636128]\n",
      "258 [D loss: 0.677706, acc.: 48.44%] [G loss: 0.633139]\n",
      "259 [D loss: 0.671986, acc.: 47.27%] [G loss: 0.617240]\n",
      "260 [D loss: 0.657767, acc.: 49.61%] [G loss: 0.617600]\n",
      "261 [D loss: 0.674503, acc.: 48.83%] [G loss: 0.616699]\n",
      "262 [D loss: 0.666798, acc.: 49.61%] [G loss: 0.622965]\n",
      "263 [D loss: 0.664209, acc.: 50.00%] [G loss: 0.621022]\n",
      "264 [D loss: 0.672006, acc.: 48.44%] [G loss: 0.619479]\n",
      "265 [D loss: 0.660866, acc.: 49.22%] [G loss: 0.607292]\n",
      "266 [D loss: 0.657718, acc.: 49.22%] [G loss: 0.610173]\n",
      "267 [D loss: 0.668377, acc.: 49.61%] [G loss: 0.621155]\n",
      "268 [D loss: 0.658516, acc.: 48.83%] [G loss: 0.620810]\n",
      "269 [D loss: 0.668850, acc.: 48.44%] [G loss: 0.631001]\n",
      "270 [D loss: 0.667832, acc.: 48.05%] [G loss: 0.628899]\n",
      "271 [D loss: 0.653625, acc.: 48.83%] [G loss: 0.637956]\n",
      "272 [D loss: 0.656407, acc.: 48.44%] [G loss: 0.634674]\n",
      "273 [D loss: 0.662689, acc.: 48.44%] [G loss: 0.630450]\n",
      "274 [D loss: 0.666998, acc.: 49.61%] [G loss: 0.625205]\n",
      "275 [D loss: 0.646565, acc.: 49.22%] [G loss: 0.637197]\n",
      "276 [D loss: 0.661600, acc.: 47.66%] [G loss: 0.633211]\n",
      "277 [D loss: 0.662227, acc.: 48.44%] [G loss: 0.634191]\n",
      "278 [D loss: 0.658098, acc.: 48.83%] [G loss: 0.640017]\n",
      "279 [D loss: 0.658181, acc.: 48.83%] [G loss: 0.636301]\n",
      "280 [D loss: 0.667147, acc.: 46.88%] [G loss: 0.634843]\n",
      "281 [D loss: 0.670207, acc.: 48.05%] [G loss: 0.631963]\n",
      "282 [D loss: 0.668350, acc.: 47.27%] [G loss: 0.627876]\n",
      "283 [D loss: 0.654602, acc.: 50.00%] [G loss: 0.627482]\n",
      "284 [D loss: 0.660033, acc.: 50.00%] [G loss: 0.631716]\n",
      "285 [D loss: 0.654265, acc.: 49.61%] [G loss: 0.632086]\n",
      "286 [D loss: 0.642034, acc.: 50.00%] [G loss: 0.630161]\n",
      "287 [D loss: 0.659314, acc.: 50.78%] [G loss: 0.641658]\n",
      "288 [D loss: 0.651884, acc.: 50.39%] [G loss: 0.642579]\n",
      "289 [D loss: 0.662973, acc.: 50.39%] [G loss: 0.651064]\n",
      "290 [D loss: 0.648979, acc.: 49.22%] [G loss: 0.658357]\n",
      "291 [D loss: 0.657793, acc.: 48.44%] [G loss: 0.651037]\n",
      "292 [D loss: 0.646140, acc.: 50.00%] [G loss: 0.646951]\n",
      "293 [D loss: 0.655814, acc.: 48.05%] [G loss: 0.635157]\n",
      "294 [D loss: 0.664845, acc.: 47.66%] [G loss: 0.635141]\n",
      "295 [D loss: 0.652945, acc.: 47.66%] [G loss: 0.635155]\n",
      "296 [D loss: 0.662877, acc.: 46.88%] [G loss: 0.630516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 [D loss: 0.662853, acc.: 48.83%] [G loss: 0.628776]\n",
      "298 [D loss: 0.664642, acc.: 49.61%] [G loss: 0.633471]\n",
      "299 [D loss: 0.661709, acc.: 47.27%] [G loss: 0.633953]\n",
      "300 [D loss: 0.668486, acc.: 48.05%] [G loss: 0.637163]\n",
      "301 [D loss: 0.662295, acc.: 48.05%] [G loss: 0.637544]\n",
      "302 [D loss: 0.653224, acc.: 50.39%] [G loss: 0.635807]\n",
      "303 [D loss: 0.662181, acc.: 47.66%] [G loss: 0.631311]\n",
      "304 [D loss: 0.651313, acc.: 49.61%] [G loss: 0.634278]\n",
      "305 [D loss: 0.661334, acc.: 48.05%] [G loss: 0.630751]\n",
      "306 [D loss: 0.659641, acc.: 50.00%] [G loss: 0.632068]\n",
      "307 [D loss: 0.652520, acc.: 49.22%] [G loss: 0.634714]\n",
      "308 [D loss: 0.652189, acc.: 48.83%] [G loss: 0.633669]\n",
      "309 [D loss: 0.658552, acc.: 49.61%] [G loss: 0.636019]\n",
      "310 [D loss: 0.656122, acc.: 49.61%] [G loss: 0.641348]\n",
      "311 [D loss: 0.653933, acc.: 49.22%] [G loss: 0.643180]\n",
      "312 [D loss: 0.656769, acc.: 48.83%] [G loss: 0.641327]\n",
      "313 [D loss: 0.652302, acc.: 49.61%] [G loss: 0.645945]\n",
      "314 [D loss: 0.654866, acc.: 48.44%] [G loss: 0.652124]\n",
      "315 [D loss: 0.656573, acc.: 48.44%] [G loss: 0.650969]\n",
      "316 [D loss: 0.656248, acc.: 49.22%] [G loss: 0.648467]\n",
      "317 [D loss: 0.656365, acc.: 47.66%] [G loss: 0.646387]\n",
      "318 [D loss: 0.658301, acc.: 48.44%] [G loss: 0.644384]\n",
      "319 [D loss: 0.660398, acc.: 50.00%] [G loss: 0.643365]\n",
      "320 [D loss: 0.656918, acc.: 49.61%] [G loss: 0.645322]\n",
      "321 [D loss: 0.656960, acc.: 47.66%] [G loss: 0.646326]\n",
      "322 [D loss: 0.654662, acc.: 49.61%] [G loss: 0.646484]\n",
      "323 [D loss: 0.654649, acc.: 48.83%] [G loss: 0.648121]\n",
      "324 [D loss: 0.655094, acc.: 49.22%] [G loss: 0.652595]\n",
      "325 [D loss: 0.658649, acc.: 48.44%] [G loss: 0.667341]\n",
      "326 [D loss: 0.662993, acc.: 48.44%] [G loss: 0.662028]\n",
      "327 [D loss: 0.659119, acc.: 49.22%] [G loss: 0.654929]\n",
      "328 [D loss: 0.659301, acc.: 49.61%] [G loss: 0.645085]\n",
      "329 [D loss: 0.654528, acc.: 50.00%] [G loss: 0.645836]\n",
      "330 [D loss: 0.652183, acc.: 48.83%] [G loss: 0.642831]\n",
      "331 [D loss: 0.647628, acc.: 50.00%] [G loss: 0.647916]\n",
      "332 [D loss: 0.649694, acc.: 49.22%] [G loss: 0.657220]\n",
      "333 [D loss: 0.658635, acc.: 50.00%] [G loss: 0.657434]\n",
      "334 [D loss: 0.665839, acc.: 48.83%] [G loss: 0.676623]\n",
      "335 [D loss: 0.664696, acc.: 48.83%] [G loss: 0.675672]\n",
      "336 [D loss: 0.657779, acc.: 48.83%] [G loss: 0.676232]\n",
      "337 [D loss: 0.657026, acc.: 48.44%] [G loss: 0.665522]\n",
      "338 [D loss: 0.662001, acc.: 49.22%] [G loss: 0.660046]\n",
      "339 [D loss: 0.662685, acc.: 47.66%] [G loss: 0.654462]\n",
      "340 [D loss: 0.662669, acc.: 48.83%] [G loss: 0.654812]\n",
      "341 [D loss: 0.664982, acc.: 50.00%] [G loss: 0.651199]\n",
      "342 [D loss: 0.649764, acc.: 49.61%] [G loss: 0.650930]\n",
      "343 [D loss: 0.659987, acc.: 48.83%] [G loss: 0.656731]\n",
      "344 [D loss: 0.660591, acc.: 47.27%] [G loss: 0.651250]\n",
      "345 [D loss: 0.669822, acc.: 47.66%] [G loss: 0.654660]\n",
      "346 [D loss: 0.647661, acc.: 50.00%] [G loss: 0.665656]\n",
      "347 [D loss: 0.644122, acc.: 48.83%] [G loss: 0.655790]\n",
      "348 [D loss: 0.653394, acc.: 48.83%] [G loss: 0.660895]\n",
      "349 [D loss: 0.657326, acc.: 49.22%] [G loss: 0.661979]\n",
      "350 [D loss: 0.648643, acc.: 50.00%] [G loss: 0.662935]\n",
      "351 [D loss: 0.658014, acc.: 50.39%] [G loss: 0.671974]\n",
      "352 [D loss: 0.659023, acc.: 49.61%] [G loss: 0.674505]\n",
      "353 [D loss: 0.651061, acc.: 49.61%] [G loss: 0.671220]\n",
      "354 [D loss: 0.661305, acc.: 48.83%] [G loss: 0.667655]\n",
      "355 [D loss: 0.651735, acc.: 49.22%] [G loss: 0.658254]\n",
      "356 [D loss: 0.651559, acc.: 49.22%] [G loss: 0.663108]\n",
      "357 [D loss: 0.651305, acc.: 49.22%] [G loss: 0.664694]\n",
      "358 [D loss: 0.648749, acc.: 49.22%] [G loss: 0.664046]\n",
      "359 [D loss: 0.642975, acc.: 50.00%] [G loss: 0.653417]\n",
      "360 [D loss: 0.638335, acc.: 49.61%] [G loss: 0.656427]\n",
      "361 [D loss: 0.643118, acc.: 50.39%] [G loss: 0.664653]\n",
      "362 [D loss: 0.636971, acc.: 49.61%] [G loss: 0.665042]\n",
      "363 [D loss: 0.639242, acc.: 50.39%] [G loss: 0.669387]\n",
      "364 [D loss: 0.637201, acc.: 50.39%] [G loss: 0.664404]\n",
      "365 [D loss: 0.645335, acc.: 49.22%] [G loss: 0.658833]\n",
      "366 [D loss: 0.635666, acc.: 50.00%] [G loss: 0.664476]\n",
      "367 [D loss: 0.642724, acc.: 50.00%] [G loss: 0.657794]\n",
      "368 [D loss: 0.645575, acc.: 48.05%] [G loss: 0.655884]\n",
      "369 [D loss: 0.637930, acc.: 51.17%] [G loss: 0.657899]\n",
      "370 [D loss: 0.650163, acc.: 50.39%] [G loss: 0.672094]\n",
      "371 [D loss: 0.646499, acc.: 52.34%] [G loss: 0.678954]\n",
      "372 [D loss: 0.644552, acc.: 53.12%] [G loss: 0.678524]\n",
      "373 [D loss: 0.642159, acc.: 52.34%] [G loss: 0.673937]\n",
      "374 [D loss: 0.634704, acc.: 52.34%] [G loss: 0.665732]\n",
      "375 [D loss: 0.634895, acc.: 52.73%] [G loss: 0.651354]\n",
      "376 [D loss: 0.645936, acc.: 51.56%] [G loss: 0.657417]\n",
      "377 [D loss: 0.631507, acc.: 53.52%] [G loss: 0.657299]\n",
      "378 [D loss: 0.634335, acc.: 56.25%] [G loss: 0.661681]\n",
      "379 [D loss: 0.635866, acc.: 55.47%] [G loss: 0.673033]\n",
      "380 [D loss: 0.633387, acc.: 55.86%] [G loss: 0.685971]\n",
      "381 [D loss: 0.632219, acc.: 56.25%] [G loss: 0.701938]\n",
      "382 [D loss: 0.630277, acc.: 56.64%] [G loss: 0.698482]\n",
      "383 [D loss: 0.635608, acc.: 54.69%] [G loss: 0.685242]\n",
      "384 [D loss: 0.630909, acc.: 53.12%] [G loss: 0.676005]\n",
      "385 [D loss: 0.627553, acc.: 53.52%] [G loss: 0.668424]\n",
      "386 [D loss: 0.626330, acc.: 57.42%] [G loss: 0.672295]\n",
      "387 [D loss: 0.624348, acc.: 54.69%] [G loss: 0.670881]\n",
      "388 [D loss: 0.629182, acc.: 54.69%] [G loss: 0.672921]\n",
      "389 [D loss: 0.626319, acc.: 55.47%] [G loss: 0.674924]\n",
      "390 [D loss: 0.627197, acc.: 54.69%] [G loss: 0.681201]\n",
      "391 [D loss: 0.629536, acc.: 55.86%] [G loss: 0.682533]\n",
      "392 [D loss: 0.634888, acc.: 53.91%] [G loss: 0.676411]\n",
      "393 [D loss: 0.630348, acc.: 54.69%] [G loss: 0.675675]\n",
      "394 [D loss: 0.634620, acc.: 52.73%] [G loss: 0.678489]\n",
      "395 [D loss: 0.632969, acc.: 52.34%] [G loss: 0.679262]\n",
      "396 [D loss: 0.644268, acc.: 51.56%] [G loss: 0.675722]\n",
      "397 [D loss: 0.632527, acc.: 52.34%] [G loss: 0.683014]\n",
      "398 [D loss: 0.633502, acc.: 55.08%] [G loss: 0.678329]\n",
      "399 [D loss: 0.641118, acc.: 53.52%] [G loss: 0.688134]\n",
      "400 [D loss: 0.637699, acc.: 53.12%] [G loss: 0.682308]\n",
      "401 [D loss: 0.637226, acc.: 53.52%] [G loss: 0.675566]\n",
      "402 [D loss: 0.639038, acc.: 57.03%] [G loss: 0.679949]\n",
      "403 [D loss: 0.636664, acc.: 58.59%] [G loss: 0.683518]\n",
      "404 [D loss: 0.644257, acc.: 53.91%] [G loss: 0.685232]\n",
      "405 [D loss: 0.645505, acc.: 52.73%] [G loss: 0.682428]\n",
      "406 [D loss: 0.638352, acc.: 54.30%] [G loss: 0.684159]\n",
      "407 [D loss: 0.645936, acc.: 52.34%] [G loss: 0.681690]\n",
      "408 [D loss: 0.646499, acc.: 51.56%] [G loss: 0.679191]\n",
      "409 [D loss: 0.645624, acc.: 51.56%] [G loss: 0.678257]\n",
      "410 [D loss: 0.644577, acc.: 52.73%] [G loss: 0.674262]\n",
      "411 [D loss: 0.654876, acc.: 53.12%] [G loss: 0.689209]\n",
      "412 [D loss: 0.640322, acc.: 53.52%] [G loss: 0.688769]\n",
      "413 [D loss: 0.642853, acc.: 53.52%] [G loss: 0.687335]\n",
      "414 [D loss: 0.648363, acc.: 53.52%] [G loss: 0.696620]\n",
      "415 [D loss: 0.640495, acc.: 54.69%] [G loss: 0.703321]\n",
      "416 [D loss: 0.649517, acc.: 51.56%] [G loss: 0.702857]\n",
      "417 [D loss: 0.642781, acc.: 53.12%] [G loss: 0.702904]\n",
      "418 [D loss: 0.642859, acc.: 54.30%] [G loss: 0.700078]\n",
      "419 [D loss: 0.647732, acc.: 51.56%] [G loss: 0.695622]\n",
      "420 [D loss: 0.646938, acc.: 55.86%] [G loss: 0.688366]\n",
      "421 [D loss: 0.642285, acc.: 54.30%] [G loss: 0.689683]\n",
      "422 [D loss: 0.634945, acc.: 55.86%] [G loss: 0.689422]\n",
      "423 [D loss: 0.648407, acc.: 53.52%] [G loss: 0.689095]\n",
      "424 [D loss: 0.637027, acc.: 56.25%] [G loss: 0.687563]\n",
      "425 [D loss: 0.632718, acc.: 56.64%] [G loss: 0.689633]\n",
      "426 [D loss: 0.628573, acc.: 57.03%] [G loss: 0.696273]\n",
      "427 [D loss: 0.635458, acc.: 57.81%] [G loss: 0.697733]\n",
      "428 [D loss: 0.644595, acc.: 53.52%] [G loss: 0.701637]\n",
      "429 [D loss: 0.648867, acc.: 53.12%] [G loss: 0.704309]\n",
      "430 [D loss: 0.636169, acc.: 60.16%] [G loss: 0.707754]\n",
      "431 [D loss: 0.629917, acc.: 61.72%] [G loss: 0.706695]\n",
      "432 [D loss: 0.643633, acc.: 57.42%] [G loss: 0.712033]\n",
      "433 [D loss: 0.639409, acc.: 57.81%] [G loss: 0.708147]\n",
      "434 [D loss: 0.635580, acc.: 56.64%] [G loss: 0.709043]\n",
      "435 [D loss: 0.631256, acc.: 55.47%] [G loss: 0.704812]\n",
      "436 [D loss: 0.626448, acc.: 57.81%] [G loss: 0.710177]\n",
      "437 [D loss: 0.625971, acc.: 58.20%] [G loss: 0.708421]\n",
      "438 [D loss: 0.628023, acc.: 60.94%] [G loss: 0.705125]\n",
      "439 [D loss: 0.623924, acc.: 61.33%] [G loss: 0.714645]\n",
      "440 [D loss: 0.630341, acc.: 61.72%] [G loss: 0.709334]\n",
      "441 [D loss: 0.624368, acc.: 61.72%] [G loss: 0.728127]\n",
      "442 [D loss: 0.624184, acc.: 61.72%] [G loss: 0.735413]\n",
      "443 [D loss: 0.632013, acc.: 60.55%] [G loss: 0.726719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 [D loss: 0.639532, acc.: 58.20%] [G loss: 0.743047]\n",
      "445 [D loss: 0.619885, acc.: 60.55%] [G loss: 0.747721]\n",
      "446 [D loss: 0.632983, acc.: 61.33%] [G loss: 0.728128]\n",
      "447 [D loss: 0.627236, acc.: 59.38%] [G loss: 0.718624]\n",
      "448 [D loss: 0.637961, acc.: 55.86%] [G loss: 0.726679]\n",
      "449 [D loss: 0.625116, acc.: 60.16%] [G loss: 0.735084]\n",
      "450 [D loss: 0.628262, acc.: 61.33%] [G loss: 0.727777]\n",
      "451 [D loss: 0.623910, acc.: 62.11%] [G loss: 0.724525]\n",
      "452 [D loss: 0.610749, acc.: 66.41%] [G loss: 0.735028]\n",
      "453 [D loss: 0.618964, acc.: 68.36%] [G loss: 0.725430]\n",
      "454 [D loss: 0.616538, acc.: 69.14%] [G loss: 0.722293]\n",
      "455 [D loss: 0.627449, acc.: 63.67%] [G loss: 0.721621]\n",
      "456 [D loss: 0.624302, acc.: 67.58%] [G loss: 0.727432]\n",
      "457 [D loss: 0.623203, acc.: 65.62%] [G loss: 0.735722]\n",
      "458 [D loss: 0.621538, acc.: 63.67%] [G loss: 0.732979]\n",
      "459 [D loss: 0.623751, acc.: 64.06%] [G loss: 0.741414]\n",
      "460 [D loss: 0.632096, acc.: 61.72%] [G loss: 0.754096]\n",
      "461 [D loss: 0.629811, acc.: 60.16%] [G loss: 0.746900]\n",
      "462 [D loss: 0.627797, acc.: 60.94%] [G loss: 0.739156]\n",
      "463 [D loss: 0.613381, acc.: 64.45%] [G loss: 0.731910]\n",
      "464 [D loss: 0.622763, acc.: 62.89%] [G loss: 0.732365]\n",
      "465 [D loss: 0.618355, acc.: 60.94%] [G loss: 0.733252]\n",
      "466 [D loss: 0.612515, acc.: 60.55%] [G loss: 0.736186]\n",
      "467 [D loss: 0.609531, acc.: 62.11%] [G loss: 0.742169]\n",
      "468 [D loss: 0.615073, acc.: 62.89%] [G loss: 0.735638]\n",
      "469 [D loss: 0.621082, acc.: 58.59%] [G loss: 0.735121]\n",
      "470 [D loss: 0.604313, acc.: 60.94%] [G loss: 0.735994]\n",
      "471 [D loss: 0.603461, acc.: 63.28%] [G loss: 0.738549]\n",
      "472 [D loss: 0.616784, acc.: 58.98%] [G loss: 0.728868]\n",
      "473 [D loss: 0.599667, acc.: 64.84%] [G loss: 0.735023]\n",
      "474 [D loss: 0.595314, acc.: 62.50%] [G loss: 0.749325]\n",
      "475 [D loss: 0.610828, acc.: 60.94%] [G loss: 0.755741]\n",
      "476 [D loss: 0.607846, acc.: 64.06%] [G loss: 0.755158]\n",
      "477 [D loss: 0.605676, acc.: 62.11%] [G loss: 0.749532]\n",
      "478 [D loss: 0.616926, acc.: 61.33%] [G loss: 0.750707]\n",
      "479 [D loss: 0.598419, acc.: 65.62%] [G loss: 0.758697]\n",
      "480 [D loss: 0.602139, acc.: 63.28%] [G loss: 0.760587]\n",
      "481 [D loss: 0.620583, acc.: 58.98%] [G loss: 0.751308]\n",
      "482 [D loss: 0.629385, acc.: 56.64%] [G loss: 0.752166]\n",
      "483 [D loss: 0.623027, acc.: 53.91%] [G loss: 0.757926]\n",
      "484 [D loss: 0.629991, acc.: 55.47%] [G loss: 0.757466]\n",
      "485 [D loss: 0.629656, acc.: 57.03%] [G loss: 0.763940]\n",
      "486 [D loss: 0.627673, acc.: 58.59%] [G loss: 0.759937]\n",
      "487 [D loss: 0.635380, acc.: 56.25%] [G loss: 0.755119]\n",
      "488 [D loss: 0.644307, acc.: 56.64%] [G loss: 0.759779]\n",
      "489 [D loss: 0.641863, acc.: 56.25%] [G loss: 0.772431]\n",
      "490 [D loss: 0.642543, acc.: 56.64%] [G loss: 0.752671]\n",
      "491 [D loss: 0.657909, acc.: 51.56%] [G loss: 0.742659]\n",
      "492 [D loss: 0.642804, acc.: 57.81%] [G loss: 0.747521]\n",
      "493 [D loss: 0.638035, acc.: 57.03%] [G loss: 0.735127]\n",
      "494 [D loss: 0.642303, acc.: 57.81%] [G loss: 0.739908]\n",
      "495 [D loss: 0.639465, acc.: 60.16%] [G loss: 0.727933]\n",
      "496 [D loss: 0.644649, acc.: 57.03%] [G loss: 0.740348]\n",
      "497 [D loss: 0.646974, acc.: 56.64%] [G loss: 0.743579]\n",
      "498 [D loss: 0.658502, acc.: 54.69%] [G loss: 0.751068]\n",
      "499 [D loss: 0.668238, acc.: 50.39%] [G loss: 0.761134]\n",
      "500 [D loss: 0.654112, acc.: 56.64%] [G loss: 0.768337]\n",
      "501 [D loss: 0.656918, acc.: 55.08%] [G loss: 0.773714]\n",
      "502 [D loss: 0.659707, acc.: 55.47%] [G loss: 0.762098]\n",
      "503 [D loss: 0.670256, acc.: 52.34%] [G loss: 0.748980]\n",
      "504 [D loss: 0.677753, acc.: 51.95%] [G loss: 0.719678]\n",
      "505 [D loss: 0.650926, acc.: 55.08%] [G loss: 0.721640]\n",
      "506 [D loss: 0.654187, acc.: 53.52%] [G loss: 0.741060]\n",
      "507 [D loss: 0.652557, acc.: 52.34%] [G loss: 0.745725]\n",
      "508 [D loss: 0.654904, acc.: 55.08%] [G loss: 0.758601]\n",
      "509 [D loss: 0.650205, acc.: 57.03%] [G loss: 0.785026]\n",
      "510 [D loss: 0.639750, acc.: 52.34%] [G loss: 0.785473]\n",
      "511 [D loss: 0.652874, acc.: 50.39%] [G loss: 0.757543]\n",
      "512 [D loss: 0.648971, acc.: 53.91%] [G loss: 0.766310]\n",
      "513 [D loss: 0.654273, acc.: 50.78%] [G loss: 0.753445]\n",
      "514 [D loss: 0.633855, acc.: 51.95%] [G loss: 0.758884]\n",
      "515 [D loss: 0.640717, acc.: 55.86%] [G loss: 0.742643]\n",
      "516 [D loss: 0.638527, acc.: 55.47%] [G loss: 0.748328]\n",
      "517 [D loss: 0.632620, acc.: 60.16%] [G loss: 0.746274]\n",
      "518 [D loss: 0.643196, acc.: 55.08%] [G loss: 0.754444]\n",
      "519 [D loss: 0.634800, acc.: 58.98%] [G loss: 0.766998]\n",
      "520 [D loss: 0.637415, acc.: 60.94%] [G loss: 0.764773]\n",
      "521 [D loss: 0.643465, acc.: 55.08%] [G loss: 0.753157]\n",
      "522 [D loss: 0.640141, acc.: 57.81%] [G loss: 0.755842]\n",
      "523 [D loss: 0.634217, acc.: 62.11%] [G loss: 0.747270]\n",
      "524 [D loss: 0.629836, acc.: 60.16%] [G loss: 0.740522]\n",
      "525 [D loss: 0.640973, acc.: 61.72%] [G loss: 0.733021]\n",
      "526 [D loss: 0.637390, acc.: 63.67%] [G loss: 0.729838]\n",
      "527 [D loss: 0.638244, acc.: 61.72%] [G loss: 0.737851]\n",
      "528 [D loss: 0.629470, acc.: 63.28%] [G loss: 0.745217]\n",
      "529 [D loss: 0.639552, acc.: 58.59%] [G loss: 0.745092]\n",
      "530 [D loss: 0.636690, acc.: 62.89%] [G loss: 0.749476]\n",
      "531 [D loss: 0.642265, acc.: 59.38%] [G loss: 0.756694]\n",
      "532 [D loss: 0.638253, acc.: 58.20%] [G loss: 0.733424]\n",
      "533 [D loss: 0.636425, acc.: 60.94%] [G loss: 0.738573]\n",
      "534 [D loss: 0.633687, acc.: 57.42%] [G loss: 0.756158]\n",
      "535 [D loss: 0.639747, acc.: 57.81%] [G loss: 0.757040]\n",
      "536 [D loss: 0.632150, acc.: 57.42%] [G loss: 0.763444]\n",
      "537 [D loss: 0.642507, acc.: 55.08%] [G loss: 0.751640]\n",
      "538 [D loss: 0.633411, acc.: 57.81%] [G loss: 0.729977]\n",
      "539 [D loss: 0.628300, acc.: 59.38%] [G loss: 0.721025]\n",
      "540 [D loss: 0.623850, acc.: 60.16%] [G loss: 0.734593]\n",
      "541 [D loss: 0.634919, acc.: 58.98%] [G loss: 0.733298]\n",
      "542 [D loss: 0.640008, acc.: 60.16%] [G loss: 0.733465]\n",
      "543 [D loss: 0.634127, acc.: 62.89%] [G loss: 0.738270]\n",
      "544 [D loss: 0.620806, acc.: 62.89%] [G loss: 0.749090]\n",
      "545 [D loss: 0.643859, acc.: 59.38%] [G loss: 0.752813]\n",
      "546 [D loss: 0.643046, acc.: 57.03%] [G loss: 0.767502]\n",
      "547 [D loss: 0.635690, acc.: 59.38%] [G loss: 0.775124]\n",
      "548 [D loss: 0.630118, acc.: 64.06%] [G loss: 0.750215]\n",
      "549 [D loss: 0.627785, acc.: 59.77%] [G loss: 0.743987]\n",
      "550 [D loss: 0.619489, acc.: 61.72%] [G loss: 0.744415]\n",
      "551 [D loss: 0.616657, acc.: 62.50%] [G loss: 0.749039]\n",
      "552 [D loss: 0.607158, acc.: 66.41%] [G loss: 0.757276]\n",
      "553 [D loss: 0.617207, acc.: 63.28%] [G loss: 0.754426]\n",
      "554 [D loss: 0.615977, acc.: 65.23%] [G loss: 0.776267]\n",
      "555 [D loss: 0.620340, acc.: 69.92%] [G loss: 0.794310]\n",
      "556 [D loss: 0.612960, acc.: 66.41%] [G loss: 0.777496]\n",
      "557 [D loss: 0.614810, acc.: 65.62%] [G loss: 0.768422]\n",
      "558 [D loss: 0.619016, acc.: 62.11%] [G loss: 0.761059]\n",
      "559 [D loss: 0.600735, acc.: 71.09%] [G loss: 0.759220]\n",
      "560 [D loss: 0.606385, acc.: 68.36%] [G loss: 0.761609]\n",
      "561 [D loss: 0.628974, acc.: 61.33%] [G loss: 0.761021]\n",
      "562 [D loss: 0.609497, acc.: 67.58%] [G loss: 0.775671]\n",
      "563 [D loss: 0.617610, acc.: 66.41%] [G loss: 0.779155]\n",
      "564 [D loss: 0.623407, acc.: 64.06%] [G loss: 0.782744]\n",
      "565 [D loss: 0.611026, acc.: 71.48%] [G loss: 0.769735]\n",
      "566 [D loss: 0.624256, acc.: 67.58%] [G loss: 0.771034]\n",
      "567 [D loss: 0.617102, acc.: 71.09%] [G loss: 0.775844]\n",
      "568 [D loss: 0.623181, acc.: 69.14%] [G loss: 0.787989]\n",
      "569 [D loss: 0.620529, acc.: 66.02%] [G loss: 0.786843]\n",
      "570 [D loss: 0.630078, acc.: 64.06%] [G loss: 0.790501]\n",
      "571 [D loss: 0.629664, acc.: 62.50%] [G loss: 0.787983]\n",
      "572 [D loss: 0.627725, acc.: 69.14%] [G loss: 0.773684]\n",
      "573 [D loss: 0.625457, acc.: 66.41%] [G loss: 0.765649]\n",
      "574 [D loss: 0.631911, acc.: 62.50%] [G loss: 0.762522]\n",
      "575 [D loss: 0.623247, acc.: 70.31%] [G loss: 0.770328]\n",
      "576 [D loss: 0.648922, acc.: 61.72%] [G loss: 0.762865]\n",
      "577 [D loss: 0.616434, acc.: 62.89%] [G loss: 0.778994]\n",
      "578 [D loss: 0.617290, acc.: 64.45%] [G loss: 0.784146]\n",
      "579 [D loss: 0.632910, acc.: 61.33%] [G loss: 0.764571]\n",
      "580 [D loss: 0.620574, acc.: 63.67%] [G loss: 0.747343]\n",
      "581 [D loss: 0.632778, acc.: 59.77%] [G loss: 0.760155]\n",
      "582 [D loss: 0.611748, acc.: 64.84%] [G loss: 0.779090]\n",
      "583 [D loss: 0.615092, acc.: 67.19%] [G loss: 0.771126]\n",
      "584 [D loss: 0.625756, acc.: 61.72%] [G loss: 0.770237]\n",
      "585 [D loss: 0.624775, acc.: 61.33%] [G loss: 0.778458]\n",
      "586 [D loss: 0.622461, acc.: 67.19%] [G loss: 0.784522]\n",
      "587 [D loss: 0.628411, acc.: 62.50%] [G loss: 0.760553]\n",
      "588 [D loss: 0.631246, acc.: 60.94%] [G loss: 0.751790]\n",
      "589 [D loss: 0.635186, acc.: 61.72%] [G loss: 0.759339]\n",
      "590 [D loss: 0.612802, acc.: 66.02%] [G loss: 0.749026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 [D loss: 0.621795, acc.: 63.67%] [G loss: 0.751474]\n",
      "592 [D loss: 0.617155, acc.: 68.36%] [G loss: 0.768209]\n",
      "593 [D loss: 0.610699, acc.: 66.80%] [G loss: 0.779912]\n",
      "594 [D loss: 0.611706, acc.: 67.97%] [G loss: 0.763199]\n",
      "595 [D loss: 0.607320, acc.: 67.97%] [G loss: 0.777296]\n",
      "596 [D loss: 0.619232, acc.: 62.89%] [G loss: 0.783575]\n",
      "597 [D loss: 0.623017, acc.: 63.28%] [G loss: 0.779951]\n",
      "598 [D loss: 0.611606, acc.: 66.41%] [G loss: 0.784342]\n",
      "599 [D loss: 0.623934, acc.: 63.28%] [G loss: 0.788355]\n",
      "600 [D loss: 0.624417, acc.: 64.84%] [G loss: 0.782001]\n",
      "601 [D loss: 0.626042, acc.: 66.41%] [G loss: 0.776995]\n",
      "602 [D loss: 0.633234, acc.: 60.55%] [G loss: 0.784090]\n",
      "603 [D loss: 0.629794, acc.: 66.80%] [G loss: 0.779195]\n",
      "604 [D loss: 0.635399, acc.: 63.67%] [G loss: 0.793558]\n",
      "605 [D loss: 0.641308, acc.: 64.06%] [G loss: 0.789551]\n",
      "606 [D loss: 0.637184, acc.: 63.67%] [G loss: 0.801161]\n",
      "607 [D loss: 0.658972, acc.: 60.16%] [G loss: 0.798061]\n",
      "608 [D loss: 0.629668, acc.: 66.80%] [G loss: 0.794014]\n",
      "609 [D loss: 0.636977, acc.: 61.72%] [G loss: 0.806995]\n",
      "610 [D loss: 0.640975, acc.: 63.67%] [G loss: 0.810174]\n",
      "611 [D loss: 0.621565, acc.: 66.80%] [G loss: 0.803697]\n",
      "612 [D loss: 0.622869, acc.: 68.36%] [G loss: 0.814638]\n",
      "613 [D loss: 0.634521, acc.: 65.62%] [G loss: 0.787473]\n",
      "614 [D loss: 0.612911, acc.: 67.97%] [G loss: 0.784434]\n",
      "615 [D loss: 0.619095, acc.: 70.70%] [G loss: 0.774794]\n",
      "616 [D loss: 0.628991, acc.: 67.19%] [G loss: 0.766411]\n",
      "617 [D loss: 0.628575, acc.: 65.62%] [G loss: 0.758274]\n",
      "618 [D loss: 0.619354, acc.: 68.36%] [G loss: 0.765224]\n",
      "619 [D loss: 0.626476, acc.: 67.58%] [G loss: 0.791322]\n",
      "620 [D loss: 0.613600, acc.: 75.00%] [G loss: 0.760637]\n",
      "621 [D loss: 0.624154, acc.: 66.02%] [G loss: 0.758979]\n",
      "622 [D loss: 0.627324, acc.: 67.19%] [G loss: 0.778138]\n",
      "623 [D loss: 0.614767, acc.: 66.41%] [G loss: 0.800041]\n",
      "624 [D loss: 0.612243, acc.: 71.09%] [G loss: 0.815341]\n",
      "625 [D loss: 0.611293, acc.: 74.22%] [G loss: 0.790877]\n",
      "626 [D loss: 0.606650, acc.: 66.80%] [G loss: 0.789555]\n",
      "627 [D loss: 0.601382, acc.: 72.66%] [G loss: 0.795466]\n",
      "628 [D loss: 0.599630, acc.: 72.27%] [G loss: 0.784215]\n",
      "629 [D loss: 0.616891, acc.: 66.02%] [G loss: 0.800416]\n",
      "630 [D loss: 0.616568, acc.: 74.22%] [G loss: 0.784196]\n",
      "631 [D loss: 0.633074, acc.: 65.62%] [G loss: 0.775797]\n",
      "632 [D loss: 0.618435, acc.: 70.31%] [G loss: 0.784992]\n",
      "633 [D loss: 0.615536, acc.: 72.66%] [G loss: 0.784682]\n",
      "634 [D loss: 0.626434, acc.: 68.75%] [G loss: 0.768658]\n",
      "635 [D loss: 0.632248, acc.: 69.92%] [G loss: 0.758742]\n",
      "636 [D loss: 0.617980, acc.: 73.05%] [G loss: 0.762458]\n",
      "637 [D loss: 0.623282, acc.: 67.97%] [G loss: 0.778543]\n",
      "638 [D loss: 0.623719, acc.: 71.09%] [G loss: 0.790652]\n",
      "639 [D loss: 0.626452, acc.: 69.53%] [G loss: 0.790663]\n",
      "640 [D loss: 0.629195, acc.: 68.75%] [G loss: 0.779882]\n",
      "641 [D loss: 0.618375, acc.: 67.97%] [G loss: 0.782857]\n",
      "642 [D loss: 0.634059, acc.: 65.23%] [G loss: 0.785371]\n",
      "643 [D loss: 0.607866, acc.: 69.92%] [G loss: 0.788160]\n",
      "644 [D loss: 0.628456, acc.: 67.19%] [G loss: 0.783930]\n",
      "645 [D loss: 0.619981, acc.: 65.23%] [G loss: 0.775917]\n",
      "646 [D loss: 0.610793, acc.: 69.92%] [G loss: 0.775520]\n",
      "647 [D loss: 0.619131, acc.: 65.62%] [G loss: 0.765364]\n",
      "648 [D loss: 0.622213, acc.: 66.80%] [G loss: 0.766415]\n",
      "649 [D loss: 0.612215, acc.: 72.66%] [G loss: 0.771370]\n",
      "650 [D loss: 0.609581, acc.: 69.92%] [G loss: 0.767240]\n",
      "651 [D loss: 0.606112, acc.: 68.36%] [G loss: 0.767204]\n",
      "652 [D loss: 0.583833, acc.: 69.53%] [G loss: 0.780312]\n",
      "653 [D loss: 0.620271, acc.: 63.28%] [G loss: 0.779094]\n",
      "654 [D loss: 0.618207, acc.: 69.92%] [G loss: 0.780284]\n",
      "655 [D loss: 0.620708, acc.: 71.48%] [G loss: 0.790747]\n",
      "656 [D loss: 0.616089, acc.: 66.80%] [G loss: 0.794616]\n",
      "657 [D loss: 0.623867, acc.: 69.53%] [G loss: 0.787024]\n",
      "658 [D loss: 0.622169, acc.: 62.89%] [G loss: 0.793666]\n",
      "659 [D loss: 0.605588, acc.: 68.36%] [G loss: 0.799408]\n",
      "660 [D loss: 0.621812, acc.: 67.58%] [G loss: 0.804642]\n",
      "661 [D loss: 0.613069, acc.: 72.27%] [G loss: 0.795707]\n",
      "662 [D loss: 0.624735, acc.: 65.62%] [G loss: 0.790523]\n",
      "663 [D loss: 0.627503, acc.: 66.80%] [G loss: 0.791366]\n",
      "664 [D loss: 0.620512, acc.: 66.41%] [G loss: 0.776382]\n",
      "665 [D loss: 0.622509, acc.: 63.67%] [G loss: 0.781396]\n",
      "666 [D loss: 0.615357, acc.: 67.58%] [G loss: 0.782892]\n",
      "667 [D loss: 0.613118, acc.: 68.36%] [G loss: 0.788094]\n",
      "668 [D loss: 0.608096, acc.: 66.02%] [G loss: 0.807085]\n",
      "669 [D loss: 0.609400, acc.: 70.31%] [G loss: 0.814692]\n",
      "670 [D loss: 0.619902, acc.: 68.75%] [G loss: 0.823604]\n",
      "671 [D loss: 0.615612, acc.: 67.58%] [G loss: 0.806841]\n",
      "672 [D loss: 0.615848, acc.: 69.53%] [G loss: 0.780515]\n",
      "673 [D loss: 0.617057, acc.: 65.62%] [G loss: 0.794054]\n",
      "674 [D loss: 0.620671, acc.: 68.75%] [G loss: 0.811124]\n",
      "675 [D loss: 0.627403, acc.: 63.28%] [G loss: 0.799761]\n",
      "676 [D loss: 0.621221, acc.: 65.23%] [G loss: 0.800681]\n",
      "677 [D loss: 0.612702, acc.: 63.28%] [G loss: 0.821251]\n",
      "678 [D loss: 0.620006, acc.: 65.62%] [G loss: 0.813044]\n",
      "679 [D loss: 0.619015, acc.: 62.11%] [G loss: 0.822075]\n",
      "680 [D loss: 0.620732, acc.: 68.36%] [G loss: 0.820690]\n",
      "681 [D loss: 0.615962, acc.: 67.97%] [G loss: 0.811438]\n",
      "682 [D loss: 0.614544, acc.: 70.31%] [G loss: 0.804431]\n",
      "683 [D loss: 0.613271, acc.: 64.84%] [G loss: 0.789678]\n",
      "684 [D loss: 0.598613, acc.: 67.19%] [G loss: 0.810578]\n",
      "685 [D loss: 0.600977, acc.: 71.09%] [G loss: 0.819243]\n",
      "686 [D loss: 0.622730, acc.: 67.58%] [G loss: 0.812967]\n",
      "687 [D loss: 0.601405, acc.: 71.09%] [G loss: 0.820685]\n",
      "688 [D loss: 0.616993, acc.: 73.83%] [G loss: 0.808587]\n",
      "689 [D loss: 0.610524, acc.: 72.27%] [G loss: 0.812467]\n",
      "690 [D loss: 0.632717, acc.: 61.72%] [G loss: 0.802377]\n",
      "691 [D loss: 0.620976, acc.: 68.36%] [G loss: 0.821938]\n",
      "692 [D loss: 0.616217, acc.: 67.97%] [G loss: 0.797657]\n",
      "693 [D loss: 0.622985, acc.: 69.53%] [G loss: 0.807885]\n",
      "694 [D loss: 0.594134, acc.: 69.53%] [G loss: 0.808579]\n",
      "695 [D loss: 0.628302, acc.: 64.84%] [G loss: 0.811831]\n",
      "696 [D loss: 0.625216, acc.: 66.80%] [G loss: 0.812535]\n",
      "697 [D loss: 0.620593, acc.: 65.62%] [G loss: 0.814052]\n",
      "698 [D loss: 0.620392, acc.: 69.53%] [G loss: 0.818298]\n",
      "699 [D loss: 0.634003, acc.: 64.84%] [G loss: 0.846208]\n",
      "700 [D loss: 0.612083, acc.: 71.88%] [G loss: 0.827045]\n",
      "701 [D loss: 0.612345, acc.: 69.92%] [G loss: 0.803502]\n",
      "702 [D loss: 0.631020, acc.: 65.23%] [G loss: 0.789709]\n",
      "703 [D loss: 0.620586, acc.: 65.62%] [G loss: 0.800133]\n",
      "704 [D loss: 0.625097, acc.: 66.41%] [G loss: 0.805374]\n",
      "705 [D loss: 0.626626, acc.: 66.41%] [G loss: 0.807034]\n",
      "706 [D loss: 0.622333, acc.: 65.62%] [G loss: 0.802302]\n",
      "707 [D loss: 0.610564, acc.: 68.36%] [G loss: 0.801950]\n",
      "708 [D loss: 0.616867, acc.: 70.70%] [G loss: 0.781386]\n",
      "709 [D loss: 0.623489, acc.: 66.02%] [G loss: 0.784688]\n",
      "710 [D loss: 0.624878, acc.: 62.11%] [G loss: 0.799564]\n",
      "711 [D loss: 0.603113, acc.: 66.41%] [G loss: 0.818206]\n",
      "712 [D loss: 0.622684, acc.: 67.19%] [G loss: 0.798629]\n",
      "713 [D loss: 0.632586, acc.: 63.28%] [G loss: 0.795100]\n",
      "714 [D loss: 0.634653, acc.: 64.45%] [G loss: 0.815332]\n",
      "715 [D loss: 0.620069, acc.: 68.36%] [G loss: 0.807697]\n",
      "716 [D loss: 0.618742, acc.: 67.97%] [G loss: 0.820354]\n",
      "717 [D loss: 0.617483, acc.: 66.02%] [G loss: 0.790916]\n",
      "718 [D loss: 0.622187, acc.: 62.89%] [G loss: 0.811062]\n",
      "719 [D loss: 0.620187, acc.: 71.88%] [G loss: 0.816529]\n",
      "720 [D loss: 0.635978, acc.: 68.75%] [G loss: 0.813775]\n",
      "721 [D loss: 0.625799, acc.: 63.67%] [G loss: 0.823922]\n",
      "722 [D loss: 0.632742, acc.: 63.67%] [G loss: 0.828797]\n",
      "723 [D loss: 0.623059, acc.: 66.41%] [G loss: 0.809138]\n",
      "724 [D loss: 0.634433, acc.: 63.67%] [G loss: 0.829298]\n",
      "725 [D loss: 0.604674, acc.: 71.48%] [G loss: 0.831479]\n",
      "726 [D loss: 0.618881, acc.: 68.36%] [G loss: 0.857596]\n",
      "727 [D loss: 0.622740, acc.: 65.23%] [G loss: 0.850166]\n",
      "728 [D loss: 0.615915, acc.: 65.62%] [G loss: 0.841359]\n",
      "729 [D loss: 0.629959, acc.: 62.89%] [G loss: 0.811689]\n",
      "730 [D loss: 0.624768, acc.: 69.14%] [G loss: 0.808587]\n",
      "731 [D loss: 0.644771, acc.: 64.84%] [G loss: 0.813148]\n",
      "732 [D loss: 0.614567, acc.: 67.19%] [G loss: 0.828788]\n",
      "733 [D loss: 0.607463, acc.: 68.36%] [G loss: 0.804175]\n",
      "734 [D loss: 0.607322, acc.: 67.97%] [G loss: 0.824536]\n",
      "735 [D loss: 0.611569, acc.: 67.19%] [G loss: 0.812410]\n",
      "736 [D loss: 0.623841, acc.: 68.75%] [G loss: 0.828459]\n",
      "737 [D loss: 0.608017, acc.: 69.92%] [G loss: 0.847043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738 [D loss: 0.625558, acc.: 65.62%] [G loss: 0.837945]\n",
      "739 [D loss: 0.621596, acc.: 71.48%] [G loss: 0.824417]\n",
      "740 [D loss: 0.623332, acc.: 66.80%] [G loss: 0.809942]\n",
      "741 [D loss: 0.626137, acc.: 66.41%] [G loss: 0.805115]\n",
      "742 [D loss: 0.629990, acc.: 63.67%] [G loss: 0.805090]\n",
      "743 [D loss: 0.633897, acc.: 66.41%] [G loss: 0.810620]\n",
      "744 [D loss: 0.622523, acc.: 64.84%] [G loss: 0.831880]\n",
      "745 [D loss: 0.630401, acc.: 64.45%] [G loss: 0.801643]\n",
      "746 [D loss: 0.613052, acc.: 69.14%] [G loss: 0.817512]\n",
      "747 [D loss: 0.630739, acc.: 63.28%] [G loss: 0.810170]\n",
      "748 [D loss: 0.620448, acc.: 62.50%] [G loss: 0.850345]\n",
      "749 [D loss: 0.639448, acc.: 64.06%] [G loss: 0.818778]\n",
      "750 [D loss: 0.637051, acc.: 66.02%] [G loss: 0.815640]\n",
      "751 [D loss: 0.624559, acc.: 70.31%] [G loss: 0.795708]\n",
      "752 [D loss: 0.604942, acc.: 73.83%] [G loss: 0.796513]\n",
      "753 [D loss: 0.648019, acc.: 64.84%] [G loss: 0.800719]\n",
      "754 [D loss: 0.590180, acc.: 74.61%] [G loss: 0.828948]\n",
      "755 [D loss: 0.624725, acc.: 67.19%] [G loss: 0.806199]\n",
      "756 [D loss: 0.596065, acc.: 71.88%] [G loss: 0.821418]\n",
      "757 [D loss: 0.588329, acc.: 77.73%] [G loss: 0.820172]\n",
      "758 [D loss: 0.613326, acc.: 69.92%] [G loss: 0.810229]\n",
      "759 [D loss: 0.608903, acc.: 69.53%] [G loss: 0.836439]\n",
      "760 [D loss: 0.615749, acc.: 68.75%] [G loss: 0.840045]\n",
      "761 [D loss: 0.607810, acc.: 69.53%] [G loss: 0.820243]\n",
      "762 [D loss: 0.612682, acc.: 71.09%] [G loss: 0.858915]\n",
      "763 [D loss: 0.608599, acc.: 68.75%] [G loss: 0.867434]\n",
      "764 [D loss: 0.599668, acc.: 71.88%] [G loss: 0.851015]\n",
      "765 [D loss: 0.633300, acc.: 64.06%] [G loss: 0.837437]\n",
      "766 [D loss: 0.576060, acc.: 73.44%] [G loss: 0.837571]\n",
      "767 [D loss: 0.590630, acc.: 72.27%] [G loss: 0.854360]\n",
      "768 [D loss: 0.596942, acc.: 69.92%] [G loss: 0.853499]\n",
      "769 [D loss: 0.588217, acc.: 71.09%] [G loss: 0.867729]\n",
      "770 [D loss: 0.592395, acc.: 72.66%] [G loss: 0.851393]\n",
      "771 [D loss: 0.602594, acc.: 68.36%] [G loss: 0.862970]\n",
      "772 [D loss: 0.597510, acc.: 71.88%] [G loss: 0.861402]\n",
      "773 [D loss: 0.607898, acc.: 66.80%] [G loss: 0.856713]\n",
      "774 [D loss: 0.597342, acc.: 71.09%] [G loss: 0.859550]\n",
      "775 [D loss: 0.606434, acc.: 68.36%] [G loss: 0.862356]\n",
      "776 [D loss: 0.607227, acc.: 67.19%] [G loss: 0.829047]\n",
      "777 [D loss: 0.609551, acc.: 67.58%] [G loss: 0.853206]\n",
      "778 [D loss: 0.607874, acc.: 66.80%] [G loss: 0.871816]\n",
      "779 [D loss: 0.624730, acc.: 65.62%] [G loss: 0.835909]\n",
      "780 [D loss: 0.624511, acc.: 66.02%] [G loss: 0.833219]\n",
      "781 [D loss: 0.617888, acc.: 67.97%] [G loss: 0.869876]\n",
      "782 [D loss: 0.626843, acc.: 66.02%] [G loss: 0.883025]\n",
      "783 [D loss: 0.650495, acc.: 62.11%] [G loss: 0.869634]\n",
      "784 [D loss: 0.639319, acc.: 61.72%] [G loss: 0.860862]\n",
      "785 [D loss: 0.638598, acc.: 60.94%] [G loss: 0.844532]\n",
      "786 [D loss: 0.631047, acc.: 62.50%] [G loss: 0.838090]\n",
      "787 [D loss: 0.623030, acc.: 67.97%] [G loss: 0.845295]\n",
      "788 [D loss: 0.624562, acc.: 64.84%] [G loss: 0.888678]\n",
      "789 [D loss: 0.627791, acc.: 69.14%] [G loss: 0.882813]\n",
      "790 [D loss: 0.625236, acc.: 66.02%] [G loss: 0.871221]\n",
      "791 [D loss: 0.606318, acc.: 69.92%] [G loss: 0.834646]\n",
      "792 [D loss: 0.623106, acc.: 61.72%] [G loss: 0.845014]\n",
      "793 [D loss: 0.624839, acc.: 68.36%] [G loss: 0.858858]\n",
      "794 [D loss: 0.593747, acc.: 71.88%] [G loss: 0.863068]\n",
      "795 [D loss: 0.611099, acc.: 66.80%] [G loss: 0.892265]\n",
      "796 [D loss: 0.634737, acc.: 61.72%] [G loss: 0.870890]\n",
      "797 [D loss: 0.619869, acc.: 67.19%] [G loss: 0.837009]\n",
      "798 [D loss: 0.608442, acc.: 66.41%] [G loss: 0.836884]\n",
      "799 [D loss: 0.634545, acc.: 60.55%] [G loss: 0.833000]\n",
      "800 [D loss: 0.625591, acc.: 61.33%] [G loss: 0.836942]\n",
      "801 [D loss: 0.614787, acc.: 68.75%] [G loss: 0.851113]\n",
      "802 [D loss: 0.630312, acc.: 65.62%] [G loss: 0.854133]\n",
      "803 [D loss: 0.619413, acc.: 71.48%] [G loss: 0.844948]\n",
      "804 [D loss: 0.631956, acc.: 63.67%] [G loss: 0.853093]\n",
      "805 [D loss: 0.617715, acc.: 69.14%] [G loss: 0.835949]\n",
      "806 [D loss: 0.612655, acc.: 66.80%] [G loss: 0.824239]\n",
      "807 [D loss: 0.602363, acc.: 67.97%] [G loss: 0.832900]\n",
      "808 [D loss: 0.591267, acc.: 74.22%] [G loss: 0.845778]\n",
      "809 [D loss: 0.591587, acc.: 71.48%] [G loss: 0.866380]\n",
      "810 [D loss: 0.589497, acc.: 70.31%] [G loss: 0.883910]\n",
      "811 [D loss: 0.604317, acc.: 68.36%] [G loss: 0.865816]\n",
      "812 [D loss: 0.618015, acc.: 67.58%] [G loss: 0.849353]\n",
      "813 [D loss: 0.599676, acc.: 68.75%] [G loss: 0.855877]\n",
      "814 [D loss: 0.590292, acc.: 72.66%] [G loss: 0.865032]\n",
      "815 [D loss: 0.609741, acc.: 70.31%] [G loss: 0.878941]\n",
      "816 [D loss: 0.608089, acc.: 73.83%] [G loss: 0.888689]\n",
      "817 [D loss: 0.612129, acc.: 71.09%] [G loss: 0.875549]\n",
      "818 [D loss: 0.600598, acc.: 71.09%] [G loss: 0.838345]\n",
      "819 [D loss: 0.609837, acc.: 69.92%] [G loss: 0.831809]\n",
      "820 [D loss: 0.588229, acc.: 75.00%] [G loss: 0.836088]\n",
      "821 [D loss: 0.618039, acc.: 66.80%] [G loss: 0.834546]\n",
      "822 [D loss: 0.601257, acc.: 72.27%] [G loss: 0.852180]\n",
      "823 [D loss: 0.609880, acc.: 70.70%] [G loss: 0.854710]\n",
      "824 [D loss: 0.600390, acc.: 74.61%] [G loss: 0.879788]\n",
      "825 [D loss: 0.604512, acc.: 74.61%] [G loss: 0.895397]\n",
      "826 [D loss: 0.610366, acc.: 69.92%] [G loss: 0.869571]\n",
      "827 [D loss: 0.604387, acc.: 71.48%] [G loss: 0.850826]\n",
      "828 [D loss: 0.621147, acc.: 66.80%] [G loss: 0.842821]\n",
      "829 [D loss: 0.605832, acc.: 71.09%] [G loss: 0.830112]\n",
      "830 [D loss: 0.619888, acc.: 69.53%] [G loss: 0.853683]\n",
      "831 [D loss: 0.605150, acc.: 72.27%] [G loss: 0.833120]\n",
      "832 [D loss: 0.595246, acc.: 75.39%] [G loss: 0.851929]\n",
      "833 [D loss: 0.587096, acc.: 75.00%] [G loss: 0.860865]\n",
      "834 [D loss: 0.593501, acc.: 74.22%] [G loss: 0.882721]\n",
      "835 [D loss: 0.588836, acc.: 73.44%] [G loss: 0.910139]\n",
      "836 [D loss: 0.590619, acc.: 75.39%] [G loss: 0.882144]\n",
      "837 [D loss: 0.577598, acc.: 81.64%] [G loss: 0.877811]\n",
      "838 [D loss: 0.580954, acc.: 74.22%] [G loss: 0.903371]\n",
      "839 [D loss: 0.595664, acc.: 69.53%] [G loss: 0.955061]\n",
      "840 [D loss: 0.585358, acc.: 76.56%] [G loss: 0.948057]\n",
      "841 [D loss: 0.610149, acc.: 71.09%] [G loss: 0.924610]\n",
      "842 [D loss: 0.587821, acc.: 78.52%] [G loss: 0.911688]\n",
      "843 [D loss: 0.606238, acc.: 72.27%] [G loss: 0.910405]\n",
      "844 [D loss: 0.582821, acc.: 77.34%] [G loss: 0.877652]\n",
      "845 [D loss: 0.580392, acc.: 79.30%] [G loss: 0.859459]\n",
      "846 [D loss: 0.595978, acc.: 74.61%] [G loss: 0.867136]\n",
      "847 [D loss: 0.576712, acc.: 73.05%] [G loss: 0.897263]\n",
      "848 [D loss: 0.583295, acc.: 72.66%] [G loss: 0.874791]\n",
      "849 [D loss: 0.571217, acc.: 74.61%] [G loss: 0.906518]\n",
      "850 [D loss: 0.575739, acc.: 75.00%] [G loss: 0.917745]\n",
      "851 [D loss: 0.597803, acc.: 69.92%] [G loss: 0.883305]\n",
      "852 [D loss: 0.584202, acc.: 69.53%] [G loss: 0.900726]\n",
      "853 [D loss: 0.592325, acc.: 71.88%] [G loss: 0.889663]\n",
      "854 [D loss: 0.603990, acc.: 69.92%] [G loss: 0.910361]\n",
      "855 [D loss: 0.598175, acc.: 72.66%] [G loss: 0.912090]\n",
      "856 [D loss: 0.630163, acc.: 63.28%] [G loss: 0.900698]\n",
      "857 [D loss: 0.625896, acc.: 70.31%] [G loss: 0.873800]\n",
      "858 [D loss: 0.614116, acc.: 69.14%] [G loss: 0.851834]\n",
      "859 [D loss: 0.595698, acc.: 73.05%] [G loss: 0.862422]\n",
      "860 [D loss: 0.597288, acc.: 69.14%] [G loss: 0.877981]\n",
      "861 [D loss: 0.594302, acc.: 69.14%] [G loss: 0.848584]\n",
      "862 [D loss: 0.583753, acc.: 75.78%] [G loss: 0.873092]\n",
      "863 [D loss: 0.585483, acc.: 76.56%] [G loss: 0.895963]\n",
      "864 [D loss: 0.554556, acc.: 77.34%] [G loss: 0.939882]\n",
      "865 [D loss: 0.578508, acc.: 73.44%] [G loss: 0.927929]\n",
      "866 [D loss: 0.579815, acc.: 71.09%] [G loss: 0.948031]\n",
      "867 [D loss: 0.567605, acc.: 78.52%] [G loss: 0.956642]\n",
      "868 [D loss: 0.592329, acc.: 75.39%] [G loss: 0.948042]\n",
      "869 [D loss: 0.595381, acc.: 76.95%] [G loss: 0.913366]\n",
      "870 [D loss: 0.581595, acc.: 73.05%] [G loss: 0.901033]\n",
      "871 [D loss: 0.596262, acc.: 73.05%] [G loss: 0.913403]\n",
      "872 [D loss: 0.585561, acc.: 73.44%] [G loss: 0.914059]\n",
      "873 [D loss: 0.575092, acc.: 76.56%] [G loss: 0.911180]\n",
      "874 [D loss: 0.569279, acc.: 77.73%] [G loss: 0.938009]\n",
      "875 [D loss: 0.547035, acc.: 79.69%] [G loss: 0.930469]\n",
      "876 [D loss: 0.542325, acc.: 79.69%] [G loss: 0.947727]\n",
      "877 [D loss: 0.549499, acc.: 78.91%] [G loss: 0.973816]\n",
      "878 [D loss: 0.554517, acc.: 80.47%] [G loss: 0.941130]\n",
      "879 [D loss: 0.560832, acc.: 77.73%] [G loss: 0.940260]\n",
      "880 [D loss: 0.578182, acc.: 75.78%] [G loss: 0.907869]\n",
      "881 [D loss: 0.576876, acc.: 72.27%] [G loss: 0.933877]\n",
      "882 [D loss: 0.577604, acc.: 75.39%] [G loss: 0.921169]\n",
      "883 [D loss: 0.595659, acc.: 69.53%] [G loss: 0.918088]\n",
      "884 [D loss: 0.561056, acc.: 80.86%] [G loss: 0.917442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885 [D loss: 0.564549, acc.: 80.08%] [G loss: 0.916331]\n",
      "886 [D loss: 0.583076, acc.: 75.39%] [G loss: 0.918519]\n",
      "887 [D loss: 0.560488, acc.: 79.69%] [G loss: 0.915672]\n",
      "888 [D loss: 0.573851, acc.: 76.17%] [G loss: 0.904014]\n",
      "889 [D loss: 0.584193, acc.: 72.66%] [G loss: 0.908135]\n",
      "890 [D loss: 0.552116, acc.: 80.47%] [G loss: 0.930791]\n",
      "891 [D loss: 0.555113, acc.: 83.59%] [G loss: 0.924620]\n",
      "892 [D loss: 0.563828, acc.: 73.83%] [G loss: 0.948047]\n",
      "893 [D loss: 0.589851, acc.: 74.61%] [G loss: 0.921921]\n",
      "894 [D loss: 0.576627, acc.: 73.05%] [G loss: 0.927448]\n",
      "895 [D loss: 0.605593, acc.: 71.48%] [G loss: 0.933859]\n",
      "896 [D loss: 0.570604, acc.: 76.56%] [G loss: 0.940138]\n",
      "897 [D loss: 0.587621, acc.: 76.56%] [G loss: 0.914131]\n",
      "898 [D loss: 0.563459, acc.: 77.73%] [G loss: 0.915940]\n",
      "899 [D loss: 0.564318, acc.: 77.34%] [G loss: 0.904638]\n",
      "900 [D loss: 0.577954, acc.: 75.00%] [G loss: 0.922927]\n",
      "901 [D loss: 0.554828, acc.: 81.25%] [G loss: 0.926977]\n",
      "902 [D loss: 0.561092, acc.: 80.47%] [G loss: 0.923332]\n",
      "903 [D loss: 0.556867, acc.: 78.91%] [G loss: 0.951879]\n",
      "904 [D loss: 0.574652, acc.: 73.83%] [G loss: 0.976459]\n",
      "905 [D loss: 0.554493, acc.: 79.69%] [G loss: 0.991010]\n",
      "906 [D loss: 0.560489, acc.: 81.64%] [G loss: 0.962929]\n",
      "907 [D loss: 0.544658, acc.: 84.77%] [G loss: 0.932826]\n",
      "908 [D loss: 0.546550, acc.: 82.81%] [G loss: 0.942677]\n",
      "909 [D loss: 0.545007, acc.: 82.81%] [G loss: 0.984424]\n",
      "910 [D loss: 0.547576, acc.: 84.38%] [G loss: 0.980759]\n",
      "911 [D loss: 0.553245, acc.: 80.47%] [G loss: 0.960885]\n",
      "912 [D loss: 0.541293, acc.: 80.86%] [G loss: 0.994969]\n",
      "913 [D loss: 0.557599, acc.: 80.08%] [G loss: 0.970830]\n",
      "914 [D loss: 0.580793, acc.: 76.95%] [G loss: 0.972916]\n",
      "915 [D loss: 0.555648, acc.: 83.20%] [G loss: 0.971583]\n",
      "916 [D loss: 0.570715, acc.: 78.12%] [G loss: 0.982800]\n",
      "917 [D loss: 0.565128, acc.: 79.69%] [G loss: 0.971897]\n",
      "918 [D loss: 0.576850, acc.: 74.22%] [G loss: 0.928202]\n",
      "919 [D loss: 0.591294, acc.: 70.70%] [G loss: 0.960920]\n",
      "920 [D loss: 0.559004, acc.: 76.56%] [G loss: 0.985961]\n",
      "921 [D loss: 0.582665, acc.: 72.66%] [G loss: 0.981243]\n",
      "922 [D loss: 0.633116, acc.: 62.89%] [G loss: 0.958433]\n",
      "923 [D loss: 0.617708, acc.: 64.45%] [G loss: 0.995253]\n",
      "924 [D loss: 0.623004, acc.: 63.67%] [G loss: 0.987918]\n",
      "925 [D loss: 0.629623, acc.: 59.77%] [G loss: 0.984205]\n",
      "926 [D loss: 0.584853, acc.: 72.66%] [G loss: 1.011136]\n",
      "927 [D loss: 0.615731, acc.: 67.58%] [G loss: 0.989047]\n",
      "928 [D loss: 0.593972, acc.: 71.48%] [G loss: 1.014417]\n",
      "929 [D loss: 0.588004, acc.: 71.88%] [G loss: 1.028606]\n",
      "930 [D loss: 0.609814, acc.: 71.48%] [G loss: 1.015256]\n",
      "931 [D loss: 0.598466, acc.: 72.27%] [G loss: 0.991943]\n",
      "932 [D loss: 0.578475, acc.: 76.17%] [G loss: 0.991710]\n",
      "933 [D loss: 0.593619, acc.: 70.70%] [G loss: 0.984499]\n",
      "934 [D loss: 0.601230, acc.: 68.75%] [G loss: 0.963308]\n",
      "935 [D loss: 0.582639, acc.: 71.48%] [G loss: 0.933289]\n",
      "936 [D loss: 0.577958, acc.: 73.83%] [G loss: 0.942459]\n",
      "937 [D loss: 0.583344, acc.: 73.05%] [G loss: 0.934390]\n",
      "938 [D loss: 0.602880, acc.: 70.70%] [G loss: 0.952524]\n",
      "939 [D loss: 0.586290, acc.: 74.61%] [G loss: 0.924129]\n",
      "940 [D loss: 0.583225, acc.: 73.05%] [G loss: 0.900302]\n",
      "941 [D loss: 0.563754, acc.: 75.39%] [G loss: 0.944853]\n",
      "942 [D loss: 0.565200, acc.: 74.61%] [G loss: 0.973830]\n",
      "943 [D loss: 0.570839, acc.: 75.00%] [G loss: 0.971576]\n",
      "944 [D loss: 0.566195, acc.: 77.34%] [G loss: 0.987190]\n",
      "945 [D loss: 0.557736, acc.: 81.25%] [G loss: 0.988299]\n",
      "946 [D loss: 0.556203, acc.: 78.12%] [G loss: 0.972193]\n",
      "947 [D loss: 0.580735, acc.: 69.14%] [G loss: 0.991647]\n",
      "948 [D loss: 0.574330, acc.: 80.08%] [G loss: 0.994381]\n",
      "949 [D loss: 0.574044, acc.: 79.30%] [G loss: 0.955222]\n",
      "950 [D loss: 0.569473, acc.: 80.47%] [G loss: 0.935240]\n",
      "951 [D loss: 0.581203, acc.: 73.83%] [G loss: 0.932721]\n",
      "952 [D loss: 0.566209, acc.: 76.17%] [G loss: 0.940036]\n",
      "953 [D loss: 0.573448, acc.: 76.95%] [G loss: 0.931241]\n",
      "954 [D loss: 0.540075, acc.: 79.69%] [G loss: 0.933025]\n",
      "955 [D loss: 0.568483, acc.: 70.31%] [G loss: 0.950675]\n",
      "956 [D loss: 0.582234, acc.: 71.09%] [G loss: 0.961783]\n",
      "957 [D loss: 0.578723, acc.: 74.22%] [G loss: 0.957443]\n",
      "958 [D loss: 0.590868, acc.: 73.83%] [G loss: 0.919057]\n",
      "959 [D loss: 0.596737, acc.: 69.92%] [G loss: 0.921551]\n",
      "960 [D loss: 0.593132, acc.: 73.44%] [G loss: 0.935706]\n",
      "961 [D loss: 0.591574, acc.: 74.61%] [G loss: 0.911573]\n",
      "962 [D loss: 0.592749, acc.: 71.88%] [G loss: 0.911829]\n",
      "963 [D loss: 0.571609, acc.: 75.39%] [G loss: 0.919307]\n",
      "964 [D loss: 0.575564, acc.: 73.44%] [G loss: 0.904011]\n",
      "965 [D loss: 0.601175, acc.: 69.14%] [G loss: 0.883694]\n",
      "966 [D loss: 0.585549, acc.: 72.27%] [G loss: 0.888462]\n",
      "967 [D loss: 0.606683, acc.: 66.41%] [G loss: 0.887511]\n",
      "968 [D loss: 0.598517, acc.: 67.97%] [G loss: 0.931674]\n",
      "969 [D loss: 0.575450, acc.: 77.73%] [G loss: 0.916915]\n",
      "970 [D loss: 0.583927, acc.: 72.27%] [G loss: 0.934664]\n",
      "971 [D loss: 0.608959, acc.: 70.31%] [G loss: 0.936065]\n",
      "972 [D loss: 0.565553, acc.: 79.30%] [G loss: 0.965391]\n",
      "973 [D loss: 0.577902, acc.: 77.34%] [G loss: 0.946542]\n",
      "974 [D loss: 0.564277, acc.: 76.17%] [G loss: 0.926944]\n",
      "975 [D loss: 0.605857, acc.: 70.70%] [G loss: 0.941086]\n",
      "976 [D loss: 0.580258, acc.: 73.83%] [G loss: 0.919609]\n",
      "977 [D loss: 0.568657, acc.: 76.17%] [G loss: 0.940027]\n",
      "978 [D loss: 0.586503, acc.: 73.83%] [G loss: 0.932809]\n",
      "979 [D loss: 0.573463, acc.: 78.91%] [G loss: 0.965601]\n",
      "980 [D loss: 0.574945, acc.: 75.78%] [G loss: 0.940882]\n",
      "981 [D loss: 0.597916, acc.: 69.53%] [G loss: 0.922676]\n",
      "982 [D loss: 0.600085, acc.: 70.31%] [G loss: 0.938749]\n",
      "983 [D loss: 0.565392, acc.: 77.73%] [G loss: 0.937712]\n",
      "984 [D loss: 0.606578, acc.: 67.58%] [G loss: 0.952961]\n",
      "985 [D loss: 0.585628, acc.: 72.66%] [G loss: 0.973316]\n",
      "986 [D loss: 0.593960, acc.: 70.70%] [G loss: 0.957364]\n",
      "987 [D loss: 0.592457, acc.: 75.00%] [G loss: 0.939655]\n",
      "988 [D loss: 0.584980, acc.: 76.17%] [G loss: 0.893200]\n",
      "989 [D loss: 0.592970, acc.: 69.53%] [G loss: 0.911903]\n",
      "990 [D loss: 0.598910, acc.: 66.80%] [G loss: 0.948601]\n",
      "991 [D loss: 0.595364, acc.: 69.92%] [G loss: 0.967026]\n",
      "992 [D loss: 0.596776, acc.: 69.92%] [G loss: 0.975699]\n",
      "993 [D loss: 0.614027, acc.: 69.92%] [G loss: 0.901346]\n",
      "994 [D loss: 0.590640, acc.: 72.27%] [G loss: 0.906424]\n",
      "995 [D loss: 0.584547, acc.: 73.83%] [G loss: 0.941638]\n",
      "996 [D loss: 0.613079, acc.: 69.14%] [G loss: 0.934812]\n",
      "997 [D loss: 0.587829, acc.: 72.66%] [G loss: 0.952686]\n",
      "998 [D loss: 0.591345, acc.: 72.66%] [G loss: 0.943955]\n",
      "999 [D loss: 0.583665, acc.: 74.61%] [G loss: 0.929603]\n",
      "1000 [D loss: 0.586175, acc.: 71.09%] [G loss: 0.913184]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=1000, batch_size=128, sample_interval=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_ML",
   "language": "python",
   "name": "py_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
