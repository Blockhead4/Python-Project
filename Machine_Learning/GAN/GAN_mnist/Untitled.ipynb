{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 커닝횟수\n",
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Flatten, Dense, Reshape\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# 디스크리미네이터 \n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# 제너레이터\n",
    "generator = build_generator()\n",
    "\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "validity = discriminator(img)\n",
    "\n",
    "combined = Model(z, validity)\n",
    "combined.compile(loss='binary_crossentropy',\n",
    "                 optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "    \n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    img = Input(shape=img_shape)\n",
    "    valid = model(img)\n",
    "    \n",
    "    return Model(img, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size=128, sample_interval=50):\n",
    "    \n",
    "    # load data\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "    \n",
    "    # Rescale -1 - 1\n",
    "    X_train = X_train / 127.5 - 1\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "    \n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    for epoch in range(epochs+1):\n",
    "        \n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        \n",
    "        # 디스크리미네이터 학습\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        # 제너레이터 학습\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "        \n",
    "        print (\"%d [D Loss: %f, acc: %.2f%%] [G Loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "        \n",
    "        # save sample images\n",
    "        if epoch % sample_interval == 0:\n",
    "            sample_images(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    \n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "    \n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"test/%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D Loss: 0.033409, acc: 100.00%] [G Loss: 3.309580]\n",
      "1 [D Loss: 0.021302, acc: 100.00%] [G Loss: 3.301536]\n",
      "2 [D Loss: 0.029605, acc: 100.00%] [G Loss: 3.339492]\n",
      "3 [D Loss: 0.023999, acc: 100.00%] [G Loss: 3.397108]\n",
      "4 [D Loss: 0.021380, acc: 100.00%] [G Loss: 3.318574]\n",
      "5 [D Loss: 0.024138, acc: 100.00%] [G Loss: 3.361635]\n",
      "6 [D Loss: 0.019185, acc: 100.00%] [G Loss: 3.334551]\n",
      "7 [D Loss: 0.026881, acc: 100.00%] [G Loss: 3.622271]\n",
      "8 [D Loss: 0.026207, acc: 100.00%] [G Loss: 3.637349]\n",
      "9 [D Loss: 0.015165, acc: 100.00%] [G Loss: 3.781073]\n",
      "10 [D Loss: 0.019685, acc: 100.00%] [G Loss: 3.844564]\n",
      "11 [D Loss: 0.017735, acc: 100.00%] [G Loss: 3.824104]\n",
      "12 [D Loss: 0.018398, acc: 100.00%] [G Loss: 3.807186]\n",
      "13 [D Loss: 0.018256, acc: 100.00%] [G Loss: 3.779669]\n",
      "14 [D Loss: 0.018489, acc: 100.00%] [G Loss: 3.785824]\n",
      "15 [D Loss: 0.019120, acc: 100.00%] [G Loss: 3.871091]\n",
      "16 [D Loss: 0.018253, acc: 100.00%] [G Loss: 3.701205]\n",
      "17 [D Loss: 0.018436, acc: 100.00%] [G Loss: 3.812041]\n",
      "18 [D Loss: 0.012296, acc: 100.00%] [G Loss: 3.826574]\n",
      "19 [D Loss: 0.013984, acc: 100.00%] [G Loss: 3.885195]\n",
      "20 [D Loss: 0.020093, acc: 100.00%] [G Loss: 3.856557]\n",
      "21 [D Loss: 0.014634, acc: 100.00%] [G Loss: 3.892057]\n",
      "22 [D Loss: 0.016488, acc: 100.00%] [G Loss: 3.805614]\n",
      "23 [D Loss: 0.018064, acc: 100.00%] [G Loss: 4.159547]\n",
      "24 [D Loss: 0.016863, acc: 100.00%] [G Loss: 4.020465]\n",
      "25 [D Loss: 0.016585, acc: 100.00%] [G Loss: 4.019596]\n",
      "26 [D Loss: 0.017441, acc: 100.00%] [G Loss: 4.179653]\n",
      "27 [D Loss: 0.013036, acc: 100.00%] [G Loss: 4.083755]\n",
      "28 [D Loss: 0.012076, acc: 100.00%] [G Loss: 4.206224]\n",
      "29 [D Loss: 0.018817, acc: 100.00%] [G Loss: 4.307232]\n",
      "30 [D Loss: 0.017723, acc: 100.00%] [G Loss: 4.277745]\n",
      "31 [D Loss: 0.013824, acc: 100.00%] [G Loss: 4.403749]\n",
      "32 [D Loss: 0.013191, acc: 100.00%] [G Loss: 4.364161]\n",
      "33 [D Loss: 0.014175, acc: 100.00%] [G Loss: 4.250258]\n",
      "34 [D Loss: 0.012553, acc: 100.00%] [G Loss: 4.331268]\n",
      "35 [D Loss: 0.009909, acc: 100.00%] [G Loss: 4.470891]\n",
      "36 [D Loss: 0.008719, acc: 100.00%] [G Loss: 4.302220]\n",
      "37 [D Loss: 0.010682, acc: 100.00%] [G Loss: 4.203459]\n",
      "38 [D Loss: 0.012150, acc: 100.00%] [G Loss: 4.496881]\n",
      "39 [D Loss: 0.008352, acc: 100.00%] [G Loss: 4.389100]\n",
      "40 [D Loss: 0.014086, acc: 100.00%] [G Loss: 4.269448]\n",
      "41 [D Loss: 0.015438, acc: 100.00%] [G Loss: 4.394343]\n",
      "42 [D Loss: 0.012469, acc: 100.00%] [G Loss: 4.239575]\n",
      "43 [D Loss: 0.012885, acc: 100.00%] [G Loss: 4.537617]\n",
      "44 [D Loss: 0.012129, acc: 100.00%] [G Loss: 4.385071]\n",
      "45 [D Loss: 0.009717, acc: 100.00%] [G Loss: 4.161219]\n",
      "46 [D Loss: 0.010915, acc: 100.00%] [G Loss: 4.336415]\n",
      "47 [D Loss: 0.017787, acc: 100.00%] [G Loss: 4.501949]\n",
      "48 [D Loss: 0.015682, acc: 100.00%] [G Loss: 4.337197]\n",
      "49 [D Loss: 0.014070, acc: 100.00%] [G Loss: 4.616898]\n",
      "50 [D Loss: 0.009957, acc: 100.00%] [G Loss: 4.605408]\n",
      "51 [D Loss: 0.015495, acc: 100.00%] [G Loss: 4.640018]\n",
      "52 [D Loss: 0.012560, acc: 100.00%] [G Loss: 4.580356]\n",
      "53 [D Loss: 0.013516, acc: 100.00%] [G Loss: 4.529627]\n",
      "54 [D Loss: 0.015030, acc: 100.00%] [G Loss: 4.581904]\n",
      "55 [D Loss: 0.016557, acc: 100.00%] [G Loss: 4.666634]\n",
      "56 [D Loss: 0.010831, acc: 100.00%] [G Loss: 4.757598]\n",
      "57 [D Loss: 0.015500, acc: 100.00%] [G Loss: 4.652775]\n",
      "58 [D Loss: 0.014314, acc: 100.00%] [G Loss: 4.611086]\n",
      "59 [D Loss: 0.010024, acc: 100.00%] [G Loss: 4.737098]\n",
      "60 [D Loss: 0.014154, acc: 100.00%] [G Loss: 4.799176]\n",
      "61 [D Loss: 0.011648, acc: 100.00%] [G Loss: 4.781250]\n",
      "62 [D Loss: 0.009266, acc: 100.00%] [G Loss: 4.753233]\n",
      "63 [D Loss: 0.016046, acc: 100.00%] [G Loss: 4.690136]\n",
      "64 [D Loss: 0.010121, acc: 100.00%] [G Loss: 4.591781]\n",
      "65 [D Loss: 0.014643, acc: 100.00%] [G Loss: 4.612560]\n",
      "66 [D Loss: 0.019711, acc: 100.00%] [G Loss: 4.747950]\n",
      "67 [D Loss: 0.020528, acc: 100.00%] [G Loss: 4.650118]\n",
      "68 [D Loss: 0.012354, acc: 100.00%] [G Loss: 4.597525]\n",
      "69 [D Loss: 0.012922, acc: 100.00%] [G Loss: 4.587352]\n",
      "70 [D Loss: 0.017241, acc: 100.00%] [G Loss: 4.802089]\n",
      "71 [D Loss: 0.018001, acc: 100.00%] [G Loss: 5.100570]\n",
      "72 [D Loss: 0.079324, acc: 98.44%] [G Loss: 4.715171]\n",
      "73 [D Loss: 0.018457, acc: 100.00%] [G Loss: 5.120835]\n",
      "74 [D Loss: 0.027917, acc: 100.00%] [G Loss: 5.026968]\n",
      "75 [D Loss: 0.020260, acc: 100.00%] [G Loss: 4.895000]\n",
      "76 [D Loss: 0.016589, acc: 100.00%] [G Loss: 4.922578]\n",
      "77 [D Loss: 0.048140, acc: 100.00%] [G Loss: 5.179265]\n",
      "78 [D Loss: 0.019087, acc: 100.00%] [G Loss: 5.072645]\n",
      "79 [D Loss: 0.024372, acc: 100.00%] [G Loss: 4.997106]\n",
      "80 [D Loss: 0.054882, acc: 98.44%] [G Loss: 5.739658]\n",
      "81 [D Loss: 1.483042, acc: 50.00%] [G Loss: 4.130770]\n",
      "82 [D Loss: 1.360931, acc: 73.44%] [G Loss: 3.574822]\n",
      "83 [D Loss: 0.324108, acc: 84.38%] [G Loss: 3.216047]\n",
      "84 [D Loss: 0.164001, acc: 92.19%] [G Loss: 3.874228]\n",
      "85 [D Loss: 0.154827, acc: 89.06%] [G Loss: 4.155434]\n",
      "86 [D Loss: 0.047001, acc: 98.44%] [G Loss: 4.113505]\n",
      "87 [D Loss: 0.051221, acc: 98.44%] [G Loss: 4.199181]\n",
      "88 [D Loss: 0.058517, acc: 96.88%] [G Loss: 4.182292]\n",
      "89 [D Loss: 0.023433, acc: 100.00%] [G Loss: 4.203119]\n",
      "90 [D Loss: 0.052543, acc: 98.44%] [G Loss: 4.034121]\n",
      "91 [D Loss: 0.061646, acc: 100.00%] [G Loss: 4.003869]\n",
      "92 [D Loss: 0.044764, acc: 100.00%] [G Loss: 3.937093]\n",
      "93 [D Loss: 0.048653, acc: 100.00%] [G Loss: 3.692207]\n",
      "94 [D Loss: 0.094987, acc: 95.31%] [G Loss: 3.822356]\n",
      "95 [D Loss: 0.035506, acc: 100.00%] [G Loss: 3.721277]\n",
      "96 [D Loss: 0.065347, acc: 100.00%] [G Loss: 3.832404]\n",
      "97 [D Loss: 0.064762, acc: 100.00%] [G Loss: 3.823852]\n",
      "98 [D Loss: 0.048868, acc: 100.00%] [G Loss: 3.607439]\n",
      "99 [D Loss: 0.080131, acc: 96.88%] [G Loss: 3.348429]\n",
      "100 [D Loss: 0.093390, acc: 95.31%] [G Loss: 3.769201]\n",
      "101 [D Loss: 0.068100, acc: 98.44%] [G Loss: 3.841100]\n",
      "102 [D Loss: 0.129958, acc: 100.00%] [G Loss: 3.441647]\n",
      "103 [D Loss: 0.063106, acc: 100.00%] [G Loss: 3.693807]\n",
      "104 [D Loss: 0.056834, acc: 100.00%] [G Loss: 3.451605]\n",
      "105 [D Loss: 0.134497, acc: 96.88%] [G Loss: 3.680055]\n",
      "106 [D Loss: 0.069990, acc: 98.44%] [G Loss: 3.768447]\n",
      "107 [D Loss: 0.113496, acc: 95.31%] [G Loss: 3.599546]\n",
      "108 [D Loss: 0.067937, acc: 100.00%] [G Loss: 3.670289]\n",
      "109 [D Loss: 0.108027, acc: 96.88%] [G Loss: 3.445602]\n",
      "110 [D Loss: 0.155925, acc: 93.75%] [G Loss: 3.652227]\n",
      "111 [D Loss: 0.234616, acc: 90.62%] [G Loss: 3.843685]\n",
      "112 [D Loss: 0.050812, acc: 100.00%] [G Loss: 4.284566]\n",
      "113 [D Loss: 0.295950, acc: 84.38%] [G Loss: 3.916718]\n",
      "114 [D Loss: 0.060469, acc: 100.00%] [G Loss: 4.443573]\n",
      "115 [D Loss: 0.115557, acc: 95.31%] [G Loss: 4.556570]\n",
      "116 [D Loss: 0.200273, acc: 93.75%] [G Loss: 3.167478]\n",
      "117 [D Loss: 0.143161, acc: 93.75%] [G Loss: 3.941541]\n",
      "118 [D Loss: 0.073302, acc: 95.31%] [G Loss: 4.339337]\n",
      "119 [D Loss: 0.101777, acc: 100.00%] [G Loss: 3.629768]\n",
      "120 [D Loss: 0.122537, acc: 92.19%] [G Loss: 4.272765]\n",
      "121 [D Loss: 0.168900, acc: 93.75%] [G Loss: 4.079641]\n",
      "122 [D Loss: 0.079837, acc: 96.88%] [G Loss: 3.992744]\n",
      "123 [D Loss: 0.243912, acc: 90.62%] [G Loss: 3.543585]\n",
      "124 [D Loss: 0.119709, acc: 96.88%] [G Loss: 3.738238]\n",
      "125 [D Loss: 0.447982, acc: 85.94%] [G Loss: 5.672841]\n",
      "126 [D Loss: 2.578055, acc: 14.06%] [G Loss: 3.092499]\n",
      "127 [D Loss: 1.920898, acc: 62.50%] [G Loss: 1.523669]\n",
      "128 [D Loss: 0.998051, acc: 57.81%] [G Loss: 1.705924]\n",
      "129 [D Loss: 0.169631, acc: 90.62%] [G Loss: 2.819120]\n",
      "130 [D Loss: 0.125532, acc: 93.75%] [G Loss: 3.344333]\n",
      "131 [D Loss: 0.079082, acc: 98.44%] [G Loss: 3.596025]\n",
      "132 [D Loss: 0.094283, acc: 98.44%] [G Loss: 3.720745]\n",
      "133 [D Loss: 0.113955, acc: 98.44%] [G Loss: 3.370987]\n",
      "134 [D Loss: 0.116185, acc: 93.75%] [G Loss: 3.675122]\n",
      "135 [D Loss: 0.122320, acc: 98.44%] [G Loss: 3.397124]\n",
      "136 [D Loss: 0.131600, acc: 98.44%] [G Loss: 2.701762]\n",
      "137 [D Loss: 0.146135, acc: 93.75%] [G Loss: 3.030425]\n",
      "138 [D Loss: 0.201927, acc: 95.31%] [G Loss: 3.638808]\n",
      "139 [D Loss: 0.206862, acc: 95.31%] [G Loss: 3.039174]\n",
      "140 [D Loss: 0.133393, acc: 96.88%] [G Loss: 3.010320]\n",
      "141 [D Loss: 0.194179, acc: 95.31%] [G Loss: 3.256998]\n",
      "142 [D Loss: 0.228151, acc: 90.62%] [G Loss: 3.132822]\n",
      "143 [D Loss: 0.180199, acc: 95.31%] [G Loss: 3.289845]\n",
      "144 [D Loss: 0.241765, acc: 90.62%] [G Loss: 2.910428]\n",
      "145 [D Loss: 0.120053, acc: 100.00%] [G Loss: 2.896535]\n",
      "146 [D Loss: 0.230695, acc: 87.50%] [G Loss: 3.347050]\n",
      "147 [D Loss: 0.222158, acc: 90.62%] [G Loss: 2.942005]\n",
      "148 [D Loss: 0.215571, acc: 93.75%] [G Loss: 3.223504]\n",
      "149 [D Loss: 0.446502, acc: 78.12%] [G Loss: 2.022439]\n",
      "150 [D Loss: 0.164583, acc: 93.75%] [G Loss: 2.857379]\n",
      "151 [D Loss: 0.181836, acc: 96.88%] [G Loss: 2.986189]\n",
      "152 [D Loss: 0.170927, acc: 95.31%] [G Loss: 3.164334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 [D Loss: 0.158126, acc: 98.44%] [G Loss: 2.903035]\n",
      "154 [D Loss: 0.161429, acc: 95.31%] [G Loss: 3.376452]\n",
      "155 [D Loss: 0.713937, acc: 64.06%] [G Loss: 2.263670]\n",
      "156 [D Loss: 0.090610, acc: 100.00%] [G Loss: 2.778209]\n",
      "157 [D Loss: 0.250919, acc: 92.19%] [G Loss: 3.056540]\n",
      "158 [D Loss: 0.183066, acc: 95.31%] [G Loss: 3.164448]\n",
      "159 [D Loss: 0.191218, acc: 95.31%] [G Loss: 3.142403]\n",
      "160 [D Loss: 0.190620, acc: 95.31%] [G Loss: 3.358222]\n",
      "161 [D Loss: 0.332386, acc: 85.94%] [G Loss: 2.731795]\n",
      "162 [D Loss: 0.213825, acc: 89.06%] [G Loss: 3.356716]\n",
      "163 [D Loss: 0.478442, acc: 76.56%] [G Loss: 2.346675]\n",
      "164 [D Loss: 0.168844, acc: 92.19%] [G Loss: 3.767096]\n",
      "165 [D Loss: 0.340462, acc: 89.06%] [G Loss: 2.759969]\n",
      "166 [D Loss: 0.170278, acc: 95.31%] [G Loss: 3.465093]\n",
      "167 [D Loss: 0.356531, acc: 79.69%] [G Loss: 3.397645]\n",
      "168 [D Loss: 0.435925, acc: 79.69%] [G Loss: 2.347059]\n",
      "169 [D Loss: 0.131914, acc: 95.31%] [G Loss: 3.422585]\n",
      "170 [D Loss: 0.516489, acc: 70.31%] [G Loss: 2.877567]\n",
      "171 [D Loss: 0.215925, acc: 90.62%] [G Loss: 3.987805]\n",
      "172 [D Loss: 1.120290, acc: 48.44%] [G Loss: 1.549838]\n",
      "173 [D Loss: 0.285259, acc: 84.38%] [G Loss: 3.006471]\n",
      "174 [D Loss: 0.145602, acc: 100.00%] [G Loss: 3.347919]\n",
      "175 [D Loss: 0.462145, acc: 75.00%] [G Loss: 2.565362]\n",
      "176 [D Loss: 0.224122, acc: 92.19%] [G Loss: 3.374554]\n",
      "177 [D Loss: 0.253998, acc: 90.62%] [G Loss: 2.977548]\n",
      "178 [D Loss: 0.499170, acc: 68.75%] [G Loss: 2.210624]\n",
      "179 [D Loss: 0.256849, acc: 85.94%] [G Loss: 3.748622]\n",
      "180 [D Loss: 0.449555, acc: 85.94%] [G Loss: 2.116439]\n",
      "181 [D Loss: 0.191566, acc: 89.06%] [G Loss: 3.415144]\n",
      "182 [D Loss: 0.196072, acc: 93.75%] [G Loss: 2.960624]\n",
      "183 [D Loss: 0.202621, acc: 95.31%] [G Loss: 2.790584]\n",
      "184 [D Loss: 0.405686, acc: 73.44%] [G Loss: 3.308720]\n",
      "185 [D Loss: 0.664007, acc: 62.50%] [G Loss: 3.183003]\n",
      "186 [D Loss: 0.309874, acc: 85.94%] [G Loss: 3.174970]\n",
      "187 [D Loss: 0.399336, acc: 84.38%] [G Loss: 2.182912]\n",
      "188 [D Loss: 0.327736, acc: 81.25%] [G Loss: 4.128589]\n",
      "189 [D Loss: 0.883341, acc: 46.88%] [G Loss: 1.689393]\n",
      "190 [D Loss: 0.300977, acc: 87.50%] [G Loss: 2.924492]\n",
      "191 [D Loss: 0.210224, acc: 95.31%] [G Loss: 3.539844]\n",
      "192 [D Loss: 0.487844, acc: 75.00%] [G Loss: 2.667954]\n",
      "193 [D Loss: 0.250196, acc: 92.19%] [G Loss: 3.187509]\n",
      "194 [D Loss: 0.585131, acc: 64.06%] [G Loss: 1.989333]\n",
      "195 [D Loss: 0.202756, acc: 89.06%] [G Loss: 3.121954]\n",
      "196 [D Loss: 0.443656, acc: 78.12%] [G Loss: 1.939009]\n",
      "197 [D Loss: 0.258596, acc: 90.62%] [G Loss: 3.192149]\n",
      "198 [D Loss: 0.868704, acc: 50.00%] [G Loss: 1.575821]\n",
      "199 [D Loss: 0.258206, acc: 84.38%] [G Loss: 3.201452]\n",
      "200 [D Loss: 0.548456, acc: 76.56%] [G Loss: 2.400508]\n",
      "201 [D Loss: 0.257493, acc: 89.06%] [G Loss: 2.966094]\n",
      "202 [D Loss: 0.603928, acc: 70.31%] [G Loss: 2.973225]\n",
      "203 [D Loss: 0.433408, acc: 79.69%] [G Loss: 2.700322]\n",
      "204 [D Loss: 0.528286, acc: 75.00%] [G Loss: 2.439819]\n",
      "205 [D Loss: 0.542351, acc: 75.00%] [G Loss: 2.764212]\n",
      "206 [D Loss: 0.676469, acc: 60.94%] [G Loss: 2.133790]\n",
      "207 [D Loss: 0.353373, acc: 82.81%] [G Loss: 2.731637]\n",
      "208 [D Loss: 1.024898, acc: 37.50%] [G Loss: 1.580263]\n",
      "209 [D Loss: 0.319211, acc: 87.50%] [G Loss: 2.994546]\n",
      "210 [D Loss: 1.038838, acc: 42.19%] [G Loss: 1.407757]\n",
      "211 [D Loss: 0.354732, acc: 78.12%] [G Loss: 2.740252]\n",
      "212 [D Loss: 0.598649, acc: 67.19%] [G Loss: 1.404725]\n",
      "213 [D Loss: 0.421402, acc: 78.12%] [G Loss: 2.168313]\n",
      "214 [D Loss: 0.647847, acc: 62.50%] [G Loss: 2.411074]\n",
      "215 [D Loss: 0.560411, acc: 68.75%] [G Loss: 1.817895]\n",
      "216 [D Loss: 0.441985, acc: 73.44%] [G Loss: 2.296100]\n",
      "217 [D Loss: 0.603842, acc: 65.62%] [G Loss: 1.795367]\n",
      "218 [D Loss: 0.510196, acc: 79.69%] [G Loss: 2.206952]\n",
      "219 [D Loss: 0.934249, acc: 43.75%] [G Loss: 1.879419]\n",
      "220 [D Loss: 0.402416, acc: 82.81%] [G Loss: 2.591082]\n",
      "221 [D Loss: 0.951202, acc: 39.06%] [G Loss: 1.388644]\n",
      "222 [D Loss: 0.516041, acc: 68.75%] [G Loss: 2.241593]\n",
      "223 [D Loss: 0.683777, acc: 59.38%] [G Loss: 1.674134]\n",
      "224 [D Loss: 0.503012, acc: 73.44%] [G Loss: 2.203730]\n",
      "225 [D Loss: 0.557874, acc: 68.75%] [G Loss: 1.977447]\n",
      "226 [D Loss: 0.718382, acc: 48.44%] [G Loss: 1.667523]\n",
      "227 [D Loss: 0.546790, acc: 67.19%] [G Loss: 2.102066]\n",
      "228 [D Loss: 0.809867, acc: 45.31%] [G Loss: 1.475822]\n",
      "229 [D Loss: 0.516347, acc: 71.88%] [G Loss: 2.163571]\n",
      "230 [D Loss: 0.736517, acc: 59.38%] [G Loss: 1.448704]\n",
      "231 [D Loss: 0.602202, acc: 62.50%] [G Loss: 1.993656]\n",
      "232 [D Loss: 0.648489, acc: 57.81%] [G Loss: 1.858527]\n",
      "233 [D Loss: 0.784274, acc: 50.00%] [G Loss: 1.350907]\n",
      "234 [D Loss: 0.485720, acc: 73.44%] [G Loss: 2.332540]\n",
      "235 [D Loss: 0.982142, acc: 29.69%] [G Loss: 1.058146]\n",
      "236 [D Loss: 0.501078, acc: 65.62%] [G Loss: 1.748546]\n",
      "237 [D Loss: 0.655729, acc: 54.69%] [G Loss: 1.498595]\n",
      "238 [D Loss: 0.648791, acc: 60.94%] [G Loss: 1.532408]\n",
      "239 [D Loss: 0.532061, acc: 70.31%] [G Loss: 1.723626]\n",
      "240 [D Loss: 0.568845, acc: 64.06%] [G Loss: 1.404611]\n",
      "241 [D Loss: 0.698550, acc: 51.56%] [G Loss: 1.292512]\n",
      "242 [D Loss: 0.633431, acc: 56.25%] [G Loss: 1.424199]\n",
      "243 [D Loss: 0.683585, acc: 51.56%] [G Loss: 1.401611]\n",
      "244 [D Loss: 0.903317, acc: 40.62%] [G Loss: 1.044938]\n",
      "245 [D Loss: 0.843954, acc: 39.06%] [G Loss: 0.999229]\n",
      "246 [D Loss: 0.613413, acc: 57.81%] [G Loss: 1.393539]\n",
      "247 [D Loss: 0.959002, acc: 31.25%] [G Loss: 0.795178]\n",
      "248 [D Loss: 0.675169, acc: 53.12%] [G Loss: 1.284294]\n",
      "249 [D Loss: 0.820052, acc: 42.19%] [G Loss: 1.004612]\n",
      "250 [D Loss: 0.764402, acc: 43.75%] [G Loss: 0.859415]\n",
      "251 [D Loss: 0.655667, acc: 54.69%] [G Loss: 1.182949]\n",
      "252 [D Loss: 0.904437, acc: 34.38%] [G Loss: 0.838341]\n",
      "253 [D Loss: 0.767704, acc: 39.06%] [G Loss: 0.893147]\n",
      "254 [D Loss: 0.728372, acc: 45.31%] [G Loss: 0.900288]\n",
      "255 [D Loss: 0.690438, acc: 48.44%] [G Loss: 1.024592]\n",
      "256 [D Loss: 0.708684, acc: 54.69%] [G Loss: 1.011236]\n",
      "257 [D Loss: 0.740557, acc: 45.31%] [G Loss: 0.854487]\n",
      "258 [D Loss: 0.701140, acc: 45.31%] [G Loss: 0.871525]\n",
      "259 [D Loss: 0.769315, acc: 39.06%] [G Loss: 0.759155]\n",
      "260 [D Loss: 0.726425, acc: 50.00%] [G Loss: 0.808973]\n",
      "261 [D Loss: 0.737234, acc: 45.31%] [G Loss: 0.822658]\n",
      "262 [D Loss: 0.745129, acc: 42.19%] [G Loss: 0.770446]\n",
      "263 [D Loss: 0.747180, acc: 48.44%] [G Loss: 0.807726]\n",
      "264 [D Loss: 0.777164, acc: 35.94%] [G Loss: 0.708520]\n",
      "265 [D Loss: 0.748733, acc: 42.19%] [G Loss: 0.753826]\n",
      "266 [D Loss: 0.736637, acc: 46.88%] [G Loss: 0.766230]\n",
      "267 [D Loss: 0.764843, acc: 43.75%] [G Loss: 0.739194]\n",
      "268 [D Loss: 0.739881, acc: 45.31%] [G Loss: 0.722853]\n",
      "269 [D Loss: 0.720907, acc: 46.88%] [G Loss: 0.711461]\n",
      "270 [D Loss: 0.715598, acc: 51.56%] [G Loss: 0.680983]\n",
      "271 [D Loss: 0.684907, acc: 48.44%] [G Loss: 0.745554]\n",
      "272 [D Loss: 0.773153, acc: 40.62%] [G Loss: 0.723593]\n",
      "273 [D Loss: 0.685505, acc: 53.12%] [G Loss: 0.725884]\n",
      "274 [D Loss: 0.756401, acc: 45.31%] [G Loss: 0.651295]\n",
      "275 [D Loss: 0.766424, acc: 39.06%] [G Loss: 0.681855]\n",
      "276 [D Loss: 0.692204, acc: 46.88%] [G Loss: 0.742313]\n",
      "277 [D Loss: 0.716995, acc: 40.62%] [G Loss: 0.728872]\n",
      "278 [D Loss: 0.692173, acc: 48.44%] [G Loss: 0.720130]\n",
      "279 [D Loss: 0.676458, acc: 48.44%] [G Loss: 0.726045]\n",
      "280 [D Loss: 0.706671, acc: 43.75%] [G Loss: 0.764340]\n",
      "281 [D Loss: 0.704361, acc: 45.31%] [G Loss: 0.727434]\n",
      "282 [D Loss: 0.705055, acc: 46.88%] [G Loss: 0.701593]\n",
      "283 [D Loss: 0.703482, acc: 46.88%] [G Loss: 0.694842]\n",
      "284 [D Loss: 0.674592, acc: 45.31%] [G Loss: 0.774846]\n",
      "285 [D Loss: 0.678734, acc: 51.56%] [G Loss: 0.750777]\n",
      "286 [D Loss: 0.696933, acc: 48.44%] [G Loss: 0.741804]\n",
      "287 [D Loss: 0.724486, acc: 39.06%] [G Loss: 0.678314]\n",
      "288 [D Loss: 0.717182, acc: 45.31%] [G Loss: 0.657316]\n",
      "289 [D Loss: 0.690013, acc: 50.00%] [G Loss: 0.688831]\n",
      "290 [D Loss: 0.708345, acc: 42.19%] [G Loss: 0.684164]\n",
      "291 [D Loss: 0.702594, acc: 45.31%] [G Loss: 0.688472]\n",
      "292 [D Loss: 0.684217, acc: 43.75%] [G Loss: 0.703428]\n",
      "293 [D Loss: 0.705295, acc: 43.75%] [G Loss: 0.707572]\n",
      "294 [D Loss: 0.721233, acc: 39.06%] [G Loss: 0.681108]\n",
      "295 [D Loss: 0.694992, acc: 48.44%] [G Loss: 0.669433]\n",
      "296 [D Loss: 0.733715, acc: 37.50%] [G Loss: 0.623024]\n",
      "297 [D Loss: 0.706878, acc: 48.44%] [G Loss: 0.625163]\n",
      "298 [D Loss: 0.678999, acc: 48.44%] [G Loss: 0.633130]\n",
      "299 [D Loss: 0.682993, acc: 46.88%] [G Loss: 0.664032]\n",
      "300 [D Loss: 0.683393, acc: 45.31%] [G Loss: 0.665151]\n",
      "301 [D Loss: 0.725116, acc: 40.62%] [G Loss: 0.646580]\n",
      "302 [D Loss: 0.688464, acc: 46.88%] [G Loss: 0.658065]\n",
      "303 [D Loss: 0.687626, acc: 43.75%] [G Loss: 0.663699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 [D Loss: 0.686387, acc: 43.75%] [G Loss: 0.639944]\n",
      "305 [D Loss: 0.676167, acc: 45.31%] [G Loss: 0.645656]\n",
      "306 [D Loss: 0.674595, acc: 45.31%] [G Loss: 0.684117]\n",
      "307 [D Loss: 0.683564, acc: 46.88%] [G Loss: 0.696172]\n",
      "308 [D Loss: 0.708746, acc: 42.19%] [G Loss: 0.689898]\n",
      "309 [D Loss: 0.710299, acc: 43.75%] [G Loss: 0.665723]\n",
      "310 [D Loss: 0.724590, acc: 40.62%] [G Loss: 0.653645]\n",
      "311 [D Loss: 0.677458, acc: 48.44%] [G Loss: 0.652895]\n",
      "312 [D Loss: 0.668118, acc: 45.31%] [G Loss: 0.655281]\n",
      "313 [D Loss: 0.686933, acc: 43.75%] [G Loss: 0.639076]\n",
      "314 [D Loss: 0.674347, acc: 46.88%] [G Loss: 0.636933]\n",
      "315 [D Loss: 0.667903, acc: 45.31%] [G Loss: 0.655167]\n",
      "316 [D Loss: 0.674041, acc: 50.00%] [G Loss: 0.657262]\n",
      "317 [D Loss: 0.661047, acc: 46.88%] [G Loss: 0.653554]\n",
      "318 [D Loss: 0.664156, acc: 50.00%] [G Loss: 0.653386]\n",
      "319 [D Loss: 0.658805, acc: 50.00%] [G Loss: 0.645821]\n",
      "320 [D Loss: 0.687781, acc: 45.31%] [G Loss: 0.650745]\n",
      "321 [D Loss: 0.656116, acc: 46.88%] [G Loss: 0.659017]\n",
      "322 [D Loss: 0.684062, acc: 46.88%] [G Loss: 0.659859]\n",
      "323 [D Loss: 0.658261, acc: 51.56%] [G Loss: 0.658359]\n",
      "324 [D Loss: 0.654368, acc: 50.00%] [G Loss: 0.659373]\n",
      "325 [D Loss: 0.680865, acc: 46.88%] [G Loss: 0.665205]\n",
      "326 [D Loss: 0.665951, acc: 48.44%] [G Loss: 0.662456]\n",
      "327 [D Loss: 0.657240, acc: 50.00%] [G Loss: 0.644430]\n",
      "328 [D Loss: 0.664020, acc: 51.56%] [G Loss: 0.678273]\n",
      "329 [D Loss: 0.666255, acc: 50.00%] [G Loss: 0.685797]\n",
      "330 [D Loss: 0.652108, acc: 48.44%] [G Loss: 0.676470]\n",
      "331 [D Loss: 0.669484, acc: 45.31%] [G Loss: 0.674307]\n",
      "332 [D Loss: 0.648547, acc: 50.00%] [G Loss: 0.667777]\n",
      "333 [D Loss: 0.653131, acc: 48.44%] [G Loss: 0.686078]\n",
      "334 [D Loss: 0.648163, acc: 50.00%] [G Loss: 0.683952]\n",
      "335 [D Loss: 0.648573, acc: 50.00%] [G Loss: 0.707162]\n",
      "336 [D Loss: 0.643862, acc: 53.12%] [G Loss: 0.735564]\n",
      "337 [D Loss: 0.646969, acc: 53.12%] [G Loss: 0.718701]\n",
      "338 [D Loss: 0.641429, acc: 54.69%] [G Loss: 0.727296]\n",
      "339 [D Loss: 0.653512, acc: 51.56%] [G Loss: 0.689790]\n",
      "340 [D Loss: 0.665774, acc: 48.44%] [G Loss: 0.663276]\n",
      "341 [D Loss: 0.638036, acc: 51.56%] [G Loss: 0.665482]\n",
      "342 [D Loss: 0.629555, acc: 57.81%] [G Loss: 0.704113]\n",
      "343 [D Loss: 0.625974, acc: 60.94%] [G Loss: 0.715917]\n",
      "344 [D Loss: 0.654204, acc: 45.31%] [G Loss: 0.699417]\n",
      "345 [D Loss: 0.649052, acc: 59.38%] [G Loss: 0.708297]\n",
      "346 [D Loss: 0.651423, acc: 54.69%] [G Loss: 0.689360]\n",
      "347 [D Loss: 0.657533, acc: 48.44%] [G Loss: 0.671972]\n",
      "348 [D Loss: 0.691647, acc: 43.75%] [G Loss: 0.683559]\n",
      "349 [D Loss: 0.645664, acc: 48.44%] [G Loss: 0.696051]\n",
      "350 [D Loss: 0.648184, acc: 45.31%] [G Loss: 0.696312]\n",
      "351 [D Loss: 0.659505, acc: 46.88%] [G Loss: 0.699999]\n",
      "352 [D Loss: 0.686108, acc: 43.75%] [G Loss: 0.694979]\n",
      "353 [D Loss: 0.642259, acc: 51.56%] [G Loss: 0.721382]\n",
      "354 [D Loss: 0.628182, acc: 59.38%] [G Loss: 0.748063]\n",
      "355 [D Loss: 0.667391, acc: 43.75%] [G Loss: 0.709316]\n",
      "356 [D Loss: 0.665012, acc: 43.75%] [G Loss: 0.708516]\n",
      "357 [D Loss: 0.662182, acc: 45.31%] [G Loss: 0.689475]\n",
      "358 [D Loss: 0.660526, acc: 45.31%] [G Loss: 0.698451]\n",
      "359 [D Loss: 0.635177, acc: 50.00%] [G Loss: 0.718261]\n",
      "360 [D Loss: 0.675864, acc: 50.00%] [G Loss: 0.698433]\n",
      "361 [D Loss: 0.627579, acc: 59.38%] [G Loss: 0.726656]\n",
      "362 [D Loss: 0.654910, acc: 51.56%] [G Loss: 0.729806]\n",
      "363 [D Loss: 0.647746, acc: 48.44%] [G Loss: 0.715105]\n",
      "364 [D Loss: 0.665564, acc: 46.88%] [G Loss: 0.716135]\n",
      "365 [D Loss: 0.651834, acc: 48.44%] [G Loss: 0.692269]\n",
      "366 [D Loss: 0.646879, acc: 54.69%] [G Loss: 0.689288]\n",
      "367 [D Loss: 0.661001, acc: 59.38%] [G Loss: 0.713972]\n",
      "368 [D Loss: 0.663615, acc: 51.56%] [G Loss: 0.698010]\n",
      "369 [D Loss: 0.679114, acc: 46.88%] [G Loss: 0.669190]\n",
      "370 [D Loss: 0.654229, acc: 45.31%] [G Loss: 0.688831]\n",
      "371 [D Loss: 0.666238, acc: 48.44%] [G Loss: 0.689731]\n",
      "372 [D Loss: 0.658342, acc: 53.12%] [G Loss: 0.683091]\n",
      "373 [D Loss: 0.645784, acc: 51.56%] [G Loss: 0.702480]\n",
      "374 [D Loss: 0.640965, acc: 51.56%] [G Loss: 0.699533]\n",
      "375 [D Loss: 0.660202, acc: 54.69%] [G Loss: 0.715851]\n",
      "376 [D Loss: 0.665080, acc: 50.00%] [G Loss: 0.701645]\n",
      "377 [D Loss: 0.675899, acc: 46.88%] [G Loss: 0.677341]\n",
      "378 [D Loss: 0.638615, acc: 53.12%] [G Loss: 0.682436]\n",
      "379 [D Loss: 0.658809, acc: 51.56%] [G Loss: 0.691455]\n",
      "380 [D Loss: 0.684072, acc: 53.12%] [G Loss: 0.682862]\n",
      "381 [D Loss: 0.665633, acc: 50.00%] [G Loss: 0.674338]\n",
      "382 [D Loss: 0.688870, acc: 48.44%] [G Loss: 0.671870]\n",
      "383 [D Loss: 0.642784, acc: 53.12%] [G Loss: 0.685288]\n",
      "384 [D Loss: 0.636606, acc: 57.81%] [G Loss: 0.705998]\n",
      "385 [D Loss: 0.644867, acc: 56.25%] [G Loss: 0.714554]\n",
      "386 [D Loss: 0.673089, acc: 53.12%] [G Loss: 0.696190]\n",
      "387 [D Loss: 0.658704, acc: 50.00%] [G Loss: 0.706091]\n",
      "388 [D Loss: 0.621962, acc: 56.25%] [G Loss: 0.707856]\n",
      "389 [D Loss: 0.653389, acc: 46.88%] [G Loss: 0.725995]\n",
      "390 [D Loss: 0.658379, acc: 51.56%] [G Loss: 0.701500]\n",
      "391 [D Loss: 0.639653, acc: 50.00%] [G Loss: 0.698772]\n",
      "392 [D Loss: 0.621636, acc: 51.56%] [G Loss: 0.718156]\n",
      "393 [D Loss: 0.630724, acc: 60.94%] [G Loss: 0.734883]\n",
      "394 [D Loss: 0.629976, acc: 57.81%] [G Loss: 0.735690]\n",
      "395 [D Loss: 0.627348, acc: 64.06%] [G Loss: 0.717504]\n",
      "396 [D Loss: 0.644570, acc: 56.25%] [G Loss: 0.706885]\n",
      "397 [D Loss: 0.649117, acc: 53.12%] [G Loss: 0.690511]\n",
      "398 [D Loss: 0.642199, acc: 51.56%] [G Loss: 0.689095]\n",
      "399 [D Loss: 0.634702, acc: 53.12%] [G Loss: 0.718219]\n",
      "400 [D Loss: 0.625728, acc: 57.81%] [G Loss: 0.703712]\n",
      "401 [D Loss: 0.649627, acc: 53.12%] [G Loss: 0.696627]\n",
      "402 [D Loss: 0.640769, acc: 46.88%] [G Loss: 0.692331]\n",
      "403 [D Loss: 0.650720, acc: 54.69%] [G Loss: 0.698011]\n",
      "404 [D Loss: 0.633841, acc: 50.00%] [G Loss: 0.691617]\n",
      "405 [D Loss: 0.628281, acc: 53.12%] [G Loss: 0.700535]\n",
      "406 [D Loss: 0.648407, acc: 51.56%] [G Loss: 0.695345]\n",
      "407 [D Loss: 0.642364, acc: 53.12%] [G Loss: 0.707935]\n",
      "408 [D Loss: 0.667736, acc: 50.00%] [G Loss: 0.708556]\n",
      "409 [D Loss: 0.653078, acc: 51.56%] [G Loss: 0.741071]\n",
      "410 [D Loss: 0.665596, acc: 53.12%] [G Loss: 0.677953]\n",
      "411 [D Loss: 0.689167, acc: 48.44%] [G Loss: 0.683388]\n",
      "412 [D Loss: 0.641568, acc: 53.12%] [G Loss: 0.680201]\n",
      "413 [D Loss: 0.629502, acc: 51.56%] [G Loss: 0.679288]\n",
      "414 [D Loss: 0.625482, acc: 56.25%] [G Loss: 0.711035]\n",
      "415 [D Loss: 0.620039, acc: 64.06%] [G Loss: 0.730629]\n",
      "416 [D Loss: 0.650845, acc: 65.62%] [G Loss: 0.749107]\n",
      "417 [D Loss: 0.643756, acc: 59.38%] [G Loss: 0.745716]\n",
      "418 [D Loss: 0.639667, acc: 53.12%] [G Loss: 0.729654]\n",
      "419 [D Loss: 0.664215, acc: 60.94%] [G Loss: 0.722417]\n",
      "420 [D Loss: 0.651663, acc: 54.69%] [G Loss: 0.723106]\n",
      "421 [D Loss: 0.635172, acc: 67.19%] [G Loss: 0.713435]\n",
      "422 [D Loss: 0.670966, acc: 53.12%] [G Loss: 0.720613]\n",
      "423 [D Loss: 0.623807, acc: 62.50%] [G Loss: 0.745807]\n",
      "424 [D Loss: 0.620275, acc: 60.94%] [G Loss: 0.757713]\n",
      "425 [D Loss: 0.635494, acc: 67.19%] [G Loss: 0.712920]\n",
      "426 [D Loss: 0.647560, acc: 51.56%] [G Loss: 0.706137]\n",
      "427 [D Loss: 0.622217, acc: 59.38%] [G Loss: 0.714649]\n",
      "428 [D Loss: 0.632734, acc: 54.69%] [G Loss: 0.728018]\n",
      "429 [D Loss: 0.655350, acc: 53.12%] [G Loss: 0.740679]\n",
      "430 [D Loss: 0.629068, acc: 67.19%] [G Loss: 0.719594]\n",
      "431 [D Loss: 0.663510, acc: 48.44%] [G Loss: 0.705045]\n",
      "432 [D Loss: 0.616709, acc: 59.38%] [G Loss: 0.720443]\n",
      "433 [D Loss: 0.628340, acc: 62.50%] [G Loss: 0.719042]\n",
      "434 [D Loss: 0.644381, acc: 57.81%] [G Loss: 0.722497]\n",
      "435 [D Loss: 0.634818, acc: 57.81%] [G Loss: 0.727035]\n",
      "436 [D Loss: 0.650134, acc: 56.25%] [G Loss: 0.711919]\n",
      "437 [D Loss: 0.631036, acc: 57.81%] [G Loss: 0.707369]\n",
      "438 [D Loss: 0.653986, acc: 53.12%] [G Loss: 0.703830]\n",
      "439 [D Loss: 0.624898, acc: 57.81%] [G Loss: 0.727532]\n",
      "440 [D Loss: 0.645123, acc: 57.81%] [G Loss: 0.721192]\n",
      "441 [D Loss: 0.627861, acc: 59.38%] [G Loss: 0.702382]\n",
      "442 [D Loss: 0.644916, acc: 57.81%] [G Loss: 0.722321]\n",
      "443 [D Loss: 0.615292, acc: 60.94%] [G Loss: 0.724359]\n",
      "444 [D Loss: 0.655317, acc: 62.50%] [G Loss: 0.706349]\n",
      "445 [D Loss: 0.649611, acc: 56.25%] [G Loss: 0.715561]\n",
      "446 [D Loss: 0.635406, acc: 53.12%] [G Loss: 0.688777]\n",
      "447 [D Loss: 0.648868, acc: 51.56%] [G Loss: 0.720016]\n",
      "448 [D Loss: 0.624235, acc: 62.50%] [G Loss: 0.722025]\n",
      "449 [D Loss: 0.612096, acc: 60.94%] [G Loss: 0.736482]\n",
      "450 [D Loss: 0.659682, acc: 54.69%] [G Loss: 0.725987]\n",
      "451 [D Loss: 0.635805, acc: 56.25%] [G Loss: 0.729160]\n",
      "452 [D Loss: 0.606326, acc: 75.00%] [G Loss: 0.752792]\n",
      "453 [D Loss: 0.637993, acc: 59.38%] [G Loss: 0.761081]\n",
      "454 [D Loss: 0.634351, acc: 56.25%] [G Loss: 0.725486]\n",
      "455 [D Loss: 0.622331, acc: 60.94%] [G Loss: 0.730491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456 [D Loss: 0.619222, acc: 60.94%] [G Loss: 0.737120]\n",
      "457 [D Loss: 0.605630, acc: 65.62%] [G Loss: 0.747916]\n",
      "458 [D Loss: 0.626693, acc: 62.50%] [G Loss: 0.741659]\n",
      "459 [D Loss: 0.665468, acc: 56.25%] [G Loss: 0.750807]\n",
      "460 [D Loss: 0.597492, acc: 75.00%] [G Loss: 0.738302]\n",
      "461 [D Loss: 0.619226, acc: 65.62%] [G Loss: 0.699757]\n",
      "462 [D Loss: 0.644586, acc: 60.94%] [G Loss: 0.723957]\n",
      "463 [D Loss: 0.621307, acc: 64.06%] [G Loss: 0.730337]\n",
      "464 [D Loss: 0.595826, acc: 62.50%] [G Loss: 0.719373]\n",
      "465 [D Loss: 0.620041, acc: 59.38%] [G Loss: 0.733035]\n",
      "466 [D Loss: 0.614468, acc: 60.94%] [G Loss: 0.763507]\n",
      "467 [D Loss: 0.616673, acc: 71.88%] [G Loss: 0.751409]\n",
      "468 [D Loss: 0.620091, acc: 68.75%] [G Loss: 0.725484]\n",
      "469 [D Loss: 0.625304, acc: 60.94%] [G Loss: 0.737226]\n",
      "470 [D Loss: 0.624186, acc: 62.50%] [G Loss: 0.763890]\n",
      "471 [D Loss: 0.642183, acc: 62.50%] [G Loss: 0.753312]\n",
      "472 [D Loss: 0.627210, acc: 67.19%] [G Loss: 0.700467]\n",
      "473 [D Loss: 0.610349, acc: 60.94%] [G Loss: 0.706319]\n",
      "474 [D Loss: 0.645185, acc: 62.50%] [G Loss: 0.730380]\n",
      "475 [D Loss: 0.631219, acc: 64.06%] [G Loss: 0.751853]\n",
      "476 [D Loss: 0.652067, acc: 59.38%] [G Loss: 0.734049]\n",
      "477 [D Loss: 0.642197, acc: 62.50%] [G Loss: 0.728029]\n",
      "478 [D Loss: 0.646409, acc: 60.94%] [G Loss: 0.771789]\n",
      "479 [D Loss: 0.639466, acc: 57.81%] [G Loss: 0.813562]\n",
      "480 [D Loss: 0.633391, acc: 64.06%] [G Loss: 0.752594]\n",
      "481 [D Loss: 0.641630, acc: 60.94%] [G Loss: 0.748035]\n",
      "482 [D Loss: 0.641986, acc: 56.25%] [G Loss: 0.754474]\n",
      "483 [D Loss: 0.646558, acc: 56.25%] [G Loss: 0.776060]\n",
      "484 [D Loss: 0.644417, acc: 59.38%] [G Loss: 0.738082]\n",
      "485 [D Loss: 0.667942, acc: 56.25%] [G Loss: 0.765911]\n",
      "486 [D Loss: 0.624660, acc: 65.62%] [G Loss: 0.763059]\n",
      "487 [D Loss: 0.652778, acc: 53.12%] [G Loss: 0.773325]\n",
      "488 [D Loss: 0.645040, acc: 54.69%] [G Loss: 0.795265]\n",
      "489 [D Loss: 0.629518, acc: 57.81%] [G Loss: 0.785093]\n",
      "490 [D Loss: 0.644060, acc: 64.06%] [G Loss: 0.773034]\n",
      "491 [D Loss: 0.636799, acc: 64.06%] [G Loss: 0.766084]\n",
      "492 [D Loss: 0.634185, acc: 57.81%] [G Loss: 0.749033]\n",
      "493 [D Loss: 0.638178, acc: 67.19%] [G Loss: 0.746677]\n",
      "494 [D Loss: 0.632179, acc: 59.38%] [G Loss: 0.749131]\n",
      "495 [D Loss: 0.623279, acc: 48.44%] [G Loss: 0.765642]\n",
      "496 [D Loss: 0.621946, acc: 53.12%] [G Loss: 0.778806]\n",
      "497 [D Loss: 0.607618, acc: 64.06%] [G Loss: 0.786052]\n",
      "498 [D Loss: 0.606702, acc: 67.19%] [G Loss: 0.784102]\n",
      "499 [D Loss: 0.611490, acc: 73.44%] [G Loss: 0.806109]\n",
      "500 [D Loss: 0.633328, acc: 64.06%] [G Loss: 0.788786]\n",
      "501 [D Loss: 0.639867, acc: 56.25%] [G Loss: 0.783196]\n",
      "502 [D Loss: 0.637846, acc: 59.38%] [G Loss: 0.774147]\n",
      "503 [D Loss: 0.676088, acc: 50.00%] [G Loss: 0.778985]\n",
      "504 [D Loss: 0.659184, acc: 65.62%] [G Loss: 0.788053]\n",
      "505 [D Loss: 0.626067, acc: 67.19%] [G Loss: 0.778089]\n",
      "506 [D Loss: 0.614115, acc: 60.94%] [G Loss: 0.791338]\n",
      "507 [D Loss: 0.590436, acc: 68.75%] [G Loss: 0.816405]\n",
      "508 [D Loss: 0.630571, acc: 65.62%] [G Loss: 0.807550]\n",
      "509 [D Loss: 0.600315, acc: 67.19%] [G Loss: 0.817843]\n",
      "510 [D Loss: 0.648155, acc: 51.56%] [G Loss: 0.775540]\n",
      "511 [D Loss: 0.664089, acc: 57.81%] [G Loss: 0.741523]\n",
      "512 [D Loss: 0.675444, acc: 53.12%] [G Loss: 0.747805]\n",
      "513 [D Loss: 0.597181, acc: 64.06%] [G Loss: 0.736352]\n",
      "514 [D Loss: 0.628201, acc: 65.62%] [G Loss: 0.712181]\n",
      "515 [D Loss: 0.647593, acc: 54.69%] [G Loss: 0.744949]\n",
      "516 [D Loss: 0.642707, acc: 65.62%] [G Loss: 0.761927]\n",
      "517 [D Loss: 0.608376, acc: 64.06%] [G Loss: 0.765091]\n",
      "518 [D Loss: 0.650495, acc: 57.81%] [G Loss: 0.746266]\n",
      "519 [D Loss: 0.593551, acc: 68.75%] [G Loss: 0.775811]\n",
      "520 [D Loss: 0.654426, acc: 57.81%] [G Loss: 0.697806]\n",
      "521 [D Loss: 0.663019, acc: 59.38%] [G Loss: 0.734105]\n",
      "522 [D Loss: 0.613971, acc: 67.19%] [G Loss: 0.733461]\n",
      "523 [D Loss: 0.628128, acc: 64.06%] [G Loss: 0.744890]\n",
      "524 [D Loss: 0.623459, acc: 67.19%] [G Loss: 0.750377]\n",
      "525 [D Loss: 0.625693, acc: 64.06%] [G Loss: 0.746644]\n",
      "526 [D Loss: 0.635683, acc: 67.19%] [G Loss: 0.772225]\n",
      "527 [D Loss: 0.664080, acc: 64.06%] [G Loss: 0.751490]\n",
      "528 [D Loss: 0.640567, acc: 60.94%] [G Loss: 0.775115]\n",
      "529 [D Loss: 0.646591, acc: 62.50%] [G Loss: 0.780378]\n",
      "530 [D Loss: 0.618264, acc: 65.62%] [G Loss: 0.767980]\n",
      "531 [D Loss: 0.645323, acc: 60.94%] [G Loss: 0.776510]\n",
      "532 [D Loss: 0.686346, acc: 51.56%] [G Loss: 0.786288]\n",
      "533 [D Loss: 0.623238, acc: 68.75%] [G Loss: 0.813227]\n",
      "534 [D Loss: 0.656921, acc: 50.00%] [G Loss: 0.795845]\n",
      "535 [D Loss: 0.631010, acc: 71.88%] [G Loss: 0.765895]\n",
      "536 [D Loss: 0.643383, acc: 56.25%] [G Loss: 0.746113]\n",
      "537 [D Loss: 0.643214, acc: 59.38%] [G Loss: 0.749454]\n",
      "538 [D Loss: 0.629679, acc: 64.06%] [G Loss: 0.730996]\n",
      "539 [D Loss: 0.616904, acc: 65.62%] [G Loss: 0.750908]\n",
      "540 [D Loss: 0.598828, acc: 65.62%] [G Loss: 0.776029]\n",
      "541 [D Loss: 0.635146, acc: 60.94%] [G Loss: 0.803665]\n",
      "542 [D Loss: 0.634516, acc: 62.50%] [G Loss: 0.802417]\n",
      "543 [D Loss: 0.633058, acc: 67.19%] [G Loss: 0.750547]\n",
      "544 [D Loss: 0.602545, acc: 62.50%] [G Loss: 0.782669]\n",
      "545 [D Loss: 0.626398, acc: 60.94%] [G Loss: 0.803319]\n",
      "546 [D Loss: 0.645938, acc: 57.81%] [G Loss: 0.783327]\n",
      "547 [D Loss: 0.621707, acc: 67.19%] [G Loss: 0.826311]\n",
      "548 [D Loss: 0.635112, acc: 70.31%] [G Loss: 0.784220]\n",
      "549 [D Loss: 0.626646, acc: 64.06%] [G Loss: 0.761883]\n",
      "550 [D Loss: 0.586928, acc: 71.88%] [G Loss: 0.752560]\n",
      "551 [D Loss: 0.596371, acc: 75.00%] [G Loss: 0.734615]\n",
      "552 [D Loss: 0.581663, acc: 73.44%] [G Loss: 0.738069]\n",
      "553 [D Loss: 0.651352, acc: 68.75%] [G Loss: 0.749937]\n",
      "554 [D Loss: 0.632161, acc: 67.19%] [G Loss: 0.782360]\n",
      "555 [D Loss: 0.624518, acc: 68.75%] [G Loss: 0.774097]\n",
      "556 [D Loss: 0.661143, acc: 60.94%] [G Loss: 0.789088]\n",
      "557 [D Loss: 0.631913, acc: 60.94%] [G Loss: 0.829300]\n",
      "558 [D Loss: 0.602926, acc: 71.88%] [G Loss: 0.811714]\n",
      "559 [D Loss: 0.627976, acc: 65.62%] [G Loss: 0.817409]\n",
      "560 [D Loss: 0.638394, acc: 57.81%] [G Loss: 0.782353]\n",
      "561 [D Loss: 0.612320, acc: 67.19%] [G Loss: 0.811149]\n",
      "562 [D Loss: 0.604285, acc: 67.19%] [G Loss: 0.821965]\n",
      "563 [D Loss: 0.597975, acc: 71.88%] [G Loss: 0.777244]\n",
      "564 [D Loss: 0.638931, acc: 70.31%] [G Loss: 0.740510]\n",
      "565 [D Loss: 0.575171, acc: 73.44%] [G Loss: 0.784790]\n",
      "566 [D Loss: 0.620174, acc: 59.38%] [G Loss: 0.801629]\n",
      "567 [D Loss: 0.597590, acc: 70.31%] [G Loss: 0.819593]\n",
      "568 [D Loss: 0.661506, acc: 54.69%] [G Loss: 0.763169]\n",
      "569 [D Loss: 0.632165, acc: 64.06%] [G Loss: 0.711775]\n",
      "570 [D Loss: 0.692923, acc: 50.00%] [G Loss: 0.707312]\n",
      "571 [D Loss: 0.673064, acc: 51.56%] [G Loss: 0.745287]\n",
      "572 [D Loss: 0.671034, acc: 57.81%] [G Loss: 0.776070]\n",
      "573 [D Loss: 0.639638, acc: 60.94%] [G Loss: 0.784482]\n",
      "574 [D Loss: 0.688594, acc: 54.69%] [G Loss: 0.798694]\n",
      "575 [D Loss: 0.642307, acc: 57.81%] [G Loss: 0.838331]\n",
      "576 [D Loss: 0.624003, acc: 68.75%] [G Loss: 0.860927]\n",
      "577 [D Loss: 0.681715, acc: 60.94%] [G Loss: 0.823313]\n",
      "578 [D Loss: 0.651886, acc: 67.19%] [G Loss: 0.777441]\n",
      "579 [D Loss: 0.667795, acc: 57.81%] [G Loss: 0.814248]\n",
      "580 [D Loss: 0.640806, acc: 60.94%] [G Loss: 0.815416]\n",
      "581 [D Loss: 0.648497, acc: 57.81%] [G Loss: 0.789130]\n",
      "582 [D Loss: 0.615765, acc: 68.75%] [G Loss: 0.732053]\n",
      "583 [D Loss: 0.653609, acc: 54.69%] [G Loss: 0.763294]\n",
      "584 [D Loss: 0.622761, acc: 65.62%] [G Loss: 0.773234]\n",
      "585 [D Loss: 0.657418, acc: 59.38%] [G Loss: 0.787177]\n",
      "586 [D Loss: 0.637778, acc: 60.94%] [G Loss: 0.799951]\n",
      "587 [D Loss: 0.595610, acc: 71.88%] [G Loss: 0.800435]\n",
      "588 [D Loss: 0.616038, acc: 70.31%] [G Loss: 0.814257]\n",
      "589 [D Loss: 0.652847, acc: 62.50%] [G Loss: 0.786073]\n",
      "590 [D Loss: 0.640930, acc: 60.94%] [G Loss: 0.753527]\n",
      "591 [D Loss: 0.635999, acc: 62.50%] [G Loss: 0.752450]\n",
      "592 [D Loss: 0.622191, acc: 62.50%] [G Loss: 0.730977]\n",
      "593 [D Loss: 0.652375, acc: 59.38%] [G Loss: 0.760907]\n",
      "594 [D Loss: 0.646295, acc: 65.62%] [G Loss: 0.799384]\n",
      "595 [D Loss: 0.631316, acc: 70.31%] [G Loss: 0.804267]\n",
      "596 [D Loss: 0.627339, acc: 59.38%] [G Loss: 0.776461]\n",
      "597 [D Loss: 0.618207, acc: 67.19%] [G Loss: 0.783662]\n",
      "598 [D Loss: 0.624150, acc: 62.50%] [G Loss: 0.792453]\n",
      "599 [D Loss: 0.637761, acc: 59.38%] [G Loss: 0.787274]\n",
      "600 [D Loss: 0.615876, acc: 71.88%] [G Loss: 0.747111]\n",
      "601 [D Loss: 0.638498, acc: 64.06%] [G Loss: 0.738086]\n",
      "602 [D Loss: 0.628469, acc: 71.88%] [G Loss: 0.772570]\n",
      "603 [D Loss: 0.605611, acc: 67.19%] [G Loss: 0.814475]\n",
      "604 [D Loss: 0.599254, acc: 79.69%] [G Loss: 0.749939]\n",
      "605 [D Loss: 0.599790, acc: 71.88%] [G Loss: 0.769712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606 [D Loss: 0.621489, acc: 62.50%] [G Loss: 0.773433]\n",
      "607 [D Loss: 0.588624, acc: 70.31%] [G Loss: 0.773524]\n",
      "608 [D Loss: 0.628656, acc: 73.44%] [G Loss: 0.777327]\n",
      "609 [D Loss: 0.646507, acc: 57.81%] [G Loss: 0.774494]\n",
      "610 [D Loss: 0.659698, acc: 57.81%] [G Loss: 0.815336]\n",
      "611 [D Loss: 0.613866, acc: 64.06%] [G Loss: 0.832455]\n",
      "612 [D Loss: 0.639607, acc: 65.62%] [G Loss: 0.806999]\n",
      "613 [D Loss: 0.636620, acc: 64.06%] [G Loss: 0.744727]\n",
      "614 [D Loss: 0.632330, acc: 70.31%] [G Loss: 0.735515]\n",
      "615 [D Loss: 0.632448, acc: 70.31%] [G Loss: 0.770518]\n",
      "616 [D Loss: 0.663597, acc: 53.12%] [G Loss: 0.753907]\n",
      "617 [D Loss: 0.620387, acc: 60.94%] [G Loss: 0.731258]\n",
      "618 [D Loss: 0.644397, acc: 67.19%] [G Loss: 0.740884]\n",
      "619 [D Loss: 0.632815, acc: 64.06%] [G Loss: 0.728187]\n",
      "620 [D Loss: 0.631893, acc: 64.06%] [G Loss: 0.745008]\n",
      "621 [D Loss: 0.613308, acc: 67.19%] [G Loss: 0.764404]\n",
      "622 [D Loss: 0.629141, acc: 56.25%] [G Loss: 0.766275]\n",
      "623 [D Loss: 0.632335, acc: 57.81%] [G Loss: 0.773887]\n",
      "624 [D Loss: 0.608767, acc: 73.44%] [G Loss: 0.780057]\n",
      "625 [D Loss: 0.637079, acc: 57.81%] [G Loss: 0.798972]\n",
      "626 [D Loss: 0.630777, acc: 65.62%] [G Loss: 0.757529]\n",
      "627 [D Loss: 0.616240, acc: 78.12%] [G Loss: 0.756698]\n",
      "628 [D Loss: 0.629214, acc: 64.06%] [G Loss: 0.759029]\n",
      "629 [D Loss: 0.614964, acc: 65.62%] [G Loss: 0.827642]\n",
      "630 [D Loss: 0.648883, acc: 62.50%] [G Loss: 0.785139]\n",
      "631 [D Loss: 0.656954, acc: 51.56%] [G Loss: 0.789230]\n",
      "632 [D Loss: 0.633427, acc: 70.31%] [G Loss: 0.776798]\n",
      "633 [D Loss: 0.596732, acc: 71.88%] [G Loss: 0.805100]\n",
      "634 [D Loss: 0.657552, acc: 56.25%] [G Loss: 0.762031]\n",
      "635 [D Loss: 0.635812, acc: 60.94%] [G Loss: 0.749851]\n",
      "636 [D Loss: 0.625085, acc: 64.06%] [G Loss: 0.746772]\n",
      "637 [D Loss: 0.626964, acc: 60.94%] [G Loss: 0.754612]\n",
      "638 [D Loss: 0.637377, acc: 65.62%] [G Loss: 0.770695]\n",
      "639 [D Loss: 0.626654, acc: 64.06%] [G Loss: 0.749400]\n",
      "640 [D Loss: 0.636080, acc: 65.62%] [G Loss: 0.757880]\n",
      "641 [D Loss: 0.650407, acc: 59.38%] [G Loss: 0.777418]\n",
      "642 [D Loss: 0.623559, acc: 64.06%] [G Loss: 0.765146]\n",
      "643 [D Loss: 0.625216, acc: 65.62%] [G Loss: 0.785130]\n",
      "644 [D Loss: 0.619110, acc: 73.44%] [G Loss: 0.788770]\n",
      "645 [D Loss: 0.583256, acc: 79.69%] [G Loss: 0.813435]\n",
      "646 [D Loss: 0.641572, acc: 62.50%] [G Loss: 0.822564]\n",
      "647 [D Loss: 0.617303, acc: 62.50%] [G Loss: 0.800350]\n",
      "648 [D Loss: 0.611279, acc: 68.75%] [G Loss: 0.773640]\n",
      "649 [D Loss: 0.654157, acc: 54.69%] [G Loss: 0.799063]\n",
      "650 [D Loss: 0.632226, acc: 64.06%] [G Loss: 0.759095]\n",
      "651 [D Loss: 0.610197, acc: 73.44%] [G Loss: 0.786638]\n",
      "652 [D Loss: 0.633251, acc: 67.19%] [G Loss: 0.777303]\n",
      "653 [D Loss: 0.615639, acc: 70.31%] [G Loss: 0.801000]\n",
      "654 [D Loss: 0.668537, acc: 59.38%] [G Loss: 0.794689]\n",
      "655 [D Loss: 0.610685, acc: 70.31%] [G Loss: 0.766087]\n",
      "656 [D Loss: 0.670507, acc: 51.56%] [G Loss: 0.791185]\n",
      "657 [D Loss: 0.653682, acc: 54.69%] [G Loss: 0.771454]\n",
      "658 [D Loss: 0.620038, acc: 65.62%] [G Loss: 0.725617]\n",
      "659 [D Loss: 0.660672, acc: 54.69%] [G Loss: 0.782112]\n",
      "660 [D Loss: 0.639598, acc: 57.81%] [G Loss: 0.855348]\n",
      "661 [D Loss: 0.637005, acc: 59.38%] [G Loss: 0.774163]\n",
      "662 [D Loss: 0.634870, acc: 56.25%] [G Loss: 0.748189]\n",
      "663 [D Loss: 0.625867, acc: 62.50%] [G Loss: 0.754799]\n",
      "664 [D Loss: 0.639458, acc: 59.38%] [G Loss: 0.781463]\n",
      "665 [D Loss: 0.657307, acc: 56.25%] [G Loss: 0.782710]\n",
      "666 [D Loss: 0.654854, acc: 56.25%] [G Loss: 0.795288]\n",
      "667 [D Loss: 0.622222, acc: 71.88%] [G Loss: 0.762103]\n",
      "668 [D Loss: 0.662223, acc: 62.50%] [G Loss: 0.756095]\n",
      "669 [D Loss: 0.652397, acc: 56.25%] [G Loss: 0.834490]\n",
      "670 [D Loss: 0.650062, acc: 56.25%] [G Loss: 0.786263]\n",
      "671 [D Loss: 0.639524, acc: 70.31%] [G Loss: 0.787919]\n",
      "672 [D Loss: 0.632220, acc: 68.75%] [G Loss: 0.768472]\n",
      "673 [D Loss: 0.657131, acc: 60.94%] [G Loss: 0.754619]\n",
      "674 [D Loss: 0.634206, acc: 59.38%] [G Loss: 0.735865]\n",
      "675 [D Loss: 0.614887, acc: 70.31%] [G Loss: 0.743367]\n",
      "676 [D Loss: 0.612616, acc: 67.19%] [G Loss: 0.733305]\n",
      "677 [D Loss: 0.610114, acc: 71.88%] [G Loss: 0.797190]\n",
      "678 [D Loss: 0.639382, acc: 68.75%] [G Loss: 0.783200]\n",
      "679 [D Loss: 0.608115, acc: 65.62%] [G Loss: 0.789071]\n",
      "680 [D Loss: 0.646656, acc: 62.50%] [G Loss: 0.752640]\n",
      "681 [D Loss: 0.596972, acc: 70.31%] [G Loss: 0.732305]\n",
      "682 [D Loss: 0.616230, acc: 68.75%] [G Loss: 0.736649]\n",
      "683 [D Loss: 0.614533, acc: 75.00%] [G Loss: 0.762106]\n",
      "684 [D Loss: 0.600672, acc: 71.88%] [G Loss: 0.786036]\n",
      "685 [D Loss: 0.626178, acc: 70.31%] [G Loss: 0.733848]\n",
      "686 [D Loss: 0.633283, acc: 62.50%] [G Loss: 0.773628]\n",
      "687 [D Loss: 0.638016, acc: 67.19%] [G Loss: 0.778827]\n",
      "688 [D Loss: 0.617458, acc: 75.00%] [G Loss: 0.794617]\n",
      "689 [D Loss: 0.653216, acc: 60.94%] [G Loss: 0.798464]\n",
      "690 [D Loss: 0.642588, acc: 59.38%] [G Loss: 0.769007]\n",
      "691 [D Loss: 0.618333, acc: 73.44%] [G Loss: 0.757290]\n",
      "692 [D Loss: 0.633352, acc: 64.06%] [G Loss: 0.754397]\n",
      "693 [D Loss: 0.628004, acc: 70.31%] [G Loss: 0.762978]\n",
      "694 [D Loss: 0.634120, acc: 65.62%] [G Loss: 0.745029]\n",
      "695 [D Loss: 0.648060, acc: 67.19%] [G Loss: 0.750535]\n",
      "696 [D Loss: 0.623859, acc: 62.50%] [G Loss: 0.812824]\n",
      "697 [D Loss: 0.622194, acc: 64.06%] [G Loss: 0.791675]\n",
      "698 [D Loss: 0.629941, acc: 62.50%] [G Loss: 0.724182]\n",
      "699 [D Loss: 0.625518, acc: 67.19%] [G Loss: 0.734434]\n",
      "700 [D Loss: 0.627059, acc: 67.19%] [G Loss: 0.732720]\n",
      "701 [D Loss: 0.645139, acc: 57.81%] [G Loss: 0.765349]\n",
      "702 [D Loss: 0.616518, acc: 62.50%] [G Loss: 0.774186]\n",
      "703 [D Loss: 0.638798, acc: 67.19%] [G Loss: 0.724581]\n",
      "704 [D Loss: 0.610282, acc: 70.31%] [G Loss: 0.761536]\n",
      "705 [D Loss: 0.693885, acc: 56.25%] [G Loss: 0.728995]\n",
      "706 [D Loss: 0.646421, acc: 62.50%] [G Loss: 0.763726]\n",
      "707 [D Loss: 0.683870, acc: 62.50%] [G Loss: 0.805157]\n",
      "708 [D Loss: 0.656681, acc: 64.06%] [G Loss: 0.825268]\n",
      "709 [D Loss: 0.654258, acc: 57.81%] [G Loss: 0.812753]\n",
      "710 [D Loss: 0.648292, acc: 59.38%] [G Loss: 0.816415]\n",
      "711 [D Loss: 0.647810, acc: 68.75%] [G Loss: 0.735473]\n",
      "712 [D Loss: 0.625021, acc: 68.75%] [G Loss: 0.777473]\n",
      "713 [D Loss: 0.664513, acc: 46.88%] [G Loss: 0.808159]\n",
      "714 [D Loss: 0.603468, acc: 67.19%] [G Loss: 0.821407]\n",
      "715 [D Loss: 0.636547, acc: 67.19%] [G Loss: 0.788875]\n",
      "716 [D Loss: 0.636131, acc: 64.06%] [G Loss: 0.742553]\n",
      "717 [D Loss: 0.644014, acc: 57.81%] [G Loss: 0.781349]\n",
      "718 [D Loss: 0.626804, acc: 57.81%] [G Loss: 0.798426]\n",
      "719 [D Loss: 0.617892, acc: 67.19%] [G Loss: 0.756565]\n",
      "720 [D Loss: 0.654400, acc: 62.50%] [G Loss: 0.764724]\n",
      "721 [D Loss: 0.629278, acc: 64.06%] [G Loss: 0.768411]\n",
      "722 [D Loss: 0.614938, acc: 60.94%] [G Loss: 0.769713]\n",
      "723 [D Loss: 0.615489, acc: 68.75%] [G Loss: 0.773416]\n",
      "724 [D Loss: 0.584620, acc: 71.88%] [G Loss: 0.761935]\n",
      "725 [D Loss: 0.660973, acc: 60.94%] [G Loss: 0.800195]\n",
      "726 [D Loss: 0.602766, acc: 71.88%] [G Loss: 0.791992]\n",
      "727 [D Loss: 0.651644, acc: 53.12%] [G Loss: 0.806691]\n",
      "728 [D Loss: 0.618945, acc: 65.62%] [G Loss: 0.800462]\n",
      "729 [D Loss: 0.626714, acc: 64.06%] [G Loss: 0.766530]\n",
      "730 [D Loss: 0.631110, acc: 65.62%] [G Loss: 0.720556]\n",
      "731 [D Loss: 0.619437, acc: 68.75%] [G Loss: 0.729963]\n",
      "732 [D Loss: 0.627078, acc: 57.81%] [G Loss: 0.722273]\n",
      "733 [D Loss: 0.595224, acc: 67.19%] [G Loss: 0.769472]\n",
      "734 [D Loss: 0.599761, acc: 70.31%] [G Loss: 0.775945]\n",
      "735 [D Loss: 0.625056, acc: 65.62%] [G Loss: 0.780638]\n",
      "736 [D Loss: 0.607114, acc: 71.88%] [G Loss: 0.778229]\n",
      "737 [D Loss: 0.636168, acc: 59.38%] [G Loss: 0.791453]\n",
      "738 [D Loss: 0.645714, acc: 62.50%] [G Loss: 0.813148]\n",
      "739 [D Loss: 0.615710, acc: 67.19%] [G Loss: 0.799477]\n",
      "740 [D Loss: 0.614951, acc: 67.19%] [G Loss: 0.758779]\n",
      "741 [D Loss: 0.615813, acc: 59.38%] [G Loss: 0.764003]\n",
      "742 [D Loss: 0.622800, acc: 70.31%] [G Loss: 0.793732]\n",
      "743 [D Loss: 0.596624, acc: 67.19%] [G Loss: 0.764813]\n",
      "744 [D Loss: 0.647970, acc: 57.81%] [G Loss: 0.771186]\n",
      "745 [D Loss: 0.647392, acc: 62.50%] [G Loss: 0.753126]\n",
      "746 [D Loss: 0.604364, acc: 67.19%] [G Loss: 0.779834]\n",
      "747 [D Loss: 0.659970, acc: 57.81%] [G Loss: 0.762102]\n",
      "748 [D Loss: 0.648497, acc: 64.06%] [G Loss: 0.739734]\n",
      "749 [D Loss: 0.654987, acc: 67.19%] [G Loss: 0.775226]\n",
      "750 [D Loss: 0.628594, acc: 56.25%] [G Loss: 0.790952]\n",
      "751 [D Loss: 0.605206, acc: 79.69%] [G Loss: 0.852985]\n",
      "752 [D Loss: 0.684301, acc: 56.25%] [G Loss: 0.810707]\n",
      "753 [D Loss: 0.675742, acc: 59.38%] [G Loss: 0.777064]\n",
      "754 [D Loss: 0.635168, acc: 71.88%] [G Loss: 0.781064]\n",
      "755 [D Loss: 0.619958, acc: 68.75%] [G Loss: 0.727492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756 [D Loss: 0.642991, acc: 64.06%] [G Loss: 0.777777]\n",
      "757 [D Loss: 0.638822, acc: 60.94%] [G Loss: 0.757771]\n",
      "758 [D Loss: 0.659680, acc: 62.50%] [G Loss: 0.745785]\n",
      "759 [D Loss: 0.601776, acc: 68.75%] [G Loss: 0.784087]\n",
      "760 [D Loss: 0.631331, acc: 64.06%] [G Loss: 0.785597]\n",
      "761 [D Loss: 0.643039, acc: 68.75%] [G Loss: 0.775095]\n",
      "762 [D Loss: 0.636063, acc: 64.06%] [G Loss: 0.778788]\n",
      "763 [D Loss: 0.626772, acc: 59.38%] [G Loss: 0.832472]\n",
      "764 [D Loss: 0.621740, acc: 68.75%] [G Loss: 0.843231]\n",
      "765 [D Loss: 0.649997, acc: 60.94%] [G Loss: 0.801495]\n",
      "766 [D Loss: 0.634969, acc: 64.06%] [G Loss: 0.766703]\n",
      "767 [D Loss: 0.597613, acc: 64.06%] [G Loss: 0.768587]\n",
      "768 [D Loss: 0.623136, acc: 67.19%] [G Loss: 0.776669]\n",
      "769 [D Loss: 0.659957, acc: 57.81%] [G Loss: 0.772526]\n",
      "770 [D Loss: 0.619506, acc: 60.94%] [G Loss: 0.787199]\n",
      "771 [D Loss: 0.559080, acc: 81.25%] [G Loss: 0.756305]\n",
      "772 [D Loss: 0.657515, acc: 65.62%] [G Loss: 0.739246]\n",
      "773 [D Loss: 0.627442, acc: 70.31%] [G Loss: 0.818967]\n",
      "774 [D Loss: 0.605610, acc: 71.88%] [G Loss: 0.831007]\n",
      "775 [D Loss: 0.646847, acc: 57.81%] [G Loss: 0.885173]\n",
      "776 [D Loss: 0.606524, acc: 73.44%] [G Loss: 0.841574]\n",
      "777 [D Loss: 0.631823, acc: 60.94%] [G Loss: 0.825342]\n",
      "778 [D Loss: 0.621255, acc: 70.31%] [G Loss: 0.797706]\n",
      "779 [D Loss: 0.605409, acc: 67.19%] [G Loss: 0.801070]\n",
      "780 [D Loss: 0.653215, acc: 59.38%] [G Loss: 0.821661]\n",
      "781 [D Loss: 0.611083, acc: 62.50%] [G Loss: 0.838018]\n",
      "782 [D Loss: 0.603299, acc: 76.56%] [G Loss: 0.805381]\n",
      "783 [D Loss: 0.626339, acc: 62.50%] [G Loss: 0.826045]\n",
      "784 [D Loss: 0.618011, acc: 71.88%] [G Loss: 0.795172]\n",
      "785 [D Loss: 0.580440, acc: 75.00%] [G Loss: 0.804355]\n",
      "786 [D Loss: 0.635275, acc: 67.19%] [G Loss: 0.807784]\n",
      "787 [D Loss: 0.621265, acc: 68.75%] [G Loss: 0.780403]\n",
      "788 [D Loss: 0.617484, acc: 68.75%] [G Loss: 0.794871]\n",
      "789 [D Loss: 0.641854, acc: 57.81%] [G Loss: 0.770038]\n",
      "790 [D Loss: 0.638931, acc: 54.69%] [G Loss: 0.791154]\n",
      "791 [D Loss: 0.628638, acc: 60.94%] [G Loss: 0.797974]\n",
      "792 [D Loss: 0.609201, acc: 71.88%] [G Loss: 0.801659]\n",
      "793 [D Loss: 0.610955, acc: 73.44%] [G Loss: 0.786303]\n",
      "794 [D Loss: 0.613060, acc: 68.75%] [G Loss: 0.786336]\n",
      "795 [D Loss: 0.631150, acc: 60.94%] [G Loss: 0.777894]\n",
      "796 [D Loss: 0.632309, acc: 60.94%] [G Loss: 0.824551]\n",
      "797 [D Loss: 0.610551, acc: 68.75%] [G Loss: 0.852490]\n",
      "798 [D Loss: 0.635646, acc: 68.75%] [G Loss: 0.798975]\n",
      "799 [D Loss: 0.626745, acc: 62.50%] [G Loss: 0.766042]\n",
      "800 [D Loss: 0.622050, acc: 59.38%] [G Loss: 0.774207]\n",
      "801 [D Loss: 0.631464, acc: 54.69%] [G Loss: 0.808689]\n",
      "802 [D Loss: 0.632746, acc: 60.94%] [G Loss: 0.846700]\n",
      "803 [D Loss: 0.618803, acc: 76.56%] [G Loss: 0.843847]\n",
      "804 [D Loss: 0.672313, acc: 51.56%] [G Loss: 0.820656]\n",
      "805 [D Loss: 0.646658, acc: 60.94%] [G Loss: 0.770334]\n",
      "806 [D Loss: 0.623179, acc: 67.19%] [G Loss: 0.742073]\n",
      "807 [D Loss: 0.596058, acc: 71.88%] [G Loss: 0.735785]\n",
      "808 [D Loss: 0.634576, acc: 59.38%] [G Loss: 0.773678]\n",
      "809 [D Loss: 0.582197, acc: 78.12%] [G Loss: 0.787278]\n",
      "810 [D Loss: 0.620391, acc: 64.06%] [G Loss: 0.818905]\n",
      "811 [D Loss: 0.581217, acc: 70.31%] [G Loss: 0.779865]\n",
      "812 [D Loss: 0.605720, acc: 67.19%] [G Loss: 0.810789]\n",
      "813 [D Loss: 0.594920, acc: 65.62%] [G Loss: 0.815390]\n",
      "814 [D Loss: 0.594676, acc: 73.44%] [G Loss: 0.785415]\n",
      "815 [D Loss: 0.629049, acc: 56.25%] [G Loss: 0.797849]\n",
      "816 [D Loss: 0.613916, acc: 70.31%] [G Loss: 0.796503]\n",
      "817 [D Loss: 0.615910, acc: 68.75%] [G Loss: 0.821082]\n",
      "818 [D Loss: 0.584058, acc: 78.12%] [G Loss: 0.827056]\n",
      "819 [D Loss: 0.617241, acc: 62.50%] [G Loss: 0.785856]\n",
      "820 [D Loss: 0.651201, acc: 59.38%] [G Loss: 0.792757]\n",
      "821 [D Loss: 0.630359, acc: 64.06%] [G Loss: 0.783841]\n",
      "822 [D Loss: 0.633360, acc: 67.19%] [G Loss: 0.757436]\n",
      "823 [D Loss: 0.627208, acc: 62.50%] [G Loss: 0.806621]\n",
      "824 [D Loss: 0.634844, acc: 67.19%] [G Loss: 0.788285]\n",
      "825 [D Loss: 0.597555, acc: 70.31%] [G Loss: 0.738330]\n",
      "826 [D Loss: 0.656497, acc: 57.81%] [G Loss: 0.794704]\n",
      "827 [D Loss: 0.631834, acc: 67.19%] [G Loss: 0.840737]\n",
      "828 [D Loss: 0.606492, acc: 71.88%] [G Loss: 0.788882]\n",
      "829 [D Loss: 0.661330, acc: 62.50%] [G Loss: 0.817766]\n",
      "830 [D Loss: 0.634060, acc: 62.50%] [G Loss: 0.793874]\n",
      "831 [D Loss: 0.615543, acc: 67.19%] [G Loss: 0.802062]\n",
      "832 [D Loss: 0.628647, acc: 67.19%] [G Loss: 0.737899]\n",
      "833 [D Loss: 0.626100, acc: 65.62%] [G Loss: 0.773209]\n",
      "834 [D Loss: 0.650856, acc: 64.06%] [G Loss: 0.816641]\n",
      "835 [D Loss: 0.628751, acc: 62.50%] [G Loss: 0.814574]\n",
      "836 [D Loss: 0.592379, acc: 79.69%] [G Loss: 0.770914]\n",
      "837 [D Loss: 0.623105, acc: 73.44%] [G Loss: 0.835152]\n",
      "838 [D Loss: 0.585026, acc: 81.25%] [G Loss: 0.775414]\n",
      "839 [D Loss: 0.678845, acc: 50.00%] [G Loss: 0.764581]\n",
      "840 [D Loss: 0.657696, acc: 60.94%] [G Loss: 0.806313]\n",
      "841 [D Loss: 0.617905, acc: 67.19%] [G Loss: 0.856551]\n",
      "842 [D Loss: 0.631833, acc: 60.94%] [G Loss: 0.829414]\n",
      "843 [D Loss: 0.627652, acc: 64.06%] [G Loss: 0.796551]\n",
      "844 [D Loss: 0.611950, acc: 59.38%] [G Loss: 0.855539]\n",
      "845 [D Loss: 0.630381, acc: 64.06%] [G Loss: 0.810673]\n",
      "846 [D Loss: 0.602212, acc: 67.19%] [G Loss: 0.833262]\n",
      "847 [D Loss: 0.648219, acc: 57.81%] [G Loss: 0.823910]\n",
      "848 [D Loss: 0.636459, acc: 68.75%] [G Loss: 0.827070]\n",
      "849 [D Loss: 0.636761, acc: 67.19%] [G Loss: 0.852817]\n",
      "850 [D Loss: 0.613006, acc: 67.19%] [G Loss: 0.840919]\n",
      "851 [D Loss: 0.620923, acc: 68.75%] [G Loss: 0.770733]\n",
      "852 [D Loss: 0.616570, acc: 75.00%] [G Loss: 0.835671]\n",
      "853 [D Loss: 0.651311, acc: 60.94%] [G Loss: 0.775464]\n",
      "854 [D Loss: 0.606055, acc: 64.06%] [G Loss: 0.778489]\n",
      "855 [D Loss: 0.606615, acc: 78.12%] [G Loss: 0.816423]\n",
      "856 [D Loss: 0.585752, acc: 76.56%] [G Loss: 0.817161]\n",
      "857 [D Loss: 0.679710, acc: 57.81%] [G Loss: 0.814802]\n",
      "858 [D Loss: 0.639690, acc: 65.62%] [G Loss: 0.766301]\n",
      "859 [D Loss: 0.634529, acc: 70.31%] [G Loss: 0.793869]\n",
      "860 [D Loss: 0.654866, acc: 57.81%] [G Loss: 0.794069]\n",
      "861 [D Loss: 0.646114, acc: 59.38%] [G Loss: 0.773015]\n",
      "862 [D Loss: 0.637150, acc: 62.50%] [G Loss: 0.777466]\n",
      "863 [D Loss: 0.628640, acc: 64.06%] [G Loss: 0.782091]\n",
      "864 [D Loss: 0.631998, acc: 65.62%] [G Loss: 0.758285]\n",
      "865 [D Loss: 0.632916, acc: 70.31%] [G Loss: 0.733148]\n",
      "866 [D Loss: 0.619523, acc: 73.44%] [G Loss: 0.774372]\n",
      "867 [D Loss: 0.637807, acc: 70.31%] [G Loss: 0.759407]\n",
      "868 [D Loss: 0.631486, acc: 70.31%] [G Loss: 0.787031]\n",
      "869 [D Loss: 0.614199, acc: 73.44%] [G Loss: 0.788017]\n",
      "870 [D Loss: 0.623913, acc: 57.81%] [G Loss: 0.800438]\n",
      "871 [D Loss: 0.628952, acc: 60.94%] [G Loss: 0.772214]\n",
      "872 [D Loss: 0.622804, acc: 70.31%] [G Loss: 0.742154]\n",
      "873 [D Loss: 0.618059, acc: 59.38%] [G Loss: 0.739790]\n",
      "874 [D Loss: 0.635671, acc: 65.62%] [G Loss: 0.804862]\n",
      "875 [D Loss: 0.597282, acc: 71.88%] [G Loss: 0.864656]\n",
      "876 [D Loss: 0.618601, acc: 68.75%] [G Loss: 0.880434]\n",
      "877 [D Loss: 0.636949, acc: 75.00%] [G Loss: 0.818036]\n",
      "878 [D Loss: 0.646882, acc: 50.00%] [G Loss: 0.806133]\n",
      "879 [D Loss: 0.607777, acc: 70.31%] [G Loss: 0.818193]\n",
      "880 [D Loss: 0.637571, acc: 57.81%] [G Loss: 0.780441]\n",
      "881 [D Loss: 0.654131, acc: 64.06%] [G Loss: 0.801139]\n",
      "882 [D Loss: 0.609171, acc: 75.00%] [G Loss: 0.797823]\n",
      "883 [D Loss: 0.635957, acc: 64.06%] [G Loss: 0.800640]\n",
      "884 [D Loss: 0.599302, acc: 65.62%] [G Loss: 0.783704]\n",
      "885 [D Loss: 0.613339, acc: 65.62%] [G Loss: 0.773771]\n",
      "886 [D Loss: 0.608757, acc: 70.31%] [G Loss: 0.793389]\n",
      "887 [D Loss: 0.618041, acc: 67.19%] [G Loss: 0.760781]\n",
      "888 [D Loss: 0.633323, acc: 64.06%] [G Loss: 0.818124]\n",
      "889 [D Loss: 0.636506, acc: 60.94%] [G Loss: 0.785523]\n",
      "890 [D Loss: 0.616006, acc: 68.75%] [G Loss: 0.852670]\n",
      "891 [D Loss: 0.644491, acc: 65.62%] [G Loss: 0.802577]\n",
      "892 [D Loss: 0.611491, acc: 65.62%] [G Loss: 0.770312]\n",
      "893 [D Loss: 0.589211, acc: 68.75%] [G Loss: 0.779924]\n",
      "894 [D Loss: 0.636179, acc: 64.06%] [G Loss: 0.768228]\n",
      "895 [D Loss: 0.614583, acc: 71.88%] [G Loss: 0.753217]\n",
      "896 [D Loss: 0.601309, acc: 70.31%] [G Loss: 0.783317]\n",
      "897 [D Loss: 0.632643, acc: 57.81%] [G Loss: 0.760027]\n",
      "898 [D Loss: 0.608281, acc: 70.31%] [G Loss: 0.790491]\n",
      "899 [D Loss: 0.657011, acc: 57.81%] [G Loss: 0.800931]\n",
      "900 [D Loss: 0.640854, acc: 65.62%] [G Loss: 0.814473]\n",
      "901 [D Loss: 0.640845, acc: 56.25%] [G Loss: 0.840729]\n",
      "902 [D Loss: 0.596369, acc: 73.44%] [G Loss: 0.786531]\n",
      "903 [D Loss: 0.609450, acc: 65.62%] [G Loss: 0.803339]\n",
      "904 [D Loss: 0.669311, acc: 54.69%] [G Loss: 0.768261]\n",
      "905 [D Loss: 0.610713, acc: 70.31%] [G Loss: 0.830332]\n",
      "906 [D Loss: 0.629140, acc: 64.06%] [G Loss: 0.823939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907 [D Loss: 0.599388, acc: 79.69%] [G Loss: 0.828639]\n",
      "908 [D Loss: 0.652058, acc: 62.50%] [G Loss: 0.818304]\n",
      "909 [D Loss: 0.669908, acc: 54.69%] [G Loss: 0.770496]\n",
      "910 [D Loss: 0.577325, acc: 79.69%] [G Loss: 0.806424]\n",
      "911 [D Loss: 0.571276, acc: 71.88%] [G Loss: 0.772451]\n",
      "912 [D Loss: 0.580667, acc: 73.44%] [G Loss: 0.806882]\n",
      "913 [D Loss: 0.682124, acc: 60.94%] [G Loss: 0.813339]\n",
      "914 [D Loss: 0.621054, acc: 65.62%] [G Loss: 0.819877]\n",
      "915 [D Loss: 0.610707, acc: 71.88%] [G Loss: 0.829115]\n",
      "916 [D Loss: 0.639467, acc: 62.50%] [G Loss: 0.796959]\n",
      "917 [D Loss: 0.667358, acc: 54.69%] [G Loss: 0.790704]\n",
      "918 [D Loss: 0.611154, acc: 73.44%] [G Loss: 0.775710]\n",
      "919 [D Loss: 0.623471, acc: 73.44%] [G Loss: 0.751610]\n",
      "920 [D Loss: 0.665706, acc: 53.12%] [G Loss: 0.786341]\n",
      "921 [D Loss: 0.598320, acc: 73.44%] [G Loss: 0.817309]\n",
      "922 [D Loss: 0.596111, acc: 70.31%] [G Loss: 0.789141]\n",
      "923 [D Loss: 0.658098, acc: 64.06%] [G Loss: 0.782627]\n",
      "924 [D Loss: 0.628605, acc: 60.94%] [G Loss: 0.754829]\n",
      "925 [D Loss: 0.606519, acc: 68.75%] [G Loss: 0.794122]\n",
      "926 [D Loss: 0.602114, acc: 64.06%] [G Loss: 0.817589]\n",
      "927 [D Loss: 0.610900, acc: 67.19%] [G Loss: 0.805146]\n",
      "928 [D Loss: 0.608492, acc: 65.62%] [G Loss: 0.878316]\n",
      "929 [D Loss: 0.624859, acc: 65.62%] [G Loss: 0.829404]\n",
      "930 [D Loss: 0.647115, acc: 65.62%] [G Loss: 0.814029]\n",
      "931 [D Loss: 0.658713, acc: 64.06%] [G Loss: 0.792638]\n",
      "932 [D Loss: 0.634330, acc: 65.62%] [G Loss: 0.791695]\n",
      "933 [D Loss: 0.616374, acc: 68.75%] [G Loss: 0.774633]\n",
      "934 [D Loss: 0.629501, acc: 64.06%] [G Loss: 0.802100]\n",
      "935 [D Loss: 0.606000, acc: 68.75%] [G Loss: 0.798297]\n",
      "936 [D Loss: 0.636437, acc: 73.44%] [G Loss: 0.796144]\n",
      "937 [D Loss: 0.622906, acc: 67.19%] [G Loss: 0.777558]\n",
      "938 [D Loss: 0.599794, acc: 71.88%] [G Loss: 0.825964]\n",
      "939 [D Loss: 0.648852, acc: 62.50%] [G Loss: 0.847514]\n",
      "940 [D Loss: 0.597079, acc: 71.88%] [G Loss: 0.821870]\n",
      "941 [D Loss: 0.643532, acc: 60.94%] [G Loss: 0.841059]\n",
      "942 [D Loss: 0.592914, acc: 71.88%] [G Loss: 0.831156]\n",
      "943 [D Loss: 0.615241, acc: 70.31%] [G Loss: 0.781116]\n",
      "944 [D Loss: 0.600813, acc: 76.56%] [G Loss: 0.790416]\n",
      "945 [D Loss: 0.639749, acc: 65.62%] [G Loss: 0.834178]\n",
      "946 [D Loss: 0.625027, acc: 65.62%] [G Loss: 0.792057]\n",
      "947 [D Loss: 0.609382, acc: 67.19%] [G Loss: 0.824027]\n",
      "948 [D Loss: 0.618774, acc: 67.19%] [G Loss: 0.860269]\n",
      "949 [D Loss: 0.619813, acc: 70.31%] [G Loss: 0.898048]\n",
      "950 [D Loss: 0.618164, acc: 65.62%] [G Loss: 0.868849]\n",
      "951 [D Loss: 0.611670, acc: 68.75%] [G Loss: 0.814897]\n",
      "952 [D Loss: 0.637874, acc: 68.75%] [G Loss: 0.797475]\n",
      "953 [D Loss: 0.619510, acc: 65.62%] [G Loss: 0.786611]\n",
      "954 [D Loss: 0.582688, acc: 70.31%] [G Loss: 0.802852]\n",
      "955 [D Loss: 0.596956, acc: 71.88%] [G Loss: 0.835561]\n",
      "956 [D Loss: 0.582726, acc: 76.56%] [G Loss: 0.818912]\n",
      "957 [D Loss: 0.598632, acc: 70.31%] [G Loss: 0.822791]\n",
      "958 [D Loss: 0.610302, acc: 70.31%] [G Loss: 0.817830]\n",
      "959 [D Loss: 0.587917, acc: 68.75%] [G Loss: 0.862950]\n",
      "960 [D Loss: 0.568634, acc: 81.25%] [G Loss: 0.826126]\n",
      "961 [D Loss: 0.649030, acc: 59.38%] [G Loss: 0.831003]\n",
      "962 [D Loss: 0.620664, acc: 59.38%] [G Loss: 0.880173]\n",
      "963 [D Loss: 0.607570, acc: 67.19%] [G Loss: 0.850250]\n",
      "964 [D Loss: 0.604566, acc: 68.75%] [G Loss: 0.805642]\n",
      "965 [D Loss: 0.616546, acc: 65.62%] [G Loss: 0.810234]\n",
      "966 [D Loss: 0.601054, acc: 68.75%] [G Loss: 0.827396]\n",
      "967 [D Loss: 0.598179, acc: 67.19%] [G Loss: 0.818834]\n",
      "968 [D Loss: 0.600289, acc: 73.44%] [G Loss: 0.918813]\n",
      "969 [D Loss: 0.591362, acc: 67.19%] [G Loss: 0.885321]\n",
      "970 [D Loss: 0.636059, acc: 64.06%] [G Loss: 0.922776]\n",
      "971 [D Loss: 0.598422, acc: 71.88%] [G Loss: 0.863359]\n",
      "972 [D Loss: 0.620238, acc: 67.19%] [G Loss: 0.811556]\n",
      "973 [D Loss: 0.612031, acc: 70.31%] [G Loss: 0.822580]\n",
      "974 [D Loss: 0.596459, acc: 68.75%] [G Loss: 0.836746]\n",
      "975 [D Loss: 0.594813, acc: 71.88%] [G Loss: 0.793093]\n",
      "976 [D Loss: 0.619659, acc: 76.56%] [G Loss: 0.813717]\n",
      "977 [D Loss: 0.615831, acc: 65.62%] [G Loss: 0.830609]\n",
      "978 [D Loss: 0.614997, acc: 70.31%] [G Loss: 0.866888]\n",
      "979 [D Loss: 0.591640, acc: 76.56%] [G Loss: 0.830803]\n",
      "980 [D Loss: 0.621922, acc: 65.62%] [G Loss: 0.817626]\n",
      "981 [D Loss: 0.575189, acc: 76.56%] [G Loss: 0.795632]\n",
      "982 [D Loss: 0.616276, acc: 73.44%] [G Loss: 0.811888]\n",
      "983 [D Loss: 0.595783, acc: 75.00%] [G Loss: 0.810098]\n",
      "984 [D Loss: 0.584561, acc: 70.31%] [G Loss: 0.860088]\n",
      "985 [D Loss: 0.653219, acc: 64.06%] [G Loss: 0.841322]\n",
      "986 [D Loss: 0.625783, acc: 62.50%] [G Loss: 0.839542]\n",
      "987 [D Loss: 0.581525, acc: 76.56%] [G Loss: 0.832505]\n",
      "988 [D Loss: 0.629791, acc: 68.75%] [G Loss: 0.803486]\n",
      "989 [D Loss: 0.621064, acc: 62.50%] [G Loss: 0.803037]\n",
      "990 [D Loss: 0.636827, acc: 70.31%] [G Loss: 0.817302]\n",
      "991 [D Loss: 0.566961, acc: 85.94%] [G Loss: 0.818951]\n",
      "992 [D Loss: 0.582513, acc: 79.69%] [G Loss: 0.789600]\n",
      "993 [D Loss: 0.598640, acc: 70.31%] [G Loss: 0.804405]\n",
      "994 [D Loss: 0.614921, acc: 68.75%] [G Loss: 0.832890]\n",
      "995 [D Loss: 0.583208, acc: 76.56%] [G Loss: 0.868449]\n",
      "996 [D Loss: 0.624245, acc: 70.31%] [G Loss: 0.861707]\n",
      "997 [D Loss: 0.557772, acc: 82.81%] [G Loss: 0.881430]\n",
      "998 [D Loss: 0.583312, acc: 75.00%] [G Loss: 0.860844]\n",
      "999 [D Loss: 0.555695, acc: 87.50%] [G Loss: 0.846406]\n",
      "1000 [D Loss: 0.637946, acc: 64.06%] [G Loss: 0.804220]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=1000, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_ML",
   "language": "python",
   "name": "new_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
