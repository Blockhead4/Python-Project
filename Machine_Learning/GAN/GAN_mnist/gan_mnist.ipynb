{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import random_normal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, kernel_initializer=random_normal(), input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(1024, kernel_initializer=random_normal()))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones(batch_size)\n",
    "        fake = np.zeros(batch_size)\n",
    "\n",
    "        for epoch in range(epochs+1):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,460,225\n",
      "Trainable params: 1,460,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.733001, acc.: 33.40%] [G loss: 0.581211]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jwp\\Anaconda3\\envs\\py_ml\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.491985, acc.: 56.90%] [G loss: 0.873986]\n",
      "2 [D loss: 0.251037, acc.: 94.05%] [G loss: 1.528532]\n",
      "3 [D loss: 0.123169, acc.: 99.95%] [G loss: 2.120720]\n",
      "4 [D loss: 0.075529, acc.: 100.00%] [G loss: 2.548415]\n",
      "5 [D loss: 0.057986, acc.: 100.00%] [G loss: 2.794393]\n",
      "6 [D loss: 0.050469, acc.: 100.00%] [G loss: 2.959202]\n",
      "7 [D loss: 0.042778, acc.: 100.00%] [G loss: 3.109859]\n",
      "8 [D loss: 0.037745, acc.: 100.00%] [G loss: 3.240162]\n",
      "9 [D loss: 0.033494, acc.: 100.00%] [G loss: 3.367568]\n",
      "10 [D loss: 0.029751, acc.: 100.00%] [G loss: 3.520442]\n",
      "11 [D loss: 0.024936, acc.: 100.00%] [G loss: 3.629642]\n",
      "12 [D loss: 0.022724, acc.: 100.00%] [G loss: 3.767442]\n",
      "13 [D loss: 0.021193, acc.: 99.98%] [G loss: 3.858801]\n",
      "14 [D loss: 0.018754, acc.: 100.00%] [G loss: 4.021739]\n",
      "15 [D loss: 0.016756, acc.: 100.00%] [G loss: 4.117403]\n",
      "16 [D loss: 0.015267, acc.: 100.00%] [G loss: 4.206571]\n",
      "17 [D loss: 0.014107, acc.: 100.00%] [G loss: 4.308062]\n",
      "18 [D loss: 0.013274, acc.: 100.00%] [G loss: 4.394947]\n",
      "19 [D loss: 0.013633, acc.: 99.98%] [G loss: 4.447453]\n",
      "20 [D loss: 0.012789, acc.: 100.00%] [G loss: 4.504862]\n",
      "21 [D loss: 0.011339, acc.: 100.00%] [G loss: 4.561995]\n",
      "22 [D loss: 0.012279, acc.: 100.00%] [G loss: 4.674483]\n",
      "23 [D loss: 0.011025, acc.: 100.00%] [G loss: 4.724164]\n",
      "24 [D loss: 0.013530, acc.: 99.98%] [G loss: 4.783533]\n",
      "25 [D loss: 0.013832, acc.: 99.98%] [G loss: 4.873877]\n",
      "26 [D loss: 0.014326, acc.: 99.95%] [G loss: 4.925636]\n",
      "27 [D loss: 0.014656, acc.: 99.95%] [G loss: 5.057702]\n",
      "28 [D loss: 0.015810, acc.: 99.87%] [G loss: 5.086536]\n",
      "29 [D loss: 0.016372, acc.: 99.78%] [G loss: 5.082313]\n",
      "30 [D loss: 0.018160, acc.: 99.72%] [G loss: 5.195284]\n",
      "31 [D loss: 0.019493, acc.: 99.67%] [G loss: 5.304399]\n",
      "32 [D loss: 0.020518, acc.: 99.72%] [G loss: 5.264669]\n",
      "33 [D loss: 0.020363, acc.: 99.67%] [G loss: 5.315984]\n",
      "34 [D loss: 0.020802, acc.: 99.57%] [G loss: 5.377919]\n",
      "35 [D loss: 0.025936, acc.: 99.50%] [G loss: 5.507159]\n",
      "36 [D loss: 0.029843, acc.: 99.08%] [G loss: 5.760280]\n",
      "37 [D loss: 0.043807, acc.: 98.62%] [G loss: 5.853836]\n",
      "38 [D loss: 0.028252, acc.: 99.13%] [G loss: 6.140489]\n",
      "39 [D loss: 0.073911, acc.: 97.20%] [G loss: 5.887971]\n",
      "40 [D loss: 0.028189, acc.: 98.87%] [G loss: 6.645024]\n",
      "41 [D loss: 0.375203, acc.: 88.23%] [G loss: 4.490373]\n",
      "42 [D loss: 0.204578, acc.: 92.20%] [G loss: 5.114089]\n",
      "43 [D loss: 0.045102, acc.: 97.95%] [G loss: 6.081470]\n",
      "44 [D loss: 0.013987, acc.: 99.77%] [G loss: 6.226012]\n",
      "45 [D loss: 0.039395, acc.: 98.35%] [G loss: 6.083387]\n",
      "46 [D loss: 0.077610, acc.: 96.90%] [G loss: 5.894666]\n",
      "47 [D loss: 0.077939, acc.: 96.57%] [G loss: 6.114052]\n",
      "48 [D loss: 0.152722, acc.: 94.08%] [G loss: 5.746673]\n",
      "49 [D loss: 0.058767, acc.: 97.28%] [G loss: 6.191833]\n",
      "50 [D loss: 0.391200, acc.: 87.17%] [G loss: 4.344376]\n",
      "51 [D loss: 0.191838, acc.: 91.55%] [G loss: 4.912395]\n",
      "52 [D loss: 0.041208, acc.: 98.35%] [G loss: 5.949584]\n",
      "53 [D loss: 0.043278, acc.: 99.45%] [G loss: 5.542402]\n",
      "54 [D loss: 0.102181, acc.: 95.57%] [G loss: 5.324153]\n",
      "55 [D loss: 0.116214, acc.: 95.30%] [G loss: 5.406092]\n",
      "56 [D loss: 0.142425, acc.: 93.92%] [G loss: 5.602942]\n",
      "57 [D loss: 0.113030, acc.: 95.27%] [G loss: 5.679045]\n",
      "58 [D loss: 0.149475, acc.: 93.70%] [G loss: 5.207174]\n",
      "59 [D loss: 0.072582, acc.: 96.60%] [G loss: 5.493796]\n",
      "60 [D loss: 0.416040, acc.: 85.70%] [G loss: 4.280166]\n",
      "61 [D loss: 0.171410, acc.: 91.73%] [G loss: 4.692302]\n",
      "62 [D loss: 0.035377, acc.: 99.52%] [G loss: 5.277865]\n",
      "63 [D loss: 0.134324, acc.: 94.95%] [G loss: 4.596599]\n",
      "64 [D loss: 0.107262, acc.: 94.80%] [G loss: 5.077811]\n",
      "65 [D loss: 0.241809, acc.: 90.08%] [G loss: 4.813114]\n",
      "66 [D loss: 0.113470, acc.: 94.00%] [G loss: 5.664361]\n",
      "67 [D loss: 0.437284, acc.: 82.53%] [G loss: 4.067204]\n",
      "68 [D loss: 0.188295, acc.: 90.58%] [G loss: 4.291617]\n",
      "69 [D loss: 0.045161, acc.: 99.20%] [G loss: 4.960310]\n",
      "70 [D loss: 0.078455, acc.: 98.00%] [G loss: 4.943151]\n",
      "71 [D loss: 0.157271, acc.: 92.40%] [G loss: 4.880358]\n",
      "72 [D loss: 0.176232, acc.: 91.35%] [G loss: 5.466467]\n",
      "73 [D loss: 0.248439, acc.: 88.93%] [G loss: 5.048787]\n",
      "74 [D loss: 0.103746, acc.: 94.48%] [G loss: 5.181803]\n",
      "75 [D loss: 0.180618, acc.: 91.58%] [G loss: 4.380797]\n",
      "76 [D loss: 0.126320, acc.: 93.32%] [G loss: 4.946000]\n",
      "77 [D loss: 0.525182, acc.: 78.30%] [G loss: 3.658538]\n",
      "78 [D loss: 0.174098, acc.: 89.30%] [G loss: 3.598901]\n",
      "79 [D loss: 0.054027, acc.: 99.98%] [G loss: 4.138250]\n",
      "80 [D loss: 0.084751, acc.: 99.42%] [G loss: 4.118843]\n",
      "81 [D loss: 0.172782, acc.: 91.10%] [G loss: 5.003045]\n",
      "82 [D loss: 0.250460, acc.: 87.78%] [G loss: 5.440550]\n",
      "83 [D loss: 0.150804, acc.: 91.85%] [G loss: 5.211369]\n",
      "84 [D loss: 0.312450, acc.: 85.22%] [G loss: 3.875702]\n",
      "85 [D loss: 0.107745, acc.: 95.82%] [G loss: 4.331053]\n",
      "86 [D loss: 0.047999, acc.: 99.87%] [G loss: 4.700804]\n",
      "87 [D loss: 0.224409, acc.: 88.10%] [G loss: 4.420290]\n",
      "88 [D loss: 0.131148, acc.: 93.53%] [G loss: 4.876761]\n",
      "89 [D loss: 0.366586, acc.: 82.15%] [G loss: 3.770417]\n",
      "90 [D loss: 0.093804, acc.: 97.42%] [G loss: 4.017735]\n",
      "91 [D loss: 0.072440, acc.: 99.73%] [G loss: 4.850625]\n",
      "92 [D loss: 0.253435, acc.: 86.75%] [G loss: 3.919456]\n",
      "93 [D loss: 0.111130, acc.: 95.05%] [G loss: 4.651602]\n",
      "94 [D loss: 0.138788, acc.: 95.40%] [G loss: 4.029079]\n",
      "95 [D loss: 0.121771, acc.: 94.92%] [G loss: 4.339731]\n",
      "96 [D loss: 0.387458, acc.: 80.02%] [G loss: 3.244006]\n",
      "97 [D loss: 0.102114, acc.: 97.93%] [G loss: 3.722879]\n",
      "98 [D loss: 0.053124, acc.: 100.00%] [G loss: 4.189410]\n",
      "99 [D loss: 0.198475, acc.: 89.85%] [G loss: 3.605416]\n",
      "100 [D loss: 0.096830, acc.: 96.63%] [G loss: 4.558963]\n",
      "101 [D loss: 0.191755, acc.: 89.78%] [G loss: 3.784810]\n",
      "102 [D loss: 0.078418, acc.: 99.55%] [G loss: 4.303901]\n",
      "103 [D loss: 0.147947, acc.: 93.45%] [G loss: 4.075559]\n",
      "104 [D loss: 0.161958, acc.: 90.60%] [G loss: 4.334760]\n",
      "105 [D loss: 0.221870, acc.: 87.95%] [G loss: 4.058894]\n",
      "106 [D loss: 0.076183, acc.: 99.60%] [G loss: 4.518299]\n",
      "107 [D loss: 0.332729, acc.: 81.58%] [G loss: 3.353341]\n",
      "108 [D loss: 0.077806, acc.: 99.02%] [G loss: 4.295370]\n",
      "109 [D loss: 0.093057, acc.: 99.20%] [G loss: 4.222517]\n",
      "110 [D loss: 0.143375, acc.: 92.27%] [G loss: 5.206124]\n",
      "111 [D loss: 0.161187, acc.: 90.85%] [G loss: 4.626067]\n",
      "112 [D loss: 0.122576, acc.: 96.05%] [G loss: 4.467393]\n",
      "113 [D loss: 0.324439, acc.: 82.12%] [G loss: 3.643372]\n",
      "114 [D loss: 0.068891, acc.: 99.50%] [G loss: 4.751226]\n",
      "115 [D loss: 0.293971, acc.: 84.05%] [G loss: 3.273650]\n",
      "116 [D loss: 0.085926, acc.: 98.25%] [G loss: 4.852760]\n",
      "117 [D loss: 0.118795, acc.: 96.57%] [G loss: 4.442223]\n",
      "118 [D loss: 0.133156, acc.: 93.83%] [G loss: 4.670142]\n",
      "119 [D loss: 0.208624, acc.: 88.17%] [G loss: 4.404036]\n",
      "120 [D loss: 0.115398, acc.: 96.55%] [G loss: 4.710960]\n",
      "121 [D loss: 0.343257, acc.: 82.12%] [G loss: 3.674759]\n",
      "122 [D loss: 0.070343, acc.: 99.37%] [G loss: 5.003019]\n",
      "123 [D loss: 0.124602, acc.: 97.10%] [G loss: 4.201043]\n",
      "124 [D loss: 0.090216, acc.: 97.87%] [G loss: 4.885236]\n",
      "125 [D loss: 0.277110, acc.: 85.22%] [G loss: 4.408669]\n",
      "126 [D loss: 0.071849, acc.: 98.70%] [G loss: 5.557248]\n",
      "127 [D loss: 0.787568, acc.: 57.12%] [G loss: 2.450125]\n",
      "128 [D loss: 0.149643, acc.: 95.38%] [G loss: 3.872654]\n",
      "129 [D loss: 0.059171, acc.: 99.92%] [G loss: 4.976217]\n",
      "130 [D loss: 0.121966, acc.: 98.63%] [G loss: 3.709989]\n",
      "131 [D loss: 0.151812, acc.: 96.18%] [G loss: 5.188444]\n",
      "132 [D loss: 0.196777, acc.: 92.15%] [G loss: 3.859356]\n",
      "133 [D loss: 0.081918, acc.: 99.07%] [G loss: 4.608263]\n",
      "134 [D loss: 0.179552, acc.: 93.85%] [G loss: 4.632378]\n",
      "135 [D loss: 0.212222, acc.: 90.30%] [G loss: 4.204757]\n",
      "136 [D loss: 0.109959, acc.: 98.07%] [G loss: 4.877173]\n",
      "137 [D loss: 0.681390, acc.: 60.35%] [G loss: 3.380497]\n",
      "138 [D loss: 0.098788, acc.: 99.37%] [G loss: 5.327651]\n",
      "139 [D loss: 0.775775, acc.: 54.68%] [G loss: 1.546078]\n",
      "140 [D loss: 0.392306, acc.: 75.75%] [G loss: 3.246477]\n",
      "141 [D loss: 0.107333, acc.: 99.77%] [G loss: 4.694697]\n",
      "142 [D loss: 0.292354, acc.: 92.47%] [G loss: 2.702473]\n",
      "143 [D loss: 0.161670, acc.: 95.00%] [G loss: 3.922237]\n",
      "144 [D loss: 0.142426, acc.: 98.72%] [G loss: 4.318758]\n",
      "145 [D loss: 0.272302, acc.: 88.22%] [G loss: 3.240659]\n",
      "146 [D loss: 0.168882, acc.: 97.75%] [G loss: 4.918770]\n",
      "147 [D loss: 0.792520, acc.: 54.07%] [G loss: 1.335965]\n",
      "148 [D loss: 0.240148, acc.: 87.67%] [G loss: 2.773303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 [D loss: 0.190040, acc.: 98.57%] [G loss: 3.393847]\n",
      "150 [D loss: 0.266404, acc.: 94.92%] [G loss: 2.471030]\n",
      "151 [D loss: 0.220061, acc.: 90.40%] [G loss: 4.060589]\n",
      "152 [D loss: 0.461097, acc.: 70.23%] [G loss: 2.149742]\n",
      "153 [D loss: 0.212699, acc.: 93.62%] [G loss: 3.702365]\n",
      "154 [D loss: 0.494626, acc.: 68.15%] [G loss: 2.503268]\n",
      "155 [D loss: 0.187417, acc.: 96.60%] [G loss: 4.011427]\n",
      "156 [D loss: 0.437004, acc.: 77.80%] [G loss: 2.126423]\n",
      "157 [D loss: 0.183736, acc.: 98.58%] [G loss: 3.026796]\n",
      "158 [D loss: 0.466835, acc.: 77.40%] [G loss: 2.108689]\n",
      "159 [D loss: 0.191129, acc.: 92.55%] [G loss: 3.614312]\n",
      "160 [D loss: 0.355060, acc.: 86.52%] [G loss: 2.259910]\n",
      "161 [D loss: 0.203701, acc.: 94.50%] [G loss: 3.192406]\n",
      "162 [D loss: 0.437138, acc.: 82.80%] [G loss: 2.365353]\n",
      "163 [D loss: 0.211935, acc.: 97.40%] [G loss: 3.292119]\n",
      "164 [D loss: 0.625419, acc.: 56.27%] [G loss: 1.462229]\n",
      "165 [D loss: 0.282530, acc.: 85.55%] [G loss: 2.567493]\n",
      "166 [D loss: 0.556791, acc.: 71.25%] [G loss: 1.861448]\n",
      "167 [D loss: 0.214855, acc.: 91.47%] [G loss: 3.469086]\n",
      "168 [D loss: 0.405346, acc.: 84.10%] [G loss: 1.777420]\n",
      "169 [D loss: 0.263858, acc.: 94.08%] [G loss: 2.545656]\n",
      "170 [D loss: 0.464701, acc.: 83.60%] [G loss: 1.845609]\n",
      "171 [D loss: 0.250618, acc.: 97.65%] [G loss: 2.818944]\n",
      "172 [D loss: 0.529694, acc.: 69.20%] [G loss: 1.685053]\n",
      "173 [D loss: 0.260020, acc.: 96.45%] [G loss: 2.521065]\n",
      "174 [D loss: 0.556897, acc.: 69.40%] [G loss: 1.685018]\n",
      "175 [D loss: 0.256452, acc.: 96.00%] [G loss: 2.517344]\n",
      "176 [D loss: 0.516639, acc.: 76.02%] [G loss: 1.538406]\n",
      "177 [D loss: 0.275652, acc.: 97.57%] [G loss: 2.129782]\n",
      "178 [D loss: 0.593791, acc.: 63.20%] [G loss: 1.544130]\n",
      "179 [D loss: 0.260429, acc.: 97.50%] [G loss: 2.447163]\n",
      "180 [D loss: 0.635832, acc.: 62.87%] [G loss: 1.318501]\n",
      "181 [D loss: 0.273357, acc.: 96.88%] [G loss: 2.046331]\n",
      "182 [D loss: 0.604356, acc.: 68.17%] [G loss: 1.253745]\n",
      "183 [D loss: 0.272365, acc.: 98.80%] [G loss: 2.115339]\n",
      "184 [D loss: 0.439431, acc.: 86.83%] [G loss: 1.620444]\n",
      "185 [D loss: 0.301493, acc.: 95.15%] [G loss: 2.146952]\n",
      "186 [D loss: 0.448040, acc.: 86.10%] [G loss: 1.710812]\n",
      "187 [D loss: 0.349896, acc.: 90.58%] [G loss: 2.062104]\n",
      "188 [D loss: 0.548091, acc.: 77.50%] [G loss: 1.322939]\n",
      "189 [D loss: 0.304622, acc.: 93.47%] [G loss: 1.952407]\n",
      "190 [D loss: 0.529495, acc.: 79.00%] [G loss: 1.364244]\n",
      "191 [D loss: 0.337023, acc.: 90.52%] [G loss: 1.936548]\n",
      "192 [D loss: 0.609864, acc.: 74.10%] [G loss: 1.222502]\n",
      "193 [D loss: 0.325524, acc.: 92.30%] [G loss: 1.686456]\n",
      "194 [D loss: 0.529928, acc.: 78.50%] [G loss: 1.388363]\n",
      "195 [D loss: 0.343738, acc.: 90.97%] [G loss: 1.987690]\n",
      "196 [D loss: 0.630180, acc.: 71.57%] [G loss: 1.230317]\n",
      "197 [D loss: 0.337879, acc.: 90.97%] [G loss: 1.605349]\n",
      "198 [D loss: 0.536388, acc.: 77.57%] [G loss: 1.239156]\n",
      "199 [D loss: 0.331785, acc.: 90.62%] [G loss: 1.802291]\n",
      "200 [D loss: 0.511513, acc.: 79.00%] [G loss: 1.342726]\n",
      "201 [D loss: 0.345129, acc.: 89.08%] [G loss: 1.708216]\n",
      "202 [D loss: 0.499090, acc.: 78.28%] [G loss: 1.388326]\n",
      "203 [D loss: 0.368951, acc.: 88.32%] [G loss: 1.614902]\n",
      "204 [D loss: 0.499046, acc.: 77.65%] [G loss: 1.401061]\n",
      "205 [D loss: 0.370554, acc.: 89.47%] [G loss: 1.579752]\n",
      "206 [D loss: 0.470776, acc.: 80.02%] [G loss: 1.440344]\n",
      "207 [D loss: 0.404872, acc.: 86.20%] [G loss: 1.520795]\n",
      "208 [D loss: 0.458033, acc.: 81.97%] [G loss: 1.473795]\n",
      "209 [D loss: 0.440485, acc.: 83.33%] [G loss: 1.480905]\n",
      "210 [D loss: 0.452213, acc.: 82.23%] [G loss: 1.481704]\n",
      "211 [D loss: 0.419695, acc.: 85.68%] [G loss: 1.506310]\n",
      "212 [D loss: 0.415320, acc.: 86.10%] [G loss: 1.568685]\n",
      "213 [D loss: 0.409293, acc.: 85.45%] [G loss: 1.549107]\n",
      "214 [D loss: 0.433965, acc.: 82.98%] [G loss: 1.546235]\n",
      "215 [D loss: 0.436414, acc.: 82.38%] [G loss: 1.559442]\n",
      "216 [D loss: 0.435827, acc.: 83.00%] [G loss: 1.546376]\n",
      "217 [D loss: 0.434552, acc.: 82.62%] [G loss: 1.589077]\n",
      "218 [D loss: 0.433047, acc.: 82.23%] [G loss: 1.669479]\n",
      "219 [D loss: 0.448152, acc.: 81.88%] [G loss: 1.602178]\n",
      "220 [D loss: 0.410119, acc.: 84.07%] [G loss: 1.679254]\n",
      "221 [D loss: 0.465041, acc.: 78.15%] [G loss: 1.666293]\n",
      "222 [D loss: 0.435023, acc.: 81.00%] [G loss: 1.747233]\n",
      "223 [D loss: 0.487644, acc.: 76.90%] [G loss: 1.605568]\n",
      "224 [D loss: 0.428173, acc.: 81.60%] [G loss: 1.738482]\n",
      "225 [D loss: 0.455370, acc.: 79.95%] [G loss: 1.705041]\n",
      "226 [D loss: 0.422903, acc.: 84.00%] [G loss: 1.862613]\n",
      "227 [D loss: 0.476862, acc.: 78.08%] [G loss: 1.704151]\n",
      "228 [D loss: 0.425826, acc.: 83.00%] [G loss: 1.763190]\n",
      "229 [D loss: 0.453562, acc.: 81.38%] [G loss: 1.712700]\n",
      "230 [D loss: 0.398088, acc.: 87.10%] [G loss: 1.815777]\n",
      "231 [D loss: 0.419644, acc.: 84.57%] [G loss: 1.744328]\n",
      "232 [D loss: 0.419214, acc.: 83.45%] [G loss: 1.799805]\n",
      "233 [D loss: 0.411497, acc.: 84.68%] [G loss: 1.839831]\n",
      "234 [D loss: 0.419492, acc.: 84.83%] [G loss: 1.753011]\n",
      "235 [D loss: 0.382224, acc.: 86.85%] [G loss: 1.827732]\n",
      "236 [D loss: 0.422064, acc.: 82.03%] [G loss: 1.836244]\n",
      "237 [D loss: 0.405948, acc.: 84.33%] [G loss: 1.861345]\n",
      "238 [D loss: 0.423276, acc.: 81.88%] [G loss: 1.838005]\n",
      "239 [D loss: 0.440086, acc.: 79.97%] [G loss: 1.804704]\n",
      "240 [D loss: 0.414380, acc.: 84.40%] [G loss: 1.843205]\n",
      "241 [D loss: 0.400517, acc.: 86.20%] [G loss: 1.828748]\n",
      "242 [D loss: 0.417800, acc.: 83.03%] [G loss: 1.870648]\n",
      "243 [D loss: 0.419075, acc.: 82.77%] [G loss: 1.805640]\n",
      "244 [D loss: 0.391782, acc.: 84.80%] [G loss: 1.967700]\n",
      "245 [D loss: 0.458245, acc.: 81.27%] [G loss: 1.904727]\n",
      "246 [D loss: 0.393387, acc.: 85.57%] [G loss: 2.041587]\n",
      "247 [D loss: 0.547360, acc.: 72.57%] [G loss: 1.729402]\n",
      "248 [D loss: 0.355785, acc.: 88.45%] [G loss: 2.094848]\n",
      "249 [D loss: 0.580126, acc.: 70.18%] [G loss: 1.691026]\n",
      "250 [D loss: 0.358068, acc.: 87.58%] [G loss: 2.173864]\n",
      "251 [D loss: 0.558306, acc.: 68.50%] [G loss: 1.773166]\n",
      "252 [D loss: 0.370779, acc.: 88.67%] [G loss: 2.141071]\n",
      "253 [D loss: 0.510609, acc.: 72.07%] [G loss: 1.918271]\n",
      "254 [D loss: 0.413599, acc.: 83.83%] [G loss: 2.023928]\n",
      "255 [D loss: 0.459556, acc.: 81.67%] [G loss: 1.852356]\n",
      "256 [D loss: 0.384399, acc.: 85.03%] [G loss: 2.070832]\n",
      "257 [D loss: 0.451115, acc.: 80.80%] [G loss: 1.845452]\n",
      "258 [D loss: 0.333760, acc.: 90.75%] [G loss: 2.152530]\n",
      "259 [D loss: 0.470350, acc.: 78.25%] [G loss: 1.866079]\n",
      "260 [D loss: 0.348318, acc.: 88.78%] [G loss: 2.154026]\n",
      "261 [D loss: 0.474245, acc.: 78.15%] [G loss: 1.985241]\n",
      "262 [D loss: 0.336409, acc.: 92.12%] [G loss: 2.243185]\n",
      "263 [D loss: 0.457008, acc.: 78.87%] [G loss: 1.994451]\n",
      "264 [D loss: 0.312367, acc.: 93.15%] [G loss: 2.242712]\n",
      "265 [D loss: 0.440595, acc.: 80.60%] [G loss: 2.084885]\n",
      "266 [D loss: 0.313521, acc.: 96.32%] [G loss: 2.227423]\n",
      "267 [D loss: 0.389283, acc.: 86.30%] [G loss: 2.196049]\n",
      "268 [D loss: 0.358787, acc.: 91.20%] [G loss: 2.255274]\n",
      "269 [D loss: 0.361317, acc.: 90.45%] [G loss: 2.302954]\n",
      "270 [D loss: 0.404806, acc.: 85.48%] [G loss: 2.274496]\n",
      "271 [D loss: 0.357694, acc.: 89.52%] [G loss: 2.323638]\n",
      "272 [D loss: 0.381552, acc.: 88.02%] [G loss: 2.309094]\n",
      "273 [D loss: 0.397905, acc.: 86.00%] [G loss: 2.376519]\n",
      "274 [D loss: 0.396192, acc.: 86.87%] [G loss: 2.416530]\n",
      "275 [D loss: 0.368317, acc.: 90.62%] [G loss: 2.496918]\n",
      "276 [D loss: 0.405020, acc.: 85.60%] [G loss: 2.559315]\n",
      "277 [D loss: 0.420734, acc.: 80.95%] [G loss: 2.611058]\n",
      "278 [D loss: 0.455205, acc.: 81.03%] [G loss: 2.443958]\n",
      "279 [D loss: 0.349227, acc.: 92.10%] [G loss: 2.536655]\n",
      "280 [D loss: 0.428586, acc.: 80.25%] [G loss: 2.561435]\n",
      "281 [D loss: 0.421882, acc.: 82.92%] [G loss: 2.480036]\n",
      "282 [D loss: 0.442563, acc.: 79.77%] [G loss: 2.511723]\n",
      "283 [D loss: 0.444968, acc.: 81.53%] [G loss: 2.355135]\n",
      "284 [D loss: 0.337248, acc.: 91.15%] [G loss: 2.564207]\n",
      "285 [D loss: 0.508076, acc.: 74.83%] [G loss: 2.423473]\n",
      "286 [D loss: 0.374800, acc.: 89.45%] [G loss: 2.364730]\n",
      "287 [D loss: 0.412940, acc.: 82.90%] [G loss: 2.566312]\n",
      "288 [D loss: 0.469306, acc.: 81.12%] [G loss: 2.381180]\n",
      "289 [D loss: 0.379296, acc.: 88.55%] [G loss: 2.428336]\n",
      "290 [D loss: 0.516015, acc.: 75.58%] [G loss: 2.502180]\n",
      "291 [D loss: 0.388975, acc.: 92.55%] [G loss: 2.288702]\n",
      "292 [D loss: 0.381879, acc.: 87.23%] [G loss: 2.505383]\n",
      "293 [D loss: 0.491076, acc.: 81.18%] [G loss: 2.222156]\n",
      "294 [D loss: 0.310055, acc.: 96.15%] [G loss: 2.463898]\n",
      "295 [D loss: 0.437521, acc.: 83.85%] [G loss: 2.520305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 [D loss: 0.380384, acc.: 91.08%] [G loss: 2.408205]\n",
      "297 [D loss: 0.367747, acc.: 92.47%] [G loss: 2.409122]\n",
      "298 [D loss: 0.358851, acc.: 92.75%] [G loss: 2.365608]\n",
      "299 [D loss: 0.320276, acc.: 92.82%] [G loss: 2.502269]\n",
      "300 [D loss: 0.422457, acc.: 82.75%] [G loss: 2.402270]\n",
      "301 [D loss: 0.339301, acc.: 93.97%] [G loss: 2.430010]\n",
      "302 [D loss: 0.424968, acc.: 82.55%] [G loss: 2.643433]\n",
      "303 [D loss: 0.451811, acc.: 84.42%] [G loss: 2.559812]\n",
      "304 [D loss: 0.349117, acc.: 93.38%] [G loss: 2.750500]\n",
      "305 [D loss: 0.478903, acc.: 79.93%] [G loss: 2.675185]\n",
      "306 [D loss: 0.352869, acc.: 91.97%] [G loss: 2.654863]\n",
      "307 [D loss: 0.419928, acc.: 82.20%] [G loss: 2.833403]\n",
      "308 [D loss: 0.493369, acc.: 77.10%] [G loss: 2.605088]\n",
      "309 [D loss: 0.338634, acc.: 92.65%] [G loss: 2.694551]\n",
      "310 [D loss: 0.452772, acc.: 78.08%] [G loss: 2.732785]\n",
      "311 [D loss: 0.435133, acc.: 84.48%] [G loss: 2.604622]\n",
      "312 [D loss: 0.413247, acc.: 85.53%] [G loss: 2.730121]\n",
      "313 [D loss: 0.446738, acc.: 83.60%] [G loss: 2.722152]\n",
      "314 [D loss: 0.392040, acc.: 88.00%] [G loss: 2.698606]\n",
      "315 [D loss: 0.419544, acc.: 83.72%] [G loss: 2.914304]\n",
      "316 [D loss: 0.603734, acc.: 69.75%] [G loss: 2.642810]\n",
      "317 [D loss: 0.315572, acc.: 94.57%] [G loss: 2.483688]\n",
      "318 [D loss: 0.403774, acc.: 84.15%] [G loss: 2.725332]\n",
      "319 [D loss: 0.426850, acc.: 87.35%] [G loss: 2.524199]\n",
      "320 [D loss: 0.315727, acc.: 95.40%] [G loss: 2.678598]\n",
      "321 [D loss: 0.438980, acc.: 83.10%] [G loss: 2.680803]\n",
      "322 [D loss: 0.334858, acc.: 95.82%] [G loss: 2.536457]\n",
      "323 [D loss: 0.309053, acc.: 95.25%] [G loss: 2.729747]\n",
      "324 [D loss: 0.362253, acc.: 91.08%] [G loss: 2.685252]\n",
      "325 [D loss: 0.331751, acc.: 93.12%] [G loss: 2.692743]\n",
      "326 [D loss: 0.336724, acc.: 94.75%] [G loss: 2.643476]\n",
      "327 [D loss: 0.297655, acc.: 95.05%] [G loss: 2.750766]\n",
      "328 [D loss: 0.351947, acc.: 91.35%] [G loss: 2.756782]\n",
      "329 [D loss: 0.358612, acc.: 94.18%] [G loss: 2.696007]\n",
      "330 [D loss: 0.313298, acc.: 95.90%] [G loss: 2.679721]\n",
      "331 [D loss: 0.312025, acc.: 96.53%] [G loss: 2.774700]\n",
      "332 [D loss: 0.319478, acc.: 95.30%] [G loss: 2.745901]\n",
      "333 [D loss: 0.316187, acc.: 95.48%] [G loss: 2.817595]\n",
      "334 [D loss: 0.346657, acc.: 91.32%] [G loss: 2.867126]\n",
      "335 [D loss: 0.343450, acc.: 93.12%] [G loss: 2.828131]\n",
      "336 [D loss: 0.294563, acc.: 95.75%] [G loss: 2.904711]\n",
      "337 [D loss: 0.337092, acc.: 88.80%] [G loss: 3.030214]\n",
      "338 [D loss: 0.351125, acc.: 90.93%] [G loss: 3.048162]\n",
      "339 [D loss: 0.397072, acc.: 85.37%] [G loss: 3.096282]\n",
      "340 [D loss: 0.432547, acc.: 84.68%] [G loss: 2.957061]\n",
      "341 [D loss: 0.330984, acc.: 91.85%] [G loss: 2.978609]\n",
      "342 [D loss: 0.451293, acc.: 76.75%] [G loss: 3.167463]\n",
      "343 [D loss: 0.426543, acc.: 83.78%] [G loss: 2.668737]\n",
      "344 [D loss: 0.254082, acc.: 96.03%] [G loss: 2.940983]\n",
      "345 [D loss: 0.356620, acc.: 85.90%] [G loss: 3.046393]\n",
      "346 [D loss: 0.376500, acc.: 86.87%] [G loss: 2.865703]\n",
      "347 [D loss: 0.313679, acc.: 90.38%] [G loss: 2.897414]\n",
      "348 [D loss: 0.399270, acc.: 83.22%] [G loss: 3.135753]\n",
      "349 [D loss: 0.492791, acc.: 76.35%] [G loss: 2.816543]\n",
      "350 [D loss: 0.251617, acc.: 94.33%] [G loss: 3.071751]\n",
      "351 [D loss: 0.488390, acc.: 76.48%] [G loss: 3.204617]\n",
      "352 [D loss: 0.491448, acc.: 76.73%] [G loss: 2.745981]\n",
      "353 [D loss: 0.212669, acc.: 97.07%] [G loss: 2.972413]\n",
      "354 [D loss: 0.417470, acc.: 78.88%] [G loss: 3.125594]\n",
      "355 [D loss: 0.425474, acc.: 81.98%] [G loss: 2.941837]\n",
      "356 [D loss: 0.337044, acc.: 90.68%] [G loss: 3.019110]\n",
      "357 [D loss: 0.449377, acc.: 76.88%] [G loss: 3.200228]\n",
      "358 [D loss: 0.457829, acc.: 78.45%] [G loss: 2.779054]\n",
      "359 [D loss: 0.242983, acc.: 96.07%] [G loss: 2.953618]\n",
      "360 [D loss: 0.392145, acc.: 79.25%] [G loss: 3.124202]\n",
      "361 [D loss: 0.452223, acc.: 76.05%] [G loss: 2.980695]\n",
      "362 [D loss: 0.335310, acc.: 90.22%] [G loss: 2.970731]\n",
      "363 [D loss: 0.386560, acc.: 81.60%] [G loss: 3.214086]\n",
      "364 [D loss: 0.488208, acc.: 75.08%] [G loss: 3.143848]\n",
      "365 [D loss: 0.378715, acc.: 88.78%] [G loss: 2.969336]\n",
      "366 [D loss: 0.381112, acc.: 83.38%] [G loss: 3.489510]\n",
      "367 [D loss: 0.750730, acc.: 64.67%] [G loss: 3.209719]\n",
      "368 [D loss: 0.317450, acc.: 90.35%] [G loss: 2.945473]\n",
      "369 [D loss: 0.383338, acc.: 85.53%] [G loss: 3.609396]\n",
      "370 [D loss: 0.774846, acc.: 61.65%] [G loss: 3.061289]\n",
      "371 [D loss: 0.296999, acc.: 93.48%] [G loss: 2.773222]\n",
      "372 [D loss: 0.379243, acc.: 86.90%] [G loss: 2.999907]\n",
      "373 [D loss: 0.465345, acc.: 77.15%] [G loss: 2.858356]\n",
      "374 [D loss: 0.386621, acc.: 88.73%] [G loss: 2.860511]\n",
      "375 [D loss: 0.423283, acc.: 85.03%] [G loss: 2.949229]\n",
      "376 [D loss: 0.413537, acc.: 85.62%] [G loss: 2.990038]\n",
      "377 [D loss: 0.453219, acc.: 78.43%] [G loss: 3.014257]\n",
      "378 [D loss: 0.420958, acc.: 83.40%] [G loss: 2.878730]\n",
      "379 [D loss: 0.433906, acc.: 84.20%] [G loss: 2.844207]\n",
      "380 [D loss: 0.371052, acc.: 88.80%] [G loss: 2.893453]\n",
      "381 [D loss: 0.376642, acc.: 88.87%] [G loss: 2.966555]\n",
      "382 [D loss: 0.391539, acc.: 86.65%] [G loss: 3.079065]\n",
      "383 [D loss: 0.404816, acc.: 85.68%] [G loss: 2.982327]\n",
      "384 [D loss: 0.352025, acc.: 91.18%] [G loss: 2.968277]\n",
      "385 [D loss: 0.335708, acc.: 92.30%] [G loss: 3.057287]\n",
      "386 [D loss: 0.390040, acc.: 88.28%] [G loss: 3.007509]\n",
      "387 [D loss: 0.322469, acc.: 92.88%] [G loss: 2.891068]\n",
      "388 [D loss: 0.327439, acc.: 92.32%] [G loss: 3.108842]\n",
      "389 [D loss: 0.359131, acc.: 90.62%] [G loss: 3.043863]\n",
      "390 [D loss: 0.303231, acc.: 94.38%] [G loss: 3.053185]\n",
      "391 [D loss: 0.323747, acc.: 93.23%] [G loss: 3.034583]\n",
      "392 [D loss: 0.312666, acc.: 93.33%] [G loss: 3.107193]\n",
      "393 [D loss: 0.332921, acc.: 91.62%] [G loss: 3.218459]\n",
      "394 [D loss: 0.321564, acc.: 92.55%] [G loss: 3.116808]\n",
      "395 [D loss: 0.305197, acc.: 93.95%] [G loss: 3.249439]\n",
      "396 [D loss: 0.330654, acc.: 92.08%] [G loss: 3.219909]\n",
      "397 [D loss: 0.313520, acc.: 92.10%] [G loss: 3.230126]\n",
      "398 [D loss: 0.299863, acc.: 94.42%] [G loss: 3.236399]\n",
      "399 [D loss: 0.308280, acc.: 93.35%] [G loss: 3.334375]\n",
      "400 [D loss: 0.320844, acc.: 91.20%] [G loss: 3.396454]\n",
      "401 [D loss: 0.361838, acc.: 89.00%] [G loss: 3.390952]\n",
      "402 [D loss: 0.300500, acc.: 92.30%] [G loss: 3.252877]\n",
      "403 [D loss: 0.270601, acc.: 94.80%] [G loss: 3.458295]\n",
      "404 [D loss: 0.360062, acc.: 89.30%] [G loss: 3.540912]\n",
      "405 [D loss: 0.343614, acc.: 91.62%] [G loss: 3.198579]\n",
      "406 [D loss: 0.265741, acc.: 92.15%] [G loss: 3.583629]\n",
      "407 [D loss: 0.428901, acc.: 81.62%] [G loss: 3.381075]\n",
      "408 [D loss: 0.232827, acc.: 94.67%] [G loss: 3.041528]\n",
      "409 [D loss: 0.244848, acc.: 96.78%] [G loss: 3.572340]\n",
      "410 [D loss: 0.396095, acc.: 88.63%] [G loss: 3.455678]\n",
      "411 [D loss: 0.261506, acc.: 94.82%] [G loss: 3.102164]\n",
      "412 [D loss: 0.246778, acc.: 96.92%] [G loss: 3.597620]\n",
      "413 [D loss: 0.381021, acc.: 89.37%] [G loss: 3.568112]\n",
      "414 [D loss: 0.266549, acc.: 92.65%] [G loss: 3.377671]\n",
      "415 [D loss: 0.305223, acc.: 93.15%] [G loss: 3.638528]\n",
      "416 [D loss: 0.428882, acc.: 83.85%] [G loss: 3.721265]\n",
      "417 [D loss: 0.285429, acc.: 92.45%] [G loss: 2.983934]\n",
      "418 [D loss: 0.204285, acc.: 95.97%] [G loss: 3.883894]\n",
      "419 [D loss: 0.576053, acc.: 69.08%] [G loss: 4.161528]\n",
      "420 [D loss: 0.371774, acc.: 87.78%] [G loss: 2.712198]\n",
      "421 [D loss: 0.190552, acc.: 99.05%] [G loss: 3.774407]\n",
      "422 [D loss: 0.474693, acc.: 74.40%] [G loss: 3.661970]\n",
      "423 [D loss: 0.334188, acc.: 91.35%] [G loss: 3.122449]\n",
      "424 [D loss: 0.293839, acc.: 88.32%] [G loss: 4.104069]\n",
      "425 [D loss: 0.863890, acc.: 59.07%] [G loss: 3.823964]\n",
      "426 [D loss: 0.286091, acc.: 91.00%] [G loss: 3.207215]\n",
      "427 [D loss: 0.257832, acc.: 92.97%] [G loss: 3.376174]\n",
      "428 [D loss: 0.325319, acc.: 90.55%] [G loss: 3.508994]\n",
      "429 [D loss: 0.462723, acc.: 78.67%] [G loss: 3.444764]\n",
      "430 [D loss: 0.336149, acc.: 91.17%] [G loss: 3.092937]\n",
      "431 [D loss: 0.225163, acc.: 96.40%] [G loss: 3.447071]\n",
      "432 [D loss: 0.343373, acc.: 88.10%] [G loss: 3.654105]\n",
      "433 [D loss: 0.399331, acc.: 81.53%] [G loss: 3.658466]\n",
      "434 [D loss: 0.378667, acc.: 86.70%] [G loss: 3.311044]\n",
      "435 [D loss: 0.202429, acc.: 97.33%] [G loss: 3.227100]\n",
      "436 [D loss: 0.309620, acc.: 93.83%] [G loss: 3.802113]\n",
      "437 [D loss: 0.426585, acc.: 82.65%] [G loss: 3.499671]\n",
      "438 [D loss: 0.208805, acc.: 95.95%] [G loss: 3.337375]\n",
      "439 [D loss: 0.276546, acc.: 94.45%] [G loss: 3.845691]\n",
      "440 [D loss: 0.503271, acc.: 71.88%] [G loss: 4.181316]\n",
      "441 [D loss: 0.422462, acc.: 81.05%] [G loss: 3.045152]\n",
      "442 [D loss: 0.223950, acc.: 89.75%] [G loss: 4.026839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443 [D loss: 0.467521, acc.: 75.87%] [G loss: 3.429707]\n",
      "444 [D loss: 0.265076, acc.: 89.82%] [G loss: 3.991855]\n",
      "445 [D loss: 0.459208, acc.: 77.65%] [G loss: 3.398256]\n",
      "446 [D loss: 0.250025, acc.: 90.08%] [G loss: 3.860189]\n",
      "447 [D loss: 0.383301, acc.: 84.05%] [G loss: 3.305679]\n",
      "448 [D loss: 0.231533, acc.: 96.77%] [G loss: 3.524150]\n",
      "449 [D loss: 0.353233, acc.: 89.75%] [G loss: 3.654751]\n",
      "450 [D loss: 0.349029, acc.: 89.23%] [G loss: 3.438555]\n",
      "451 [D loss: 0.240049, acc.: 95.88%] [G loss: 3.553129]\n",
      "452 [D loss: 0.316105, acc.: 91.60%] [G loss: 3.810696]\n",
      "453 [D loss: 0.379937, acc.: 88.52%] [G loss: 3.680097]\n",
      "454 [D loss: 0.247753, acc.: 93.97%] [G loss: 3.261126]\n",
      "455 [D loss: 0.235100, acc.: 96.80%] [G loss: 3.682425]\n",
      "456 [D loss: 0.298512, acc.: 93.47%] [G loss: 3.678545]\n",
      "457 [D loss: 0.236282, acc.: 96.05%] [G loss: 3.507010]\n",
      "458 [D loss: 0.243562, acc.: 96.38%] [G loss: 3.838997]\n",
      "459 [D loss: 0.310461, acc.: 93.30%] [G loss: 3.804195]\n",
      "460 [D loss: 0.208936, acc.: 95.82%] [G loss: 3.453840]\n",
      "461 [D loss: 0.237850, acc.: 97.37%] [G loss: 4.168623]\n",
      "462 [D loss: 0.399972, acc.: 84.78%] [G loss: 4.067134]\n",
      "463 [D loss: 0.263628, acc.: 90.65%] [G loss: 3.470255]\n",
      "464 [D loss: 0.231299, acc.: 95.15%] [G loss: 3.992916]\n",
      "465 [D loss: 0.373365, acc.: 83.22%] [G loss: 4.283845]\n",
      "466 [D loss: 0.457046, acc.: 74.35%] [G loss: 4.376787]\n",
      "467 [D loss: 0.343784, acc.: 84.95%] [G loss: 3.763990]\n",
      "468 [D loss: 0.241637, acc.: 91.17%] [G loss: 4.279672]\n",
      "469 [D loss: 0.425225, acc.: 80.18%] [G loss: 3.817808]\n",
      "470 [D loss: 0.237336, acc.: 92.47%] [G loss: 3.797123]\n",
      "471 [D loss: 0.310800, acc.: 87.50%] [G loss: 4.019668]\n",
      "472 [D loss: 0.348349, acc.: 84.38%] [G loss: 4.181111]\n",
      "473 [D loss: 0.342463, acc.: 83.55%] [G loss: 3.969608]\n",
      "474 [D loss: 0.285741, acc.: 87.87%] [G loss: 4.249393]\n",
      "475 [D loss: 0.375467, acc.: 81.53%] [G loss: 4.157961]\n",
      "476 [D loss: 0.324401, acc.: 85.15%] [G loss: 4.139431]\n",
      "477 [D loss: 0.304877, acc.: 88.72%] [G loss: 3.798483]\n",
      "478 [D loss: 0.300319, acc.: 90.22%] [G loss: 4.752419]\n",
      "479 [D loss: 0.758681, acc.: 59.82%] [G loss: 4.956211]\n",
      "480 [D loss: 0.431037, acc.: 81.85%] [G loss: 3.218965]\n",
      "481 [D loss: 0.213490, acc.: 93.17%] [G loss: 4.103427]\n",
      "482 [D loss: 0.332036, acc.: 86.43%] [G loss: 4.096655]\n",
      "483 [D loss: 0.449111, acc.: 78.75%] [G loss: 4.642920]\n",
      "484 [D loss: 0.589823, acc.: 70.92%] [G loss: 4.121457]\n",
      "485 [D loss: 0.274362, acc.: 87.95%] [G loss: 3.995340]\n",
      "486 [D loss: 0.319806, acc.: 87.33%] [G loss: 4.331747]\n",
      "487 [D loss: 0.434592, acc.: 81.02%] [G loss: 3.926831]\n",
      "488 [D loss: 0.272010, acc.: 89.85%] [G loss: 3.822817]\n",
      "489 [D loss: 0.286876, acc.: 91.85%] [G loss: 4.159501]\n",
      "490 [D loss: 0.426149, acc.: 81.35%] [G loss: 4.514012]\n",
      "491 [D loss: 0.381745, acc.: 84.25%] [G loss: 3.853296]\n",
      "492 [D loss: 0.199077, acc.: 96.03%] [G loss: 3.810308]\n",
      "493 [D loss: 0.339948, acc.: 90.08%] [G loss: 4.721363]\n",
      "494 [D loss: 0.639920, acc.: 69.77%] [G loss: 4.384165]\n",
      "495 [D loss: 0.281756, acc.: 89.23%] [G loss: 3.267595]\n",
      "496 [D loss: 0.213350, acc.: 96.77%] [G loss: 3.859359]\n",
      "497 [D loss: 0.304054, acc.: 92.60%] [G loss: 3.694085]\n",
      "498 [D loss: 0.267644, acc.: 95.40%] [G loss: 3.854509]\n",
      "499 [D loss: 0.330715, acc.: 92.90%] [G loss: 4.052272]\n",
      "500 [D loss: 0.327938, acc.: 92.30%] [G loss: 3.642976]\n",
      "501 [D loss: 0.207943, acc.: 96.68%] [G loss: 3.680406]\n",
      "502 [D loss: 0.313944, acc.: 90.93%] [G loss: 4.534129]\n",
      "503 [D loss: 0.472708, acc.: 79.37%] [G loss: 4.204355]\n",
      "504 [D loss: 0.196728, acc.: 95.12%] [G loss: 3.340251]\n",
      "505 [D loss: 0.229085, acc.: 94.15%] [G loss: 4.368682]\n",
      "506 [D loss: 0.435802, acc.: 82.80%] [G loss: 4.124093]\n",
      "507 [D loss: 0.220848, acc.: 93.65%] [G loss: 3.209822]\n",
      "508 [D loss: 0.209814, acc.: 95.92%] [G loss: 4.115504]\n",
      "509 [D loss: 0.338285, acc.: 90.62%] [G loss: 3.714503]\n",
      "510 [D loss: 0.229228, acc.: 94.23%] [G loss: 3.930572]\n",
      "511 [D loss: 0.281397, acc.: 93.25%] [G loss: 3.892725]\n",
      "512 [D loss: 0.228716, acc.: 95.15%] [G loss: 3.843839]\n",
      "513 [D loss: 0.259339, acc.: 94.73%] [G loss: 3.866739]\n",
      "514 [D loss: 0.222266, acc.: 95.90%] [G loss: 3.733631]\n",
      "515 [D loss: 0.209386, acc.: 96.45%] [G loss: 3.988620]\n",
      "516 [D loss: 0.265885, acc.: 94.20%] [G loss: 4.122905]\n",
      "517 [D loss: 0.296984, acc.: 92.15%] [G loss: 4.158264]\n",
      "518 [D loss: 0.249136, acc.: 93.62%] [G loss: 3.857996]\n",
      "519 [D loss: 0.251368, acc.: 95.65%] [G loss: 4.309237]\n",
      "520 [D loss: 0.398519, acc.: 84.43%] [G loss: 4.555713]\n",
      "521 [D loss: 0.303853, acc.: 90.10%] [G loss: 3.275864]\n",
      "522 [D loss: 0.127896, acc.: 99.13%] [G loss: 3.961104]\n",
      "523 [D loss: 0.244898, acc.: 95.77%] [G loss: 4.217602]\n",
      "524 [D loss: 0.450767, acc.: 78.70%] [G loss: 5.038113]\n",
      "525 [D loss: 0.556967, acc.: 71.20%] [G loss: 3.662405]\n",
      "526 [D loss: 0.155957, acc.: 97.53%] [G loss: 3.912928]\n",
      "527 [D loss: 0.285267, acc.: 91.78%] [G loss: 3.914253]\n",
      "528 [D loss: 0.270682, acc.: 93.15%] [G loss: 3.871008]\n",
      "529 [D loss: 0.282104, acc.: 92.32%] [G loss: 4.249249]\n",
      "530 [D loss: 0.250040, acc.: 93.12%] [G loss: 4.003951]\n",
      "531 [D loss: 0.226759, acc.: 94.98%] [G loss: 4.316610]\n",
      "532 [D loss: 0.240125, acc.: 94.82%] [G loss: 4.058341]\n",
      "533 [D loss: 0.248028, acc.: 93.33%] [G loss: 4.313210]\n",
      "534 [D loss: 0.229443, acc.: 94.65%] [G loss: 3.883124]\n",
      "535 [D loss: 0.226503, acc.: 92.50%] [G loss: 4.513809]\n",
      "536 [D loss: 0.319173, acc.: 89.55%] [G loss: 4.207911]\n",
      "537 [D loss: 0.195795, acc.: 95.05%] [G loss: 4.218464]\n",
      "538 [D loss: 0.243738, acc.: 93.92%] [G loss: 4.593010]\n",
      "539 [D loss: 0.347102, acc.: 84.53%] [G loss: 5.296282]\n",
      "540 [D loss: 0.544836, acc.: 70.18%] [G loss: 5.028598]\n",
      "541 [D loss: 0.238674, acc.: 88.25%] [G loss: 5.055062]\n",
      "542 [D loss: 0.260985, acc.: 92.73%] [G loss: 3.610731]\n",
      "543 [D loss: 0.189573, acc.: 96.55%] [G loss: 4.775174]\n",
      "544 [D loss: 0.752704, acc.: 66.30%] [G loss: 5.582097]\n",
      "545 [D loss: 0.466486, acc.: 79.20%] [G loss: 2.823842]\n",
      "546 [D loss: 0.172956, acc.: 95.35%] [G loss: 4.126540]\n",
      "547 [D loss: 0.145183, acc.: 96.22%] [G loss: 3.667747]\n",
      "548 [D loss: 0.305065, acc.: 88.93%] [G loss: 4.438726]\n",
      "549 [D loss: 0.398920, acc.: 84.22%] [G loss: 3.613558]\n",
      "550 [D loss: 0.196669, acc.: 96.15%] [G loss: 3.844957]\n",
      "551 [D loss: 0.272462, acc.: 91.20%] [G loss: 4.395754]\n",
      "552 [D loss: 0.315820, acc.: 91.40%] [G loss: 4.169338]\n",
      "553 [D loss: 0.180756, acc.: 95.82%] [G loss: 3.882135]\n",
      "554 [D loss: 0.211099, acc.: 96.45%] [G loss: 4.427373]\n",
      "555 [D loss: 0.294997, acc.: 90.13%] [G loss: 4.680188]\n",
      "556 [D loss: 0.280700, acc.: 91.87%] [G loss: 3.894788]\n",
      "557 [D loss: 0.119306, acc.: 98.07%] [G loss: 4.070711]\n",
      "558 [D loss: 0.208998, acc.: 96.57%] [G loss: 4.756903]\n",
      "559 [D loss: 0.371214, acc.: 85.28%] [G loss: 5.114802]\n",
      "560 [D loss: 0.194888, acc.: 93.58%] [G loss: 3.678652]\n",
      "561 [D loss: 0.167806, acc.: 98.15%] [G loss: 5.215250]\n",
      "562 [D loss: 0.420278, acc.: 82.70%] [G loss: 5.242606]\n",
      "563 [D loss: 0.223111, acc.: 92.45%] [G loss: 3.487341]\n",
      "564 [D loss: 0.159948, acc.: 97.53%] [G loss: 5.147913]\n",
      "565 [D loss: 0.302285, acc.: 88.95%] [G loss: 3.988810]\n",
      "566 [D loss: 0.146106, acc.: 97.93%] [G loss: 4.749902]\n",
      "567 [D loss: 0.353336, acc.: 85.37%] [G loss: 5.405428]\n",
      "568 [D loss: 0.402528, acc.: 80.77%] [G loss: 4.625410]\n",
      "569 [D loss: 0.173554, acc.: 95.15%] [G loss: 4.091781]\n",
      "570 [D loss: 0.167127, acc.: 97.90%] [G loss: 4.604647]\n",
      "571 [D loss: 0.260193, acc.: 92.12%] [G loss: 4.499547]\n",
      "572 [D loss: 0.251086, acc.: 92.27%] [G loss: 5.074658]\n",
      "573 [D loss: 0.286270, acc.: 89.25%] [G loss: 4.821162]\n",
      "574 [D loss: 0.168591, acc.: 96.10%] [G loss: 4.497576]\n",
      "575 [D loss: 0.180557, acc.: 95.63%] [G loss: 4.588376]\n",
      "576 [D loss: 0.235144, acc.: 93.38%] [G loss: 4.990580]\n",
      "577 [D loss: 0.364853, acc.: 84.53%] [G loss: 5.217466]\n",
      "578 [D loss: 0.210770, acc.: 93.50%] [G loss: 4.171758]\n",
      "579 [D loss: 0.146977, acc.: 98.40%] [G loss: 4.575994]\n",
      "580 [D loss: 0.245194, acc.: 92.87%] [G loss: 5.349389]\n",
      "581 [D loss: 0.427506, acc.: 79.87%] [G loss: 5.680260]\n",
      "582 [D loss: 0.201841, acc.: 92.62%] [G loss: 3.677004]\n",
      "583 [D loss: 0.165029, acc.: 96.53%] [G loss: 5.240087]\n",
      "584 [D loss: 0.300903, acc.: 90.25%] [G loss: 4.090733]\n",
      "585 [D loss: 0.142483, acc.: 98.20%] [G loss: 4.410027]\n",
      "586 [D loss: 0.249221, acc.: 94.62%] [G loss: 4.880376]\n",
      "587 [D loss: 0.219860, acc.: 93.42%] [G loss: 4.071280]\n",
      "588 [D loss: 0.149999, acc.: 98.20%] [G loss: 4.715883]\n",
      "589 [D loss: 0.266005, acc.: 91.78%] [G loss: 5.238010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 [D loss: 0.280635, acc.: 91.67%] [G loss: 4.489670]\n",
      "591 [D loss: 0.159165, acc.: 96.42%] [G loss: 4.423234]\n",
      "592 [D loss: 0.249706, acc.: 92.68%] [G loss: 5.740496]\n",
      "593 [D loss: 0.686699, acc.: 61.45%] [G loss: 6.876118]\n",
      "594 [D loss: 0.337860, acc.: 84.60%] [G loss: 3.758067]\n",
      "595 [D loss: 0.151023, acc.: 97.48%] [G loss: 4.815889]\n",
      "596 [D loss: 0.086018, acc.: 97.70%] [G loss: 4.394578]\n",
      "597 [D loss: 0.233358, acc.: 93.55%] [G loss: 4.613567]\n",
      "598 [D loss: 0.215868, acc.: 94.02%] [G loss: 4.175692]\n",
      "599 [D loss: 0.201100, acc.: 93.62%] [G loss: 4.565181]\n",
      "600 [D loss: 0.286440, acc.: 91.03%] [G loss: 4.526464]\n",
      "601 [D loss: 0.186832, acc.: 92.10%] [G loss: 5.135504]\n",
      "602 [D loss: 0.348260, acc.: 84.38%] [G loss: 4.763655]\n",
      "603 [D loss: 0.246544, acc.: 88.57%] [G loss: 5.745213]\n",
      "604 [D loss: 0.494741, acc.: 75.30%] [G loss: 5.083505]\n",
      "605 [D loss: 0.143713, acc.: 95.12%] [G loss: 5.257413]\n",
      "606 [D loss: 0.168558, acc.: 95.72%] [G loss: 3.823623]\n",
      "607 [D loss: 0.170845, acc.: 98.25%] [G loss: 4.956708]\n",
      "608 [D loss: 0.423408, acc.: 81.77%] [G loss: 5.556812]\n",
      "609 [D loss: 0.445020, acc.: 80.98%] [G loss: 4.397919]\n",
      "610 [D loss: 0.117524, acc.: 97.42%] [G loss: 4.620172]\n",
      "611 [D loss: 0.191003, acc.: 95.40%] [G loss: 4.180237]\n",
      "612 [D loss: 0.170543, acc.: 97.12%] [G loss: 4.477576]\n",
      "613 [D loss: 0.327443, acc.: 87.17%] [G loss: 6.046318]\n",
      "614 [D loss: 0.883456, acc.: 54.75%] [G loss: 6.192944]\n",
      "615 [D loss: 0.295988, acc.: 87.80%] [G loss: 4.638787]\n",
      "616 [D loss: 0.140541, acc.: 95.77%] [G loss: 4.136809]\n",
      "617 [D loss: 0.124526, acc.: 98.15%] [G loss: 4.049073]\n",
      "618 [D loss: 0.209270, acc.: 96.18%] [G loss: 4.673359]\n",
      "619 [D loss: 0.289520, acc.: 90.05%] [G loss: 3.966002]\n",
      "620 [D loss: 0.229080, acc.: 92.27%] [G loss: 5.125587]\n",
      "621 [D loss: 0.373114, acc.: 82.15%] [G loss: 4.310692]\n",
      "622 [D loss: 0.183538, acc.: 95.60%] [G loss: 4.981059]\n",
      "623 [D loss: 0.212668, acc.: 93.77%] [G loss: 3.610604]\n",
      "624 [D loss: 0.161628, acc.: 98.63%] [G loss: 5.050231]\n",
      "625 [D loss: 0.336984, acc.: 87.67%] [G loss: 4.790789]\n",
      "626 [D loss: 0.174100, acc.: 95.12%] [G loss: 4.150044]\n",
      "627 [D loss: 0.162286, acc.: 97.70%] [G loss: 4.456221]\n",
      "628 [D loss: 0.202773, acc.: 95.55%] [G loss: 4.505214]\n",
      "629 [D loss: 0.247881, acc.: 93.75%] [G loss: 5.027147]\n",
      "630 [D loss: 0.259615, acc.: 92.88%] [G loss: 4.386307]\n",
      "631 [D loss: 0.148413, acc.: 97.28%] [G loss: 4.428010]\n",
      "632 [D loss: 0.217676, acc.: 95.40%] [G loss: 5.241582]\n",
      "633 [D loss: 0.382716, acc.: 81.50%] [G loss: 5.717392]\n",
      "634 [D loss: 0.300361, acc.: 89.57%] [G loss: 4.336966]\n",
      "635 [D loss: 0.093639, acc.: 98.72%] [G loss: 4.534908]\n",
      "636 [D loss: 0.137404, acc.: 97.60%] [G loss: 4.394596]\n",
      "637 [D loss: 0.169247, acc.: 96.78%] [G loss: 4.726022]\n",
      "638 [D loss: 0.211005, acc.: 94.90%] [G loss: 5.324880]\n",
      "639 [D loss: 0.314938, acc.: 89.25%] [G loss: 5.492087]\n",
      "640 [D loss: 0.183734, acc.: 93.25%] [G loss: 5.359076]\n",
      "641 [D loss: 0.217378, acc.: 91.85%] [G loss: 4.968530]\n",
      "642 [D loss: 0.174600, acc.: 95.60%] [G loss: 5.402970]\n",
      "643 [D loss: 0.223017, acc.: 94.48%] [G loss: 4.356893]\n",
      "644 [D loss: 0.107712, acc.: 98.48%] [G loss: 4.377830]\n",
      "645 [D loss: 0.189254, acc.: 97.37%] [G loss: 5.492138]\n",
      "646 [D loss: 0.429630, acc.: 81.35%] [G loss: 6.321124]\n",
      "647 [D loss: 0.224126, acc.: 91.08%] [G loss: 3.737437]\n",
      "648 [D loss: 0.148415, acc.: 97.25%] [G loss: 6.129507]\n",
      "649 [D loss: 0.214468, acc.: 93.12%] [G loss: 4.196010]\n",
      "650 [D loss: 0.154890, acc.: 98.30%] [G loss: 5.443516]\n",
      "651 [D loss: 0.260380, acc.: 92.43%] [G loss: 4.746096]\n",
      "652 [D loss: 0.185262, acc.: 95.57%] [G loss: 5.134577]\n",
      "653 [D loss: 0.203725, acc.: 93.23%] [G loss: 5.141027]\n",
      "654 [D loss: 0.212449, acc.: 94.10%] [G loss: 5.417287]\n",
      "655 [D loss: 0.182894, acc.: 93.95%] [G loss: 5.262259]\n",
      "656 [D loss: 0.187618, acc.: 95.45%] [G loss: 5.174308]\n",
      "657 [D loss: 0.184913, acc.: 94.85%] [G loss: 5.499359]\n",
      "658 [D loss: 0.344481, acc.: 84.47%] [G loss: 6.730372]\n",
      "659 [D loss: 0.560594, acc.: 70.80%] [G loss: 6.274601]\n",
      "660 [D loss: 0.152664, acc.: 94.08%] [G loss: 5.259253]\n",
      "661 [D loss: 0.153035, acc.: 95.88%] [G loss: 4.405497]\n",
      "662 [D loss: 0.111548, acc.: 97.53%] [G loss: 4.530228]\n",
      "663 [D loss: 0.207588, acc.: 95.50%] [G loss: 4.937931]\n",
      "664 [D loss: 0.376276, acc.: 85.40%] [G loss: 5.681502]\n",
      "665 [D loss: 0.288362, acc.: 89.80%] [G loss: 4.344787]\n",
      "666 [D loss: 0.113943, acc.: 97.37%] [G loss: 4.402755]\n",
      "667 [D loss: 0.173748, acc.: 95.95%] [G loss: 4.989456]\n",
      "668 [D loss: 0.341855, acc.: 86.92%] [G loss: 5.814377]\n",
      "669 [D loss: 0.426959, acc.: 78.75%] [G loss: 4.640763]\n",
      "670 [D loss: 0.147748, acc.: 96.03%] [G loss: 5.027215]\n",
      "671 [D loss: 0.187086, acc.: 94.98%] [G loss: 4.109830]\n",
      "672 [D loss: 0.174868, acc.: 96.77%] [G loss: 5.126297]\n",
      "673 [D loss: 0.373584, acc.: 82.68%] [G loss: 6.387678]\n",
      "674 [D loss: 0.492407, acc.: 74.58%] [G loss: 5.109858]\n",
      "675 [D loss: 0.133827, acc.: 96.72%] [G loss: 5.172466]\n",
      "676 [D loss: 0.154209, acc.: 96.63%] [G loss: 4.267856]\n",
      "677 [D loss: 0.148145, acc.: 95.55%] [G loss: 5.055423]\n",
      "678 [D loss: 0.284099, acc.: 91.80%] [G loss: 5.700717]\n",
      "679 [D loss: 0.390090, acc.: 80.20%] [G loss: 5.401370]\n",
      "680 [D loss: 0.238827, acc.: 91.18%] [G loss: 5.500810]\n",
      "681 [D loss: 0.167750, acc.: 94.88%] [G loss: 4.022394]\n",
      "682 [D loss: 0.163476, acc.: 96.83%] [G loss: 5.404771]\n",
      "683 [D loss: 0.313265, acc.: 88.15%] [G loss: 5.351413]\n",
      "684 [D loss: 0.258287, acc.: 87.58%] [G loss: 4.989449]\n",
      "685 [D loss: 0.196240, acc.: 94.52%] [G loss: 4.798840]\n",
      "686 [D loss: 0.205596, acc.: 95.42%] [G loss: 5.002590]\n",
      "687 [D loss: 0.240242, acc.: 93.42%] [G loss: 5.070470]\n",
      "688 [D loss: 0.269047, acc.: 91.15%] [G loss: 5.245147]\n",
      "689 [D loss: 0.238152, acc.: 91.12%] [G loss: 4.897384]\n",
      "690 [D loss: 0.182902, acc.: 96.60%] [G loss: 5.033484]\n",
      "691 [D loss: 0.158713, acc.: 96.63%] [G loss: 4.445114]\n",
      "692 [D loss: 0.148171, acc.: 97.62%] [G loss: 5.045980]\n",
      "693 [D loss: 0.244849, acc.: 95.20%] [G loss: 5.619271]\n",
      "694 [D loss: 0.233078, acc.: 92.73%] [G loss: 4.453999]\n",
      "695 [D loss: 0.101312, acc.: 98.40%] [G loss: 4.795496]\n",
      "696 [D loss: 0.152574, acc.: 97.48%] [G loss: 4.984498]\n",
      "697 [D loss: 0.201329, acc.: 94.82%] [G loss: 5.430506]\n",
      "698 [D loss: 0.280494, acc.: 88.05%] [G loss: 5.847811]\n",
      "699 [D loss: 0.307499, acc.: 85.12%] [G loss: 5.956713]\n",
      "700 [D loss: 0.201627, acc.: 92.45%] [G loss: 5.858556]\n",
      "701 [D loss: 0.124562, acc.: 96.65%] [G loss: 4.058002]\n",
      "702 [D loss: 0.125536, acc.: 99.25%] [G loss: 5.276540]\n",
      "703 [D loss: 0.237755, acc.: 92.55%] [G loss: 4.750153]\n",
      "704 [D loss: 0.168922, acc.: 96.78%] [G loss: 4.686176]\n",
      "705 [D loss: 0.203891, acc.: 95.70%] [G loss: 5.163130]\n",
      "706 [D loss: 0.231774, acc.: 93.27%] [G loss: 4.906462]\n",
      "707 [D loss: 0.136773, acc.: 97.25%] [G loss: 5.141837]\n",
      "708 [D loss: 0.184556, acc.: 96.42%] [G loss: 4.990816]\n",
      "709 [D loss: 0.164748, acc.: 96.35%] [G loss: 5.324572]\n",
      "710 [D loss: 0.327858, acc.: 86.22%] [G loss: 6.817840]\n",
      "711 [D loss: 0.445454, acc.: 77.58%] [G loss: 6.087497]\n",
      "712 [D loss: 0.166046, acc.: 95.07%] [G loss: 6.033634]\n",
      "713 [D loss: 0.128977, acc.: 96.95%] [G loss: 4.429495]\n",
      "714 [D loss: 0.113565, acc.: 97.40%] [G loss: 5.340818]\n",
      "715 [D loss: 0.205883, acc.: 93.90%] [G loss: 5.266991]\n",
      "716 [D loss: 0.362043, acc.: 83.78%] [G loss: 7.904100]\n",
      "717 [D loss: 1.061506, acc.: 53.52%] [G loss: 6.501074]\n",
      "718 [D loss: 0.313317, acc.: 86.40%] [G loss: 6.132270]\n",
      "719 [D loss: 0.092333, acc.: 96.85%] [G loss: 4.339721]\n",
      "720 [D loss: 0.111355, acc.: 97.65%] [G loss: 4.929261]\n",
      "721 [D loss: 0.110494, acc.: 97.50%] [G loss: 4.167679]\n",
      "722 [D loss: 0.203628, acc.: 95.75%] [G loss: 5.665272]\n",
      "723 [D loss: 0.480143, acc.: 77.25%] [G loss: 5.276200]\n",
      "724 [D loss: 0.142188, acc.: 95.27%] [G loss: 4.562201]\n",
      "725 [D loss: 0.154174, acc.: 97.48%] [G loss: 4.606192]\n",
      "726 [D loss: 0.154845, acc.: 96.70%] [G loss: 4.574768]\n",
      "727 [D loss: 0.246139, acc.: 91.50%] [G loss: 6.566711]\n",
      "728 [D loss: 0.767963, acc.: 63.28%] [G loss: 7.070889]\n",
      "729 [D loss: 0.537645, acc.: 75.35%] [G loss: 6.257792]\n",
      "730 [D loss: 0.080263, acc.: 97.45%] [G loss: 5.357000]\n",
      "731 [D loss: 0.138963, acc.: 96.72%] [G loss: 4.788620]\n",
      "732 [D loss: 0.073362, acc.: 98.93%] [G loss: 4.117764]\n",
      "733 [D loss: 0.168305, acc.: 96.00%] [G loss: 4.914574]\n",
      "734 [D loss: 0.258075, acc.: 91.37%] [G loss: 4.327413]\n",
      "735 [D loss: 0.168588, acc.: 95.88%] [G loss: 4.079501]\n",
      "736 [D loss: 0.169902, acc.: 96.83%] [G loss: 4.686471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737 [D loss: 0.230416, acc.: 94.88%] [G loss: 4.937872]\n",
      "738 [D loss: 0.228096, acc.: 92.15%] [G loss: 4.776790]\n",
      "739 [D loss: 0.174234, acc.: 94.85%] [G loss: 4.965078]\n",
      "740 [D loss: 0.178655, acc.: 96.03%] [G loss: 4.454633]\n",
      "741 [D loss: 0.153735, acc.: 97.05%] [G loss: 4.620976]\n",
      "742 [D loss: 0.194305, acc.: 95.88%] [G loss: 4.885291]\n",
      "743 [D loss: 0.199662, acc.: 95.00%] [G loss: 4.688268]\n",
      "744 [D loss: 0.168297, acc.: 95.53%] [G loss: 5.342450]\n",
      "745 [D loss: 0.316814, acc.: 84.65%] [G loss: 6.261930]\n",
      "746 [D loss: 0.386735, acc.: 80.10%] [G loss: 6.276486]\n",
      "747 [D loss: 0.296846, acc.: 84.60%] [G loss: 6.717633]\n",
      "748 [D loss: 0.201008, acc.: 93.12%] [G loss: 3.961366]\n",
      "749 [D loss: 0.162254, acc.: 94.80%] [G loss: 6.227624]\n",
      "750 [D loss: 0.353165, acc.: 82.57%] [G loss: 5.439435]\n",
      "751 [D loss: 0.176916, acc.: 93.75%] [G loss: 4.504494]\n",
      "752 [D loss: 0.121360, acc.: 98.15%] [G loss: 4.448655]\n",
      "753 [D loss: 0.140057, acc.: 97.40%] [G loss: 4.630520]\n",
      "754 [D loss: 0.208039, acc.: 96.13%] [G loss: 5.682271]\n",
      "755 [D loss: 0.498690, acc.: 72.80%] [G loss: 6.806803]\n",
      "756 [D loss: 0.402182, acc.: 81.88%] [G loss: 4.231623]\n",
      "757 [D loss: 0.077021, acc.: 99.37%] [G loss: 4.488078]\n",
      "758 [D loss: 0.094384, acc.: 98.43%] [G loss: 4.017936]\n",
      "759 [D loss: 0.143170, acc.: 98.00%] [G loss: 4.820050]\n",
      "760 [D loss: 0.216097, acc.: 94.57%] [G loss: 4.580910]\n",
      "761 [D loss: 0.198518, acc.: 95.15%] [G loss: 5.331938]\n",
      "762 [D loss: 0.351574, acc.: 81.30%] [G loss: 5.950184]\n",
      "763 [D loss: 0.361953, acc.: 81.35%] [G loss: 5.668508]\n",
      "764 [D loss: 0.132149, acc.: 96.68%] [G loss: 4.632838]\n",
      "765 [D loss: 0.112849, acc.: 98.85%] [G loss: 4.950357]\n",
      "766 [D loss: 0.148584, acc.: 96.13%] [G loss: 4.507641]\n",
      "767 [D loss: 0.180264, acc.: 96.60%] [G loss: 5.154868]\n",
      "768 [D loss: 0.310613, acc.: 90.97%] [G loss: 5.810692]\n",
      "769 [D loss: 0.249058, acc.: 91.40%] [G loss: 3.904781]\n",
      "770 [D loss: 0.117218, acc.: 98.77%] [G loss: 4.918246]\n",
      "771 [D loss: 0.209626, acc.: 95.10%] [G loss: 5.032451]\n",
      "772 [D loss: 0.269750, acc.: 91.97%] [G loss: 5.703573]\n",
      "773 [D loss: 0.354746, acc.: 80.77%] [G loss: 6.105417]\n",
      "774 [D loss: 0.396559, acc.: 80.15%] [G loss: 5.611677]\n",
      "775 [D loss: 0.153289, acc.: 95.03%] [G loss: 5.927598]\n",
      "776 [D loss: 0.134982, acc.: 96.03%] [G loss: 4.078600]\n",
      "777 [D loss: 0.131511, acc.: 97.62%] [G loss: 5.029772]\n",
      "778 [D loss: 0.200920, acc.: 94.75%] [G loss: 4.753188]\n",
      "779 [D loss: 0.246734, acc.: 91.15%] [G loss: 5.304318]\n",
      "780 [D loss: 0.266315, acc.: 92.40%] [G loss: 4.818039]\n",
      "781 [D loss: 0.148222, acc.: 96.15%] [G loss: 4.439377]\n",
      "782 [D loss: 0.138567, acc.: 97.35%] [G loss: 4.897716]\n",
      "783 [D loss: 0.167041, acc.: 96.05%] [G loss: 4.625802]\n",
      "784 [D loss: 0.155196, acc.: 96.85%] [G loss: 5.072493]\n",
      "785 [D loss: 0.255524, acc.: 91.55%] [G loss: 6.125959]\n",
      "786 [D loss: 0.400087, acc.: 82.15%] [G loss: 6.135096]\n",
      "787 [D loss: 0.252731, acc.: 89.05%] [G loss: 5.537655]\n",
      "788 [D loss: 0.110640, acc.: 97.57%] [G loss: 4.626460]\n",
      "789 [D loss: 0.111733, acc.: 98.68%] [G loss: 4.850301]\n",
      "790 [D loss: 0.170530, acc.: 95.20%] [G loss: 5.554875]\n",
      "791 [D loss: 0.326715, acc.: 86.83%] [G loss: 6.424765]\n",
      "792 [D loss: 0.341249, acc.: 81.55%] [G loss: 6.725939]\n",
      "793 [D loss: 0.239943, acc.: 92.45%] [G loss: 5.907365]\n",
      "794 [D loss: 0.066703, acc.: 98.35%] [G loss: 4.656296]\n",
      "795 [D loss: 0.110306, acc.: 98.03%] [G loss: 5.199803]\n",
      "796 [D loss: 0.205929, acc.: 93.67%] [G loss: 5.078740]\n",
      "797 [D loss: 0.228641, acc.: 93.45%] [G loss: 5.870422]\n",
      "798 [D loss: 0.391742, acc.: 83.65%] [G loss: 6.314351]\n",
      "799 [D loss: 0.237468, acc.: 90.05%] [G loss: 5.080778]\n",
      "800 [D loss: 0.095791, acc.: 98.52%] [G loss: 5.064513]\n",
      "801 [D loss: 0.158409, acc.: 95.62%] [G loss: 5.727757]\n",
      "802 [D loss: 0.280490, acc.: 87.52%] [G loss: 6.358745]\n",
      "803 [D loss: 0.476033, acc.: 78.40%] [G loss: 7.433510]\n",
      "804 [D loss: 0.806088, acc.: 67.47%] [G loss: 7.920447]\n",
      "805 [D loss: 0.201115, acc.: 91.35%] [G loss: 5.427072]\n",
      "806 [D loss: 0.175513, acc.: 90.17%] [G loss: 6.522460]\n",
      "807 [D loss: 0.103277, acc.: 96.05%] [G loss: 5.084839]\n",
      "808 [D loss: 0.128923, acc.: 97.12%] [G loss: 4.588951]\n",
      "809 [D loss: 0.081540, acc.: 98.68%] [G loss: 4.320649]\n",
      "810 [D loss: 0.145817, acc.: 97.60%] [G loss: 4.787083]\n",
      "811 [D loss: 0.189188, acc.: 95.27%] [G loss: 4.600142]\n",
      "812 [D loss: 0.174649, acc.: 96.10%] [G loss: 4.757482]\n",
      "813 [D loss: 0.192527, acc.: 95.55%] [G loss: 4.954090]\n",
      "814 [D loss: 0.208202, acc.: 95.15%] [G loss: 5.246891]\n",
      "815 [D loss: 0.253770, acc.: 92.50%] [G loss: 5.276631]\n",
      "816 [D loss: 0.162952, acc.: 95.18%] [G loss: 4.288621]\n",
      "817 [D loss: 0.144360, acc.: 97.60%] [G loss: 5.244851]\n",
      "818 [D loss: 0.326610, acc.: 87.10%] [G loss: 7.133967]\n",
      "819 [D loss: 0.739540, acc.: 65.47%] [G loss: 5.613391]\n",
      "820 [D loss: 0.126613, acc.: 95.25%] [G loss: 5.937220]\n",
      "821 [D loss: 0.157847, acc.: 95.10%] [G loss: 4.545466]\n",
      "822 [D loss: 0.068937, acc.: 99.27%] [G loss: 4.320364]\n",
      "823 [D loss: 0.108337, acc.: 98.77%] [G loss: 4.383045]\n",
      "824 [D loss: 0.201545, acc.: 92.50%] [G loss: 6.222596]\n",
      "825 [D loss: 0.826219, acc.: 53.28%] [G loss: 7.653519]\n",
      "826 [D loss: 0.471803, acc.: 84.55%] [G loss: 5.507714]\n",
      "827 [D loss: 0.102637, acc.: 96.97%] [G loss: 4.140050]\n",
      "828 [D loss: 0.072311, acc.: 99.55%] [G loss: 4.137359]\n",
      "829 [D loss: 0.076364, acc.: 98.93%] [G loss: 4.035243]\n",
      "830 [D loss: 0.129538, acc.: 97.90%] [G loss: 4.529413]\n",
      "831 [D loss: 0.213987, acc.: 94.92%] [G loss: 4.264912]\n",
      "832 [D loss: 0.154400, acc.: 96.18%] [G loss: 4.191643]\n",
      "833 [D loss: 0.162119, acc.: 96.85%] [G loss: 4.409535]\n",
      "834 [D loss: 0.170106, acc.: 95.92%] [G loss: 4.333821]\n",
      "835 [D loss: 0.156969, acc.: 96.90%] [G loss: 4.586074]\n",
      "836 [D loss: 0.195436, acc.: 95.53%] [G loss: 5.026294]\n",
      "837 [D loss: 0.214399, acc.: 94.30%] [G loss: 4.562016]\n",
      "838 [D loss: 0.113634, acc.: 97.68%] [G loss: 4.798086]\n",
      "839 [D loss: 0.153961, acc.: 96.68%] [G loss: 4.855126]\n",
      "840 [D loss: 0.144044, acc.: 96.65%] [G loss: 4.778426]\n",
      "841 [D loss: 0.160355, acc.: 96.72%] [G loss: 5.481875]\n",
      "842 [D loss: 0.227288, acc.: 93.75%] [G loss: 5.461173]\n",
      "843 [D loss: 0.159552, acc.: 94.95%] [G loss: 5.147123]\n",
      "844 [D loss: 0.139159, acc.: 97.30%] [G loss: 5.453973]\n",
      "845 [D loss: 0.177642, acc.: 95.10%] [G loss: 5.492884]\n",
      "846 [D loss: 0.166427, acc.: 96.05%] [G loss: 4.952800]\n",
      "847 [D loss: 0.107870, acc.: 97.75%] [G loss: 4.879293]\n",
      "848 [D loss: 0.190005, acc.: 95.45%] [G loss: 6.371666]\n",
      "849 [D loss: 0.347434, acc.: 87.20%] [G loss: 6.548514]\n",
      "850 [D loss: 0.176587, acc.: 93.95%] [G loss: 4.665026]\n",
      "851 [D loss: 0.074876, acc.: 99.13%] [G loss: 4.815693]\n",
      "852 [D loss: 0.101473, acc.: 97.62%] [G loss: 4.316054]\n",
      "853 [D loss: 0.157013, acc.: 97.45%] [G loss: 5.810450]\n",
      "854 [D loss: 0.347524, acc.: 84.75%] [G loss: 6.673355]\n",
      "855 [D loss: 0.324346, acc.: 88.73%] [G loss: 4.303093]\n",
      "856 [D loss: 0.078513, acc.: 98.77%] [G loss: 5.123930]\n",
      "857 [D loss: 0.098280, acc.: 97.45%] [G loss: 4.209652]\n",
      "858 [D loss: 0.143279, acc.: 98.12%] [G loss: 5.961642]\n",
      "859 [D loss: 0.355027, acc.: 84.53%] [G loss: 6.142440]\n",
      "860 [D loss: 0.312337, acc.: 85.90%] [G loss: 6.743052]\n",
      "861 [D loss: 0.213625, acc.: 91.80%] [G loss: 4.935113]\n",
      "862 [D loss: 0.127401, acc.: 97.57%] [G loss: 5.273758]\n",
      "863 [D loss: 0.145631, acc.: 96.13%] [G loss: 4.580066]\n",
      "864 [D loss: 0.188358, acc.: 93.55%] [G loss: 6.226812]\n",
      "865 [D loss: 0.386834, acc.: 83.90%] [G loss: 6.669101]\n",
      "866 [D loss: 0.196041, acc.: 92.62%] [G loss: 4.581486]\n",
      "867 [D loss: 0.110893, acc.: 98.48%] [G loss: 5.322373]\n",
      "868 [D loss: 0.134134, acc.: 96.47%] [G loss: 4.744809]\n",
      "869 [D loss: 0.231683, acc.: 93.15%] [G loss: 7.617575]\n",
      "870 [D loss: 1.265661, acc.: 40.02%] [G loss: 10.498146]\n",
      "871 [D loss: 0.872959, acc.: 72.18%] [G loss: 5.169230]\n",
      "872 [D loss: 0.137059, acc.: 95.77%] [G loss: 5.222034]\n",
      "873 [D loss: 0.039281, acc.: 99.40%] [G loss: 5.429162]\n",
      "874 [D loss: 0.076937, acc.: 98.33%] [G loss: 4.196536]\n",
      "875 [D loss: 0.083228, acc.: 98.93%] [G loss: 3.984723]\n",
      "876 [D loss: 0.081407, acc.: 98.87%] [G loss: 4.011371]\n",
      "877 [D loss: 0.105705, acc.: 98.20%] [G loss: 3.825703]\n",
      "878 [D loss: 0.118827, acc.: 97.80%] [G loss: 3.909585]\n",
      "879 [D loss: 0.135455, acc.: 96.85%] [G loss: 3.987694]\n",
      "880 [D loss: 0.122398, acc.: 96.88%] [G loss: 3.906152]\n",
      "881 [D loss: 0.148420, acc.: 96.45%] [G loss: 4.493849]\n",
      "882 [D loss: 0.177995, acc.: 95.25%] [G loss: 4.093988]\n",
      "883 [D loss: 0.144330, acc.: 96.60%] [G loss: 4.844254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884 [D loss: 0.217006, acc.: 92.77%] [G loss: 4.590529]\n",
      "885 [D loss: 0.186317, acc.: 92.15%] [G loss: 6.111562]\n",
      "886 [D loss: 0.442271, acc.: 75.40%] [G loss: 6.045447]\n",
      "887 [D loss: 0.185694, acc.: 91.18%] [G loss: 6.877862]\n",
      "888 [D loss: 0.132312, acc.: 94.92%] [G loss: 4.295366]\n",
      "889 [D loss: 0.123280, acc.: 97.10%] [G loss: 5.142423]\n",
      "890 [D loss: 0.089359, acc.: 97.12%] [G loss: 4.303138]\n",
      "891 [D loss: 0.151434, acc.: 95.65%] [G loss: 5.727878]\n",
      "892 [D loss: 0.255606, acc.: 91.40%] [G loss: 4.619656]\n",
      "893 [D loss: 0.094121, acc.: 98.05%] [G loss: 5.146233]\n",
      "894 [D loss: 0.105968, acc.: 97.30%] [G loss: 4.184152]\n",
      "895 [D loss: 0.124171, acc.: 98.25%] [G loss: 5.355712]\n",
      "896 [D loss: 0.177662, acc.: 95.23%] [G loss: 4.520927]\n",
      "897 [D loss: 0.104574, acc.: 98.10%] [G loss: 4.694378]\n",
      "898 [D loss: 0.139025, acc.: 96.95%] [G loss: 5.466395]\n",
      "899 [D loss: 0.320407, acc.: 89.15%] [G loss: 6.942307]\n",
      "900 [D loss: 0.224013, acc.: 91.75%] [G loss: 3.783622]\n",
      "901 [D loss: 0.098474, acc.: 98.98%] [G loss: 5.124971]\n",
      "902 [D loss: 0.072971, acc.: 97.95%] [G loss: 4.789332]\n",
      "903 [D loss: 0.136944, acc.: 97.40%] [G loss: 4.643397]\n",
      "904 [D loss: 0.114367, acc.: 97.12%] [G loss: 4.774554]\n",
      "905 [D loss: 0.215147, acc.: 94.10%] [G loss: 5.940157]\n",
      "906 [D loss: 0.292873, acc.: 87.75%] [G loss: 5.448397]\n",
      "907 [D loss: 0.154686, acc.: 94.82%] [G loss: 5.565604]\n",
      "908 [D loss: 0.111413, acc.: 96.07%] [G loss: 3.834393]\n",
      "909 [D loss: 0.116974, acc.: 98.37%] [G loss: 5.556649]\n",
      "910 [D loss: 0.133864, acc.: 95.90%] [G loss: 4.116725]\n",
      "911 [D loss: 0.129470, acc.: 97.75%] [G loss: 5.337995]\n",
      "912 [D loss: 0.174212, acc.: 94.35%] [G loss: 4.381675]\n",
      "913 [D loss: 0.115942, acc.: 98.07%] [G loss: 5.225553]\n",
      "914 [D loss: 0.158751, acc.: 95.68%] [G loss: 4.731072]\n",
      "915 [D loss: 0.136580, acc.: 96.25%] [G loss: 5.228958]\n",
      "916 [D loss: 0.141360, acc.: 96.65%] [G loss: 4.710474]\n",
      "917 [D loss: 0.133532, acc.: 97.00%] [G loss: 5.210094]\n",
      "918 [D loss: 0.121993, acc.: 96.65%] [G loss: 4.413171]\n",
      "919 [D loss: 0.125662, acc.: 97.80%] [G loss: 5.537403]\n",
      "920 [D loss: 0.271487, acc.: 92.17%] [G loss: 6.445596]\n",
      "921 [D loss: 0.213644, acc.: 92.65%] [G loss: 4.629226]\n",
      "922 [D loss: 0.099825, acc.: 98.35%] [G loss: 5.012494]\n",
      "923 [D loss: 0.110008, acc.: 97.18%] [G loss: 4.447629]\n",
      "924 [D loss: 0.168907, acc.: 96.40%] [G loss: 5.891072]\n",
      "925 [D loss: 0.215771, acc.: 92.93%] [G loss: 4.506036]\n",
      "926 [D loss: 0.106629, acc.: 97.62%] [G loss: 4.787013]\n",
      "927 [D loss: 0.151961, acc.: 96.20%] [G loss: 5.184128]\n",
      "928 [D loss: 0.157736, acc.: 95.33%] [G loss: 4.802743]\n",
      "929 [D loss: 0.132850, acc.: 96.77%] [G loss: 5.161170]\n",
      "930 [D loss: 0.201445, acc.: 94.70%] [G loss: 5.781611]\n",
      "931 [D loss: 0.177649, acc.: 94.40%] [G loss: 4.713704]\n",
      "932 [D loss: 0.083521, acc.: 98.30%] [G loss: 4.802545]\n",
      "933 [D loss: 0.113360, acc.: 97.60%] [G loss: 5.296257]\n",
      "934 [D loss: 0.230147, acc.: 93.17%] [G loss: 7.002869]\n",
      "935 [D loss: 0.470276, acc.: 79.80%] [G loss: 7.434600]\n",
      "936 [D loss: 0.183419, acc.: 92.23%] [G loss: 4.817173]\n",
      "937 [D loss: 0.099597, acc.: 98.68%] [G loss: 5.582653]\n",
      "938 [D loss: 0.072718, acc.: 97.65%] [G loss: 4.958028]\n",
      "939 [D loss: 0.120190, acc.: 96.92%] [G loss: 4.964053]\n",
      "940 [D loss: 0.111180, acc.: 96.95%] [G loss: 4.538078]\n",
      "941 [D loss: 0.135119, acc.: 97.35%] [G loss: 5.375862]\n",
      "942 [D loss: 0.239928, acc.: 91.87%] [G loss: 5.552443]\n",
      "943 [D loss: 0.174416, acc.: 94.65%] [G loss: 4.464001]\n",
      "944 [D loss: 0.106319, acc.: 97.98%] [G loss: 4.958621]\n",
      "945 [D loss: 0.120134, acc.: 97.30%] [G loss: 4.651463]\n",
      "946 [D loss: 0.145476, acc.: 96.88%] [G loss: 5.654350]\n",
      "947 [D loss: 0.254977, acc.: 92.15%] [G loss: 6.026266]\n",
      "948 [D loss: 0.192262, acc.: 93.83%] [G loss: 4.666977]\n",
      "949 [D loss: 0.109796, acc.: 97.83%] [G loss: 5.441307]\n",
      "950 [D loss: 0.152897, acc.: 95.20%] [G loss: 4.735499]\n",
      "951 [D loss: 0.163810, acc.: 96.45%] [G loss: 5.594526]\n",
      "952 [D loss: 0.246641, acc.: 90.78%] [G loss: 5.658884]\n",
      "953 [D loss: 0.211912, acc.: 93.58%] [G loss: 5.164373]\n",
      "954 [D loss: 0.137586, acc.: 95.95%] [G loss: 4.716803]\n",
      "955 [D loss: 0.140099, acc.: 96.68%] [G loss: 5.254977]\n",
      "956 [D loss: 0.177748, acc.: 95.42%] [G loss: 5.219351]\n",
      "957 [D loss: 0.200883, acc.: 94.77%] [G loss: 5.665611]\n",
      "958 [D loss: 0.228859, acc.: 92.53%] [G loss: 5.511312]\n",
      "959 [D loss: 0.219478, acc.: 91.32%] [G loss: 5.666411]\n",
      "960 [D loss: 0.266867, acc.: 87.70%] [G loss: 6.403484]\n",
      "961 [D loss: 0.357336, acc.: 83.00%] [G loss: 6.021075]\n",
      "962 [D loss: 0.150699, acc.: 94.25%] [G loss: 5.231244]\n",
      "963 [D loss: 0.133244, acc.: 96.45%] [G loss: 4.587475]\n",
      "964 [D loss: 0.122657, acc.: 97.50%] [G loss: 4.796544]\n",
      "965 [D loss: 0.145129, acc.: 96.35%] [G loss: 5.499410]\n",
      "966 [D loss: 0.323633, acc.: 87.65%] [G loss: 6.946131]\n",
      "967 [D loss: 0.380632, acc.: 85.98%] [G loss: 5.176344]\n",
      "968 [D loss: 0.123036, acc.: 96.42%] [G loss: 5.075352]\n",
      "969 [D loss: 0.121259, acc.: 97.13%] [G loss: 4.641191]\n",
      "970 [D loss: 0.129067, acc.: 96.35%] [G loss: 5.044262]\n",
      "971 [D loss: 0.211157, acc.: 94.18%] [G loss: 5.579638]\n",
      "972 [D loss: 0.326649, acc.: 87.37%] [G loss: 6.732176]\n",
      "973 [D loss: 0.391621, acc.: 86.90%] [G loss: 4.844604]\n",
      "974 [D loss: 0.102856, acc.: 96.85%] [G loss: 4.441730]\n",
      "975 [D loss: 0.161900, acc.: 96.18%] [G loss: 5.336653]\n",
      "976 [D loss: 0.244033, acc.: 92.08%] [G loss: 4.830146]\n",
      "977 [D loss: 0.217811, acc.: 92.75%] [G loss: 5.487570]\n",
      "978 [D loss: 0.311534, acc.: 87.58%] [G loss: 5.394187]\n",
      "979 [D loss: 0.220757, acc.: 92.05%] [G loss: 4.394349]\n",
      "980 [D loss: 0.184162, acc.: 94.00%] [G loss: 5.191569]\n",
      "981 [D loss: 0.207884, acc.: 93.23%] [G loss: 4.286355]\n",
      "982 [D loss: 0.152839, acc.: 96.35%] [G loss: 4.883632]\n",
      "983 [D loss: 0.177097, acc.: 94.90%] [G loss: 4.361183]\n",
      "984 [D loss: 0.173230, acc.: 96.00%] [G loss: 5.027161]\n",
      "985 [D loss: 0.239974, acc.: 92.80%] [G loss: 4.862856]\n",
      "986 [D loss: 0.187698, acc.: 94.17%] [G loss: 4.506999]\n",
      "987 [D loss: 0.158327, acc.: 96.05%] [G loss: 4.844554]\n",
      "988 [D loss: 0.196347, acc.: 94.03%] [G loss: 5.176655]\n",
      "989 [D loss: 0.240496, acc.: 91.93%] [G loss: 5.405510]\n",
      "990 [D loss: 0.245631, acc.: 90.35%] [G loss: 4.963601]\n",
      "991 [D loss: 0.157179, acc.: 95.55%] [G loss: 4.835056]\n",
      "992 [D loss: 0.170735, acc.: 95.90%] [G loss: 4.881081]\n",
      "993 [D loss: 0.188076, acc.: 95.35%] [G loss: 5.203569]\n",
      "994 [D loss: 0.220190, acc.: 94.05%] [G loss: 5.016053]\n",
      "995 [D loss: 0.195250, acc.: 94.23%] [G loss: 5.055946]\n",
      "996 [D loss: 0.218857, acc.: 93.73%] [G loss: 5.051271]\n",
      "997 [D loss: 0.181485, acc.: 94.68%] [G loss: 4.769322]\n",
      "998 [D loss: 0.175223, acc.: 95.48%] [G loss: 5.211567]\n",
      "999 [D loss: 0.231407, acc.: 92.65%] [G loss: 5.791208]\n",
      "1000 [D loss: 0.241737, acc.: 91.67%] [G loss: 4.864873]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=1000, batch_size=2000, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468.75"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_ML",
   "language": "python",
   "name": "py_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
