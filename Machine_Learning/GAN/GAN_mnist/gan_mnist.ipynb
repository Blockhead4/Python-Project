{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import random_normal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 32\n",
    "        self.img_cols = 32\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, kernel_initializer=random_normal(), input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(1024, kernel_initializer=random_normal()))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones(batch_size)\n",
    "        fake = np.zeros(batch_size)\n",
    "\n",
    "        for epoch in range(epochs+1):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1024)              3146752   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,803,137\n",
      "Trainable params: 3,803,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 3072)              3148800   \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 3,838,720\n",
      "Trainable params: 3,835,136\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jwp\\Anaconda3\\envs\\py_ml\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.916724, acc.: 14.06%] [G loss: 0.203265]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jwp\\Anaconda3\\envs\\py_ml\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 1.407026, acc.: 49.22%] [G loss: 0.310134]\n",
      "2 [D loss: 0.767518, acc.: 52.34%] [G loss: 0.976858]\n",
      "3 [D loss: 0.536814, acc.: 71.48%] [G loss: 1.482298]\n",
      "4 [D loss: 0.608999, acc.: 68.36%] [G loss: 1.425764]\n",
      "5 [D loss: 0.603254, acc.: 67.58%] [G loss: 1.261951]\n",
      "6 [D loss: 0.608104, acc.: 62.89%] [G loss: 1.346227]\n",
      "7 [D loss: 0.645388, acc.: 64.45%] [G loss: 1.454173]\n",
      "8 [D loss: 0.503899, acc.: 79.30%] [G loss: 1.730278]\n",
      "9 [D loss: 0.600368, acc.: 76.95%] [G loss: 1.653968]\n",
      "10 [D loss: 0.494538, acc.: 77.73%] [G loss: 1.638447]\n",
      "11 [D loss: 0.545364, acc.: 73.83%] [G loss: 1.862750]\n",
      "12 [D loss: 0.435966, acc.: 84.77%] [G loss: 1.904807]\n",
      "13 [D loss: 0.492829, acc.: 81.25%] [G loss: 1.897865]\n",
      "14 [D loss: 0.406598, acc.: 88.67%] [G loss: 1.999434]\n",
      "15 [D loss: 0.416430, acc.: 85.55%] [G loss: 2.215102]\n",
      "16 [D loss: 0.517646, acc.: 80.86%] [G loss: 2.023910]\n",
      "17 [D loss: 0.415574, acc.: 84.77%] [G loss: 2.111092]\n",
      "18 [D loss: 0.413358, acc.: 86.72%] [G loss: 2.388320]\n",
      "19 [D loss: 0.482367, acc.: 80.47%] [G loss: 2.485776]\n",
      "20 [D loss: 0.509645, acc.: 81.64%] [G loss: 2.336515]\n",
      "21 [D loss: 0.506777, acc.: 80.08%] [G loss: 2.552083]\n",
      "22 [D loss: 0.521817, acc.: 78.91%] [G loss: 2.814317]\n",
      "23 [D loss: 0.514053, acc.: 82.03%] [G loss: 2.231832]\n",
      "24 [D loss: 0.545400, acc.: 77.34%] [G loss: 2.525030]\n",
      "25 [D loss: 0.669205, acc.: 72.27%] [G loss: 2.728505]\n",
      "26 [D loss: 0.492264, acc.: 79.30%] [G loss: 2.862757]\n",
      "27 [D loss: 0.722528, acc.: 69.53%] [G loss: 2.575027]\n",
      "28 [D loss: 0.616138, acc.: 74.22%] [G loss: 2.761428]\n",
      "29 [D loss: 0.717114, acc.: 66.02%] [G loss: 3.194765]\n",
      "30 [D loss: 0.814646, acc.: 63.67%] [G loss: 2.973123]\n",
      "31 [D loss: 0.644586, acc.: 73.83%] [G loss: 2.796383]\n",
      "32 [D loss: 0.800008, acc.: 60.55%] [G loss: 2.893349]\n",
      "33 [D loss: 0.740571, acc.: 66.80%] [G loss: 2.882873]\n",
      "34 [D loss: 0.756457, acc.: 65.23%] [G loss: 2.709755]\n",
      "35 [D loss: 0.900628, acc.: 55.08%] [G loss: 2.886025]\n",
      "36 [D loss: 0.925352, acc.: 53.12%] [G loss: 2.778746]\n",
      "37 [D loss: 0.928941, acc.: 57.42%] [G loss: 2.444707]\n",
      "38 [D loss: 0.863277, acc.: 62.11%] [G loss: 2.252902]\n",
      "39 [D loss: 0.821736, acc.: 58.59%] [G loss: 3.260917]\n",
      "40 [D loss: 0.923471, acc.: 59.77%] [G loss: 2.645401]\n",
      "41 [D loss: 0.839073, acc.: 53.91%] [G loss: 2.726894]\n",
      "42 [D loss: 0.781316, acc.: 63.28%] [G loss: 2.533600]\n",
      "43 [D loss: 0.928290, acc.: 55.08%] [G loss: 2.627143]\n",
      "44 [D loss: 0.986501, acc.: 55.47%] [G loss: 2.586398]\n",
      "45 [D loss: 0.953959, acc.: 52.73%] [G loss: 2.252631]\n",
      "46 [D loss: 1.098218, acc.: 46.09%] [G loss: 3.118715]\n",
      "47 [D loss: 0.862889, acc.: 55.86%] [G loss: 2.889012]\n",
      "48 [D loss: 0.980337, acc.: 54.30%] [G loss: 2.008954]\n",
      "49 [D loss: 0.855469, acc.: 59.38%] [G loss: 2.611204]\n",
      "50 [D loss: 0.832278, acc.: 66.80%] [G loss: 2.518958]\n",
      "51 [D loss: 0.924329, acc.: 51.56%] [G loss: 2.124312]\n",
      "52 [D loss: 0.848980, acc.: 56.25%] [G loss: 2.390399]\n",
      "53 [D loss: 0.836470, acc.: 54.30%] [G loss: 2.204418]\n",
      "54 [D loss: 0.842599, acc.: 53.91%] [G loss: 2.303568]\n",
      "55 [D loss: 0.906407, acc.: 48.83%] [G loss: 1.858481]\n",
      "56 [D loss: 0.779078, acc.: 54.30%] [G loss: 2.263795]\n",
      "57 [D loss: 0.831796, acc.: 59.38%] [G loss: 2.302387]\n",
      "58 [D loss: 0.774291, acc.: 64.45%] [G loss: 2.089492]\n",
      "59 [D loss: 0.856557, acc.: 50.78%] [G loss: 2.097888]\n",
      "60 [D loss: 0.810169, acc.: 59.38%] [G loss: 1.823608]\n",
      "61 [D loss: 0.715320, acc.: 57.42%] [G loss: 2.245353]\n",
      "62 [D loss: 0.814968, acc.: 55.08%] [G loss: 1.907167]\n",
      "63 [D loss: 0.822107, acc.: 50.00%] [G loss: 1.789818]\n",
      "64 [D loss: 0.824905, acc.: 53.91%] [G loss: 2.170835]\n",
      "65 [D loss: 0.747177, acc.: 57.42%] [G loss: 2.055511]\n",
      "66 [D loss: 0.848123, acc.: 51.17%] [G loss: 1.880814]\n",
      "67 [D loss: 0.760815, acc.: 55.47%] [G loss: 2.105693]\n",
      "68 [D loss: 0.740380, acc.: 59.38%] [G loss: 2.125810]\n",
      "69 [D loss: 0.765250, acc.: 50.78%] [G loss: 1.774329]\n",
      "70 [D loss: 0.733716, acc.: 57.03%] [G loss: 1.963879]\n",
      "71 [D loss: 0.734999, acc.: 56.25%] [G loss: 2.149595]\n",
      "72 [D loss: 0.849827, acc.: 50.00%] [G loss: 1.710938]\n",
      "73 [D loss: 0.679038, acc.: 63.28%] [G loss: 1.906510]\n",
      "74 [D loss: 0.719214, acc.: 58.59%] [G loss: 1.903576]\n",
      "75 [D loss: 0.725219, acc.: 60.16%] [G loss: 1.939592]\n",
      "76 [D loss: 0.773438, acc.: 51.95%] [G loss: 1.783771]\n",
      "77 [D loss: 0.785383, acc.: 53.12%] [G loss: 1.898816]\n",
      "78 [D loss: 0.728060, acc.: 59.77%] [G loss: 1.778558]\n",
      "79 [D loss: 0.801062, acc.: 53.12%] [G loss: 1.768781]\n",
      "80 [D loss: 0.707256, acc.: 59.77%] [G loss: 1.873725]\n",
      "81 [D loss: 0.686529, acc.: 61.72%] [G loss: 1.868967]\n",
      "82 [D loss: 0.696723, acc.: 58.59%] [G loss: 2.062496]\n",
      "83 [D loss: 0.726504, acc.: 61.33%] [G loss: 1.740913]\n",
      "84 [D loss: 0.780482, acc.: 51.56%] [G loss: 1.625145]\n",
      "85 [D loss: 0.679094, acc.: 57.81%] [G loss: 1.910788]\n",
      "86 [D loss: 0.680639, acc.: 68.36%] [G loss: 1.775210]\n",
      "87 [D loss: 0.731753, acc.: 55.86%] [G loss: 1.625950]\n",
      "88 [D loss: 0.674606, acc.: 60.94%] [G loss: 1.764486]\n",
      "89 [D loss: 0.673212, acc.: 63.28%] [G loss: 1.870506]\n",
      "90 [D loss: 0.765821, acc.: 50.78%] [G loss: 1.722564]\n",
      "91 [D loss: 0.679930, acc.: 60.16%] [G loss: 1.768941]\n",
      "92 [D loss: 0.677149, acc.: 63.28%] [G loss: 1.823994]\n",
      "93 [D loss: 0.729405, acc.: 55.47%] [G loss: 1.650956]\n",
      "94 [D loss: 0.647948, acc.: 59.77%] [G loss: 1.482709]\n",
      "95 [D loss: 0.680618, acc.: 60.55%] [G loss: 1.589107]\n",
      "96 [D loss: 0.771197, acc.: 58.98%] [G loss: 1.500670]\n",
      "97 [D loss: 0.714788, acc.: 55.86%] [G loss: 1.582578]\n",
      "98 [D loss: 0.754599, acc.: 55.47%] [G loss: 1.513731]\n",
      "99 [D loss: 0.665341, acc.: 61.33%] [G loss: 1.555045]\n",
      "100 [D loss: 0.741806, acc.: 52.73%] [G loss: 1.578477]\n",
      "101 [D loss: 0.686620, acc.: 64.06%] [G loss: 1.690324]\n",
      "102 [D loss: 0.713998, acc.: 60.16%] [G loss: 1.507371]\n",
      "103 [D loss: 0.682428, acc.: 59.77%] [G loss: 1.578986]\n",
      "104 [D loss: 0.708595, acc.: 58.59%] [G loss: 1.625297]\n",
      "105 [D loss: 0.820566, acc.: 51.95%] [G loss: 1.610780]\n",
      "106 [D loss: 0.807144, acc.: 52.34%] [G loss: 1.609766]\n",
      "107 [D loss: 0.754588, acc.: 53.12%] [G loss: 1.488914]\n",
      "108 [D loss: 0.772418, acc.: 50.78%] [G loss: 1.512009]\n",
      "109 [D loss: 0.773319, acc.: 51.95%] [G loss: 1.509132]\n",
      "110 [D loss: 0.791195, acc.: 46.48%] [G loss: 1.493369]\n",
      "111 [D loss: 0.723694, acc.: 58.20%] [G loss: 1.453478]\n",
      "112 [D loss: 0.763701, acc.: 53.12%] [G loss: 1.511634]\n",
      "113 [D loss: 0.740645, acc.: 56.64%] [G loss: 1.550484]\n",
      "114 [D loss: 0.739782, acc.: 58.20%] [G loss: 1.495304]\n",
      "115 [D loss: 0.696745, acc.: 60.94%] [G loss: 1.499027]\n",
      "116 [D loss: 0.773865, acc.: 51.95%] [G loss: 1.453222]\n",
      "117 [D loss: 0.657987, acc.: 62.89%] [G loss: 1.419990]\n",
      "118 [D loss: 0.752857, acc.: 53.12%] [G loss: 1.387084]\n",
      "119 [D loss: 0.707847, acc.: 56.25%] [G loss: 1.412475]\n",
      "120 [D loss: 0.662616, acc.: 63.28%] [G loss: 1.394660]\n",
      "121 [D loss: 0.755363, acc.: 50.78%] [G loss: 1.465069]\n",
      "122 [D loss: 0.754170, acc.: 55.47%] [G loss: 1.522738]\n",
      "123 [D loss: 0.811851, acc.: 44.14%] [G loss: 1.480575]\n",
      "124 [D loss: 0.761317, acc.: 51.95%] [G loss: 1.475991]\n",
      "125 [D loss: 0.797660, acc.: 47.66%] [G loss: 1.296177]\n",
      "126 [D loss: 0.728503, acc.: 54.69%] [G loss: 1.422386]\n",
      "127 [D loss: 0.717473, acc.: 55.47%] [G loss: 1.558692]\n",
      "128 [D loss: 0.725416, acc.: 58.20%] [G loss: 1.360914]\n",
      "129 [D loss: 0.729507, acc.: 55.47%] [G loss: 1.322346]\n",
      "130 [D loss: 0.729311, acc.: 57.03%] [G loss: 1.463117]\n",
      "131 [D loss: 0.784566, acc.: 52.34%] [G loss: 1.414451]\n",
      "132 [D loss: 0.736943, acc.: 56.64%] [G loss: 1.451339]\n",
      "133 [D loss: 0.758972, acc.: 53.91%] [G loss: 1.440805]\n",
      "134 [D loss: 0.753949, acc.: 50.39%] [G loss: 1.412451]\n",
      "135 [D loss: 0.732311, acc.: 51.56%] [G loss: 1.365504]\n",
      "136 [D loss: 0.747108, acc.: 50.00%] [G loss: 1.409981]\n",
      "137 [D loss: 0.729864, acc.: 54.30%] [G loss: 1.445498]\n",
      "138 [D loss: 0.767806, acc.: 50.00%] [G loss: 1.393186]\n",
      "139 [D loss: 0.744830, acc.: 49.22%] [G loss: 1.368243]\n",
      "140 [D loss: 0.765243, acc.: 46.09%] [G loss: 1.309402]\n",
      "141 [D loss: 0.725923, acc.: 52.34%] [G loss: 1.336909]\n",
      "142 [D loss: 0.766240, acc.: 50.39%] [G loss: 1.385327]\n",
      "143 [D loss: 0.727142, acc.: 53.12%] [G loss: 1.377737]\n",
      "144 [D loss: 0.726523, acc.: 49.22%] [G loss: 1.357167]\n",
      "145 [D loss: 0.712583, acc.: 58.20%] [G loss: 1.438969]\n",
      "146 [D loss: 0.713852, acc.: 55.08%] [G loss: 1.379790]\n",
      "147 [D loss: 0.768805, acc.: 48.44%] [G loss: 1.382190]\n",
      "148 [D loss: 0.812549, acc.: 42.97%] [G loss: 1.310175]\n",
      "149 [D loss: 0.708524, acc.: 51.56%] [G loss: 1.352833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 0.742470, acc.: 52.34%] [G loss: 1.304355]\n",
      "151 [D loss: 0.756858, acc.: 46.88%] [G loss: 1.334283]\n",
      "152 [D loss: 0.765073, acc.: 50.39%] [G loss: 1.285250]\n",
      "153 [D loss: 0.689976, acc.: 56.64%] [G loss: 1.382980]\n",
      "154 [D loss: 0.699459, acc.: 57.81%] [G loss: 1.334750]\n",
      "155 [D loss: 0.708367, acc.: 58.20%] [G loss: 1.329101]\n",
      "156 [D loss: 0.675999, acc.: 59.77%] [G loss: 1.328014]\n",
      "157 [D loss: 0.686112, acc.: 60.94%] [G loss: 1.293118]\n",
      "158 [D loss: 0.745829, acc.: 58.98%] [G loss: 1.322809]\n",
      "159 [D loss: 0.696624, acc.: 52.34%] [G loss: 1.402045]\n",
      "160 [D loss: 0.722841, acc.: 57.03%] [G loss: 1.324733]\n",
      "161 [D loss: 0.709889, acc.: 56.64%] [G loss: 1.343920]\n",
      "162 [D loss: 0.717297, acc.: 55.86%] [G loss: 1.420368]\n",
      "163 [D loss: 0.718440, acc.: 59.38%] [G loss: 1.282129]\n",
      "164 [D loss: 0.696276, acc.: 57.03%] [G loss: 1.366764]\n",
      "165 [D loss: 0.689060, acc.: 58.98%] [G loss: 1.279398]\n",
      "166 [D loss: 0.670574, acc.: 58.98%] [G loss: 1.339247]\n",
      "167 [D loss: 0.665053, acc.: 62.50%] [G loss: 1.446297]\n",
      "168 [D loss: 0.702791, acc.: 52.73%] [G loss: 1.442724]\n",
      "169 [D loss: 0.697051, acc.: 56.25%] [G loss: 1.317040]\n",
      "170 [D loss: 0.697729, acc.: 57.42%] [G loss: 1.349969]\n",
      "171 [D loss: 0.705495, acc.: 56.25%] [G loss: 1.419781]\n",
      "172 [D loss: 0.672598, acc.: 58.98%] [G loss: 1.357879]\n",
      "173 [D loss: 0.719484, acc.: 54.30%] [G loss: 1.290632]\n",
      "174 [D loss: 0.683635, acc.: 58.98%] [G loss: 1.334504]\n",
      "175 [D loss: 0.730871, acc.: 55.47%] [G loss: 1.323486]\n",
      "176 [D loss: 0.722248, acc.: 57.03%] [G loss: 1.382730]\n",
      "177 [D loss: 0.663481, acc.: 62.11%] [G loss: 1.358099]\n",
      "178 [D loss: 0.748830, acc.: 51.95%] [G loss: 1.331450]\n",
      "179 [D loss: 0.695373, acc.: 57.42%] [G loss: 1.336007]\n",
      "180 [D loss: 0.689847, acc.: 57.42%] [G loss: 1.408704]\n",
      "181 [D loss: 0.722740, acc.: 57.42%] [G loss: 1.301223]\n",
      "182 [D loss: 0.716392, acc.: 56.25%] [G loss: 1.211195]\n",
      "183 [D loss: 0.693909, acc.: 60.94%] [G loss: 1.309901]\n",
      "184 [D loss: 0.662136, acc.: 60.94%] [G loss: 1.323443]\n",
      "185 [D loss: 0.714464, acc.: 53.12%] [G loss: 1.363707]\n",
      "186 [D loss: 0.712449, acc.: 55.47%] [G loss: 1.427676]\n",
      "187 [D loss: 0.680082, acc.: 62.89%] [G loss: 1.350313]\n",
      "188 [D loss: 0.729809, acc.: 53.12%] [G loss: 1.336334]\n",
      "189 [D loss: 0.691811, acc.: 57.03%] [G loss: 1.349332]\n",
      "190 [D loss: 0.680637, acc.: 54.30%] [G loss: 1.320939]\n",
      "191 [D loss: 0.675889, acc.: 59.77%] [G loss: 1.400911]\n",
      "192 [D loss: 0.704525, acc.: 57.42%] [G loss: 1.304675]\n",
      "193 [D loss: 0.662717, acc.: 61.33%] [G loss: 1.345628]\n",
      "194 [D loss: 0.678379, acc.: 61.72%] [G loss: 1.356687]\n",
      "195 [D loss: 0.668748, acc.: 60.16%] [G loss: 1.414551]\n",
      "196 [D loss: 0.688118, acc.: 59.77%] [G loss: 1.356194]\n",
      "197 [D loss: 0.648929, acc.: 63.67%] [G loss: 1.336823]\n",
      "198 [D loss: 0.702390, acc.: 55.08%] [G loss: 1.273424]\n",
      "199 [D loss: 0.650443, acc.: 60.55%] [G loss: 1.333516]\n",
      "200 [D loss: 0.680779, acc.: 59.77%] [G loss: 1.311828]\n",
      "201 [D loss: 0.677648, acc.: 55.08%] [G loss: 1.370287]\n",
      "202 [D loss: 0.669319, acc.: 60.16%] [G loss: 1.308646]\n",
      "203 [D loss: 0.646005, acc.: 62.11%] [G loss: 1.382011]\n",
      "204 [D loss: 0.615974, acc.: 69.92%] [G loss: 1.334534]\n",
      "205 [D loss: 0.666149, acc.: 59.77%] [G loss: 1.319972]\n",
      "206 [D loss: 0.618725, acc.: 66.02%] [G loss: 1.340588]\n",
      "207 [D loss: 0.643931, acc.: 61.33%] [G loss: 1.282308]\n",
      "208 [D loss: 0.622632, acc.: 60.94%] [G loss: 1.309212]\n",
      "209 [D loss: 0.633577, acc.: 63.28%] [G loss: 1.311360]\n",
      "210 [D loss: 0.653328, acc.: 65.23%] [G loss: 1.272590]\n",
      "211 [D loss: 0.639388, acc.: 62.11%] [G loss: 1.320228]\n",
      "212 [D loss: 0.680408, acc.: 60.94%] [G loss: 1.323827]\n",
      "213 [D loss: 0.701217, acc.: 53.52%] [G loss: 1.310505]\n",
      "214 [D loss: 0.631682, acc.: 62.11%] [G loss: 1.381816]\n",
      "215 [D loss: 0.655201, acc.: 62.11%] [G loss: 1.343099]\n",
      "216 [D loss: 0.640817, acc.: 62.89%] [G loss: 1.345332]\n",
      "217 [D loss: 0.647720, acc.: 59.38%] [G loss: 1.345228]\n",
      "218 [D loss: 0.668069, acc.: 63.28%] [G loss: 1.284839]\n",
      "219 [D loss: 0.666139, acc.: 58.20%] [G loss: 1.383028]\n",
      "220 [D loss: 0.684637, acc.: 53.52%] [G loss: 1.290996]\n",
      "221 [D loss: 0.685417, acc.: 60.55%] [G loss: 1.350920]\n",
      "222 [D loss: 0.674821, acc.: 58.59%] [G loss: 1.394354]\n",
      "223 [D loss: 0.707770, acc.: 53.91%] [G loss: 1.286284]\n",
      "224 [D loss: 0.690355, acc.: 58.20%] [G loss: 1.418567]\n",
      "225 [D loss: 0.684502, acc.: 57.42%] [G loss: 1.463804]\n",
      "226 [D loss: 0.682057, acc.: 58.20%] [G loss: 1.354551]\n",
      "227 [D loss: 0.681084, acc.: 60.55%] [G loss: 1.368485]\n",
      "228 [D loss: 0.671971, acc.: 60.16%] [G loss: 1.432401]\n",
      "229 [D loss: 0.725972, acc.: 55.08%] [G loss: 1.317760]\n",
      "230 [D loss: 0.694896, acc.: 57.42%] [G loss: 1.425504]\n",
      "231 [D loss: 0.736185, acc.: 51.56%] [G loss: 1.302320]\n",
      "232 [D loss: 0.675200, acc.: 58.59%] [G loss: 1.346731]\n",
      "233 [D loss: 0.715849, acc.: 53.52%] [G loss: 1.254100]\n",
      "234 [D loss: 0.706713, acc.: 55.08%] [G loss: 1.300484]\n",
      "235 [D loss: 0.698058, acc.: 53.52%] [G loss: 1.304969]\n",
      "236 [D loss: 0.680683, acc.: 56.25%] [G loss: 1.353535]\n",
      "237 [D loss: 0.690634, acc.: 60.55%] [G loss: 1.278403]\n",
      "238 [D loss: 0.694878, acc.: 58.59%] [G loss: 1.300798]\n",
      "239 [D loss: 0.733340, acc.: 53.52%] [G loss: 1.313725]\n",
      "240 [D loss: 0.675352, acc.: 59.38%] [G loss: 1.302174]\n",
      "241 [D loss: 0.663979, acc.: 57.42%] [G loss: 1.301541]\n",
      "242 [D loss: 0.686015, acc.: 61.33%] [G loss: 1.363549]\n",
      "243 [D loss: 0.702009, acc.: 60.55%] [G loss: 1.341483]\n",
      "244 [D loss: 0.714696, acc.: 57.03%] [G loss: 1.293505]\n",
      "245 [D loss: 0.696192, acc.: 60.16%] [G loss: 1.223945]\n",
      "246 [D loss: 0.716769, acc.: 52.34%] [G loss: 1.267887]\n",
      "247 [D loss: 0.689825, acc.: 57.81%] [G loss: 1.358933]\n",
      "248 [D loss: 0.705333, acc.: 58.59%] [G loss: 1.312228]\n",
      "249 [D loss: 0.756301, acc.: 48.44%] [G loss: 1.349443]\n",
      "250 [D loss: 0.691799, acc.: 57.81%] [G loss: 1.342570]\n",
      "251 [D loss: 0.765968, acc.: 48.83%] [G loss: 1.273484]\n",
      "252 [D loss: 0.741796, acc.: 52.73%] [G loss: 1.262570]\n",
      "253 [D loss: 0.729319, acc.: 54.30%] [G loss: 1.337862]\n",
      "254 [D loss: 0.730258, acc.: 55.47%] [G loss: 1.216157]\n",
      "255 [D loss: 0.765804, acc.: 48.83%] [G loss: 1.252384]\n",
      "256 [D loss: 0.695576, acc.: 60.94%] [G loss: 1.340537]\n",
      "257 [D loss: 0.705751, acc.: 56.64%] [G loss: 1.261777]\n",
      "258 [D loss: 0.756672, acc.: 48.83%] [G loss: 1.264640]\n",
      "259 [D loss: 0.737833, acc.: 51.17%] [G loss: 1.267266]\n",
      "260 [D loss: 0.714352, acc.: 53.91%] [G loss: 1.345965]\n",
      "261 [D loss: 0.774119, acc.: 49.61%] [G loss: 1.285214]\n",
      "262 [D loss: 0.742041, acc.: 48.83%] [G loss: 1.291274]\n",
      "263 [D loss: 0.711352, acc.: 55.86%] [G loss: 1.312194]\n",
      "264 [D loss: 0.707666, acc.: 51.95%] [G loss: 1.278531]\n",
      "265 [D loss: 0.722975, acc.: 56.64%] [G loss: 1.206390]\n",
      "266 [D loss: 0.749650, acc.: 53.12%] [G loss: 1.198070]\n",
      "267 [D loss: 0.731292, acc.: 52.73%] [G loss: 1.229114]\n",
      "268 [D loss: 0.721617, acc.: 57.03%] [G loss: 1.203962]\n",
      "269 [D loss: 0.704911, acc.: 58.59%] [G loss: 1.239138]\n",
      "270 [D loss: 0.698638, acc.: 58.20%] [G loss: 1.256360]\n",
      "271 [D loss: 0.698476, acc.: 56.64%] [G loss: 1.199791]\n",
      "272 [D loss: 0.712201, acc.: 55.08%] [G loss: 1.223620]\n",
      "273 [D loss: 0.704633, acc.: 54.30%] [G loss: 1.239257]\n",
      "274 [D loss: 0.713791, acc.: 51.56%] [G loss: 1.205978]\n",
      "275 [D loss: 0.705787, acc.: 54.30%] [G loss: 1.178894]\n",
      "276 [D loss: 0.735647, acc.: 54.30%] [G loss: 1.281548]\n",
      "277 [D loss: 0.716102, acc.: 54.69%] [G loss: 1.217547]\n",
      "278 [D loss: 0.742964, acc.: 51.95%] [G loss: 1.224606]\n",
      "279 [D loss: 0.719804, acc.: 54.69%] [G loss: 1.206732]\n",
      "280 [D loss: 0.722105, acc.: 53.91%] [G loss: 1.221757]\n",
      "281 [D loss: 0.741259, acc.: 53.91%] [G loss: 1.274433]\n",
      "282 [D loss: 0.749094, acc.: 50.00%] [G loss: 1.239448]\n",
      "283 [D loss: 0.740576, acc.: 51.17%] [G loss: 1.206804]\n",
      "284 [D loss: 0.735786, acc.: 50.00%] [G loss: 1.203589]\n",
      "285 [D loss: 0.755205, acc.: 48.83%] [G loss: 1.187875]\n",
      "286 [D loss: 0.727397, acc.: 50.00%] [G loss: 1.165028]\n",
      "287 [D loss: 0.714666, acc.: 56.25%] [G loss: 1.110920]\n",
      "288 [D loss: 0.769037, acc.: 51.56%] [G loss: 1.147009]\n",
      "289 [D loss: 0.700102, acc.: 53.52%] [G loss: 1.147341]\n",
      "290 [D loss: 0.734103, acc.: 50.00%] [G loss: 1.183539]\n",
      "291 [D loss: 0.701582, acc.: 54.69%] [G loss: 1.184869]\n",
      "292 [D loss: 0.736156, acc.: 51.56%] [G loss: 1.159765]\n",
      "293 [D loss: 0.760550, acc.: 46.88%] [G loss: 1.206734]\n",
      "294 [D loss: 0.744411, acc.: 48.83%] [G loss: 1.181905]\n",
      "295 [D loss: 0.753130, acc.: 46.09%] [G loss: 1.149284]\n",
      "296 [D loss: 0.706276, acc.: 57.03%] [G loss: 1.135526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 [D loss: 0.762861, acc.: 46.09%] [G loss: 1.100984]\n",
      "298 [D loss: 0.738942, acc.: 50.78%] [G loss: 1.150349]\n",
      "299 [D loss: 0.731233, acc.: 53.91%] [G loss: 1.123384]\n",
      "300 [D loss: 0.749451, acc.: 50.78%] [G loss: 1.124364]\n",
      "301 [D loss: 0.715565, acc.: 49.61%] [G loss: 1.123026]\n",
      "302 [D loss: 0.714660, acc.: 55.86%] [G loss: 1.100856]\n",
      "303 [D loss: 0.712630, acc.: 55.86%] [G loss: 1.157034]\n",
      "304 [D loss: 0.705230, acc.: 53.91%] [G loss: 1.175176]\n",
      "305 [D loss: 0.713973, acc.: 54.30%] [G loss: 1.181544]\n",
      "306 [D loss: 0.695547, acc.: 54.69%] [G loss: 1.196556]\n",
      "307 [D loss: 0.697733, acc.: 55.47%] [G loss: 1.201265]\n",
      "308 [D loss: 0.727984, acc.: 48.83%] [G loss: 1.140935]\n",
      "309 [D loss: 0.735326, acc.: 47.66%] [G loss: 1.197616]\n",
      "310 [D loss: 0.739246, acc.: 53.12%] [G loss: 1.158846]\n",
      "311 [D loss: 0.736875, acc.: 46.88%] [G loss: 1.106818]\n",
      "312 [D loss: 0.726989, acc.: 52.34%] [G loss: 1.130731]\n",
      "313 [D loss: 0.658519, acc.: 62.89%] [G loss: 1.126569]\n",
      "314 [D loss: 0.700001, acc.: 55.08%] [G loss: 1.129487]\n",
      "315 [D loss: 0.707777, acc.: 53.52%] [G loss: 1.129063]\n",
      "316 [D loss: 0.707025, acc.: 54.69%] [G loss: 1.116289]\n",
      "317 [D loss: 0.712572, acc.: 49.61%] [G loss: 1.142870]\n",
      "318 [D loss: 0.693384, acc.: 55.47%] [G loss: 1.170586]\n",
      "319 [D loss: 0.715403, acc.: 52.34%] [G loss: 1.169605]\n",
      "320 [D loss: 0.690141, acc.: 56.25%] [G loss: 1.180986]\n",
      "321 [D loss: 0.764136, acc.: 44.14%] [G loss: 1.237795]\n",
      "322 [D loss: 0.699542, acc.: 53.12%] [G loss: 1.215084]\n",
      "323 [D loss: 0.719586, acc.: 55.86%] [G loss: 1.133322]\n",
      "324 [D loss: 0.725695, acc.: 49.22%] [G loss: 1.220408]\n",
      "325 [D loss: 0.729374, acc.: 50.78%] [G loss: 1.178632]\n",
      "326 [D loss: 0.732146, acc.: 52.73%] [G loss: 1.164549]\n",
      "327 [D loss: 0.720615, acc.: 53.91%] [G loss: 1.123505]\n",
      "328 [D loss: 0.724158, acc.: 50.78%] [G loss: 1.182656]\n",
      "329 [D loss: 0.721222, acc.: 49.61%] [G loss: 1.180718]\n",
      "330 [D loss: 0.738351, acc.: 44.92%] [G loss: 1.146911]\n",
      "331 [D loss: 0.712407, acc.: 46.09%] [G loss: 1.173395]\n",
      "332 [D loss: 0.720690, acc.: 53.12%] [G loss: 1.192014]\n",
      "333 [D loss: 0.697676, acc.: 52.73%] [G loss: 1.129746]\n",
      "334 [D loss: 0.711913, acc.: 51.17%] [G loss: 1.101416]\n",
      "335 [D loss: 0.721374, acc.: 48.44%] [G loss: 1.156732]\n",
      "336 [D loss: 0.708987, acc.: 53.91%] [G loss: 1.180451]\n",
      "337 [D loss: 0.711843, acc.: 55.47%] [G loss: 1.151518]\n",
      "338 [D loss: 0.731393, acc.: 48.83%] [G loss: 1.086900]\n",
      "339 [D loss: 0.700369, acc.: 49.61%] [G loss: 1.135794]\n",
      "340 [D loss: 0.731180, acc.: 47.66%] [G loss: 1.123720]\n",
      "341 [D loss: 0.728388, acc.: 49.22%] [G loss: 1.127280]\n",
      "342 [D loss: 0.708388, acc.: 49.22%] [G loss: 1.131988]\n",
      "343 [D loss: 0.728593, acc.: 51.56%] [G loss: 1.131495]\n",
      "344 [D loss: 0.706880, acc.: 49.61%] [G loss: 1.187874]\n",
      "345 [D loss: 0.680631, acc.: 56.64%] [G loss: 1.120810]\n",
      "346 [D loss: 0.725751, acc.: 49.61%] [G loss: 1.138269]\n",
      "347 [D loss: 0.743309, acc.: 49.22%] [G loss: 1.103691]\n",
      "348 [D loss: 0.722475, acc.: 52.34%] [G loss: 1.108805]\n",
      "349 [D loss: 0.699571, acc.: 53.52%] [G loss: 1.122501]\n",
      "350 [D loss: 0.661658, acc.: 57.42%] [G loss: 1.122716]\n",
      "351 [D loss: 0.712399, acc.: 52.34%] [G loss: 1.174638]\n",
      "352 [D loss: 0.724054, acc.: 53.91%] [G loss: 1.099425]\n",
      "353 [D loss: 0.703200, acc.: 54.30%] [G loss: 1.132695]\n",
      "354 [D loss: 0.706356, acc.: 49.61%] [G loss: 1.169862]\n",
      "355 [D loss: 0.740517, acc.: 52.34%] [G loss: 1.090280]\n",
      "356 [D loss: 0.691327, acc.: 51.95%] [G loss: 1.116561]\n",
      "357 [D loss: 0.696503, acc.: 56.64%] [G loss: 1.139293]\n",
      "358 [D loss: 0.701764, acc.: 53.52%] [G loss: 1.111254]\n",
      "359 [D loss: 0.705170, acc.: 53.12%] [G loss: 1.077567]\n",
      "360 [D loss: 0.690336, acc.: 57.03%] [G loss: 1.055671]\n",
      "361 [D loss: 0.680260, acc.: 55.86%] [G loss: 1.056770]\n",
      "362 [D loss: 0.714716, acc.: 54.69%] [G loss: 1.100735]\n",
      "363 [D loss: 0.720981, acc.: 53.52%] [G loss: 1.130775]\n",
      "364 [D loss: 0.713759, acc.: 49.61%] [G loss: 1.146196]\n",
      "365 [D loss: 0.734320, acc.: 49.61%] [G loss: 1.080734]\n",
      "366 [D loss: 0.697794, acc.: 57.03%] [G loss: 1.090593]\n",
      "367 [D loss: 0.723473, acc.: 50.00%] [G loss: 1.155298]\n",
      "368 [D loss: 0.719711, acc.: 55.08%] [G loss: 1.110330]\n",
      "369 [D loss: 0.710520, acc.: 49.61%] [G loss: 1.082053]\n",
      "370 [D loss: 0.725462, acc.: 54.30%] [G loss: 1.051941]\n",
      "371 [D loss: 0.707970, acc.: 47.66%] [G loss: 1.042897]\n",
      "372 [D loss: 0.698511, acc.: 50.00%] [G loss: 1.056344]\n",
      "373 [D loss: 0.727995, acc.: 51.17%] [G loss: 1.064511]\n",
      "374 [D loss: 0.705481, acc.: 52.34%] [G loss: 1.054510]\n",
      "375 [D loss: 0.722949, acc.: 51.17%] [G loss: 1.090386]\n",
      "376 [D loss: 0.690778, acc.: 57.42%] [G loss: 1.070372]\n",
      "377 [D loss: 0.699130, acc.: 53.91%] [G loss: 1.076772]\n",
      "378 [D loss: 0.721909, acc.: 49.22%] [G loss: 1.070618]\n",
      "379 [D loss: 0.691011, acc.: 53.52%] [G loss: 1.084601]\n",
      "380 [D loss: 0.713130, acc.: 52.34%] [G loss: 1.055809]\n",
      "381 [D loss: 0.671314, acc.: 59.77%] [G loss: 1.070822]\n",
      "382 [D loss: 0.724070, acc.: 46.88%] [G loss: 1.043855]\n",
      "383 [D loss: 0.693307, acc.: 54.69%] [G loss: 1.054951]\n",
      "384 [D loss: 0.703198, acc.: 55.08%] [G loss: 1.029029]\n",
      "385 [D loss: 0.706748, acc.: 51.17%] [G loss: 1.081073]\n",
      "386 [D loss: 0.721135, acc.: 50.39%] [G loss: 1.107864]\n",
      "387 [D loss: 0.701709, acc.: 51.17%] [G loss: 1.052430]\n",
      "388 [D loss: 0.690259, acc.: 54.30%] [G loss: 1.084417]\n",
      "389 [D loss: 0.722574, acc.: 50.39%] [G loss: 1.111497]\n",
      "390 [D loss: 0.705756, acc.: 51.95%] [G loss: 1.110925]\n",
      "391 [D loss: 0.724311, acc.: 48.05%] [G loss: 1.090990]\n",
      "392 [D loss: 0.698541, acc.: 55.47%] [G loss: 1.087536]\n",
      "393 [D loss: 0.725485, acc.: 45.31%] [G loss: 1.090122]\n",
      "394 [D loss: 0.717851, acc.: 47.66%] [G loss: 1.043671]\n",
      "395 [D loss: 0.668116, acc.: 59.77%] [G loss: 1.070101]\n",
      "396 [D loss: 0.677980, acc.: 54.30%] [G loss: 1.053779]\n",
      "397 [D loss: 0.724238, acc.: 52.34%] [G loss: 1.055746]\n",
      "398 [D loss: 0.693045, acc.: 56.64%] [G loss: 1.083349]\n",
      "399 [D loss: 0.678082, acc.: 56.64%] [G loss: 1.085083]\n",
      "400 [D loss: 0.688614, acc.: 54.69%] [G loss: 1.035965]\n",
      "401 [D loss: 0.676040, acc.: 57.42%] [G loss: 1.062722]\n",
      "402 [D loss: 0.702960, acc.: 51.56%] [G loss: 1.082546]\n",
      "403 [D loss: 0.699850, acc.: 57.03%] [G loss: 1.122903]\n",
      "404 [D loss: 0.706699, acc.: 52.34%] [G loss: 1.041353]\n",
      "405 [D loss: 0.717693, acc.: 55.47%] [G loss: 1.046154]\n",
      "406 [D loss: 0.685002, acc.: 58.59%] [G loss: 1.061792]\n",
      "407 [D loss: 0.701655, acc.: 50.78%] [G loss: 1.067576]\n",
      "408 [D loss: 0.693062, acc.: 53.52%] [G loss: 1.043057]\n",
      "409 [D loss: 0.683902, acc.: 56.25%] [G loss: 1.093104]\n",
      "410 [D loss: 0.691572, acc.: 51.56%] [G loss: 1.102704]\n",
      "411 [D loss: 0.718791, acc.: 49.22%] [G loss: 1.053289]\n",
      "412 [D loss: 0.664348, acc.: 58.98%] [G loss: 1.054956]\n",
      "413 [D loss: 0.703327, acc.: 50.78%] [G loss: 1.092069]\n",
      "414 [D loss: 0.715091, acc.: 51.17%] [G loss: 1.061969]\n",
      "415 [D loss: 0.703303, acc.: 51.17%] [G loss: 1.044351]\n",
      "416 [D loss: 0.671190, acc.: 57.03%] [G loss: 1.126202]\n",
      "417 [D loss: 0.689289, acc.: 55.47%] [G loss: 1.117929]\n",
      "418 [D loss: 0.703045, acc.: 55.47%] [G loss: 1.046795]\n",
      "419 [D loss: 0.689269, acc.: 51.56%] [G loss: 1.062325]\n",
      "420 [D loss: 0.664044, acc.: 57.42%] [G loss: 1.085509]\n",
      "421 [D loss: 0.672227, acc.: 55.86%] [G loss: 1.056648]\n",
      "422 [D loss: 0.696276, acc.: 53.12%] [G loss: 1.085113]\n",
      "423 [D loss: 0.669487, acc.: 59.38%] [G loss: 1.141166]\n",
      "424 [D loss: 0.676898, acc.: 57.42%] [G loss: 1.052042]\n",
      "425 [D loss: 0.663377, acc.: 60.55%] [G loss: 1.046151]\n",
      "426 [D loss: 0.648917, acc.: 59.77%] [G loss: 1.099791]\n",
      "427 [D loss: 0.697889, acc.: 53.12%] [G loss: 1.040573]\n",
      "428 [D loss: 0.668564, acc.: 59.77%] [G loss: 1.054519]\n",
      "429 [D loss: 0.700505, acc.: 53.52%] [G loss: 1.066496]\n",
      "430 [D loss: 0.689383, acc.: 52.34%] [G loss: 1.047022]\n",
      "431 [D loss: 0.674703, acc.: 57.81%] [G loss: 1.064403]\n",
      "432 [D loss: 0.661690, acc.: 61.72%] [G loss: 1.028740]\n",
      "433 [D loss: 0.674370, acc.: 57.42%] [G loss: 1.041191]\n",
      "434 [D loss: 0.659122, acc.: 58.20%] [G loss: 1.053311]\n",
      "435 [D loss: 0.673613, acc.: 57.42%] [G loss: 1.012135]\n",
      "436 [D loss: 0.669418, acc.: 56.25%] [G loss: 1.026706]\n",
      "437 [D loss: 0.674295, acc.: 55.86%] [G loss: 1.043030]\n",
      "438 [D loss: 0.678404, acc.: 58.20%] [G loss: 1.106916]\n",
      "439 [D loss: 0.698611, acc.: 56.25%] [G loss: 1.022220]\n",
      "440 [D loss: 0.680114, acc.: 54.69%] [G loss: 1.069340]\n",
      "441 [D loss: 0.686886, acc.: 56.64%] [G loss: 1.056666]\n",
      "442 [D loss: 0.653038, acc.: 62.50%] [G loss: 1.080685]\n",
      "443 [D loss: 0.676836, acc.: 55.86%] [G loss: 1.066712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 [D loss: 0.682482, acc.: 58.59%] [G loss: 1.096239]\n",
      "445 [D loss: 0.677552, acc.: 55.08%] [G loss: 1.084449]\n",
      "446 [D loss: 0.691605, acc.: 55.08%] [G loss: 1.116174]\n",
      "447 [D loss: 0.691627, acc.: 56.64%] [G loss: 1.093160]\n",
      "448 [D loss: 0.688444, acc.: 53.12%] [G loss: 1.100723]\n",
      "449 [D loss: 0.662089, acc.: 59.77%] [G loss: 1.020172]\n",
      "450 [D loss: 0.692407, acc.: 52.34%] [G loss: 1.041900]\n",
      "451 [D loss: 0.681752, acc.: 56.64%] [G loss: 1.053547]\n",
      "452 [D loss: 0.694741, acc.: 50.78%] [G loss: 1.075951]\n",
      "453 [D loss: 0.696651, acc.: 51.56%] [G loss: 1.091083]\n",
      "454 [D loss: 0.711942, acc.: 53.52%] [G loss: 1.069325]\n",
      "455 [D loss: 0.693960, acc.: 52.73%] [G loss: 1.072907]\n",
      "456 [D loss: 0.688140, acc.: 55.86%] [G loss: 1.122003]\n",
      "457 [D loss: 0.703298, acc.: 51.17%] [G loss: 1.054364]\n",
      "458 [D loss: 0.693801, acc.: 50.39%] [G loss: 1.057025]\n",
      "459 [D loss: 0.687802, acc.: 55.08%] [G loss: 1.076481]\n",
      "460 [D loss: 0.711338, acc.: 48.83%] [G loss: 1.088633]\n",
      "461 [D loss: 0.678004, acc.: 55.47%] [G loss: 1.058717]\n",
      "462 [D loss: 0.691339, acc.: 53.12%] [G loss: 1.034492]\n",
      "463 [D loss: 0.692771, acc.: 55.86%] [G loss: 1.087903]\n",
      "464 [D loss: 0.706983, acc.: 55.47%] [G loss: 1.086120]\n",
      "465 [D loss: 0.715564, acc.: 46.88%] [G loss: 1.039981]\n",
      "466 [D loss: 0.678061, acc.: 54.69%] [G loss: 1.106467]\n",
      "467 [D loss: 0.703618, acc.: 46.88%] [G loss: 1.062007]\n",
      "468 [D loss: 0.715619, acc.: 49.61%] [G loss: 1.066198]\n",
      "469 [D loss: 0.688132, acc.: 51.95%] [G loss: 1.121350]\n",
      "470 [D loss: 0.716284, acc.: 46.09%] [G loss: 1.074703]\n",
      "471 [D loss: 0.686643, acc.: 56.25%] [G loss: 1.106181]\n",
      "472 [D loss: 0.713599, acc.: 48.05%] [G loss: 1.090803]\n",
      "473 [D loss: 0.720827, acc.: 48.44%] [G loss: 1.049185]\n",
      "474 [D loss: 0.713984, acc.: 49.61%] [G loss: 1.095078]\n",
      "475 [D loss: 0.704052, acc.: 51.17%] [G loss: 1.055750]\n",
      "476 [D loss: 0.712194, acc.: 52.73%] [G loss: 1.072345]\n",
      "477 [D loss: 0.701839, acc.: 50.78%] [G loss: 1.054521]\n",
      "478 [D loss: 0.708495, acc.: 53.12%] [G loss: 1.061213]\n",
      "479 [D loss: 0.700231, acc.: 50.00%] [G loss: 1.057850]\n",
      "480 [D loss: 0.705743, acc.: 50.78%] [G loss: 1.049281]\n",
      "481 [D loss: 0.695263, acc.: 50.39%] [G loss: 1.056993]\n",
      "482 [D loss: 0.683554, acc.: 57.42%] [G loss: 1.102551]\n",
      "483 [D loss: 0.692439, acc.: 50.00%] [G loss: 1.086457]\n",
      "484 [D loss: 0.718177, acc.: 51.17%] [G loss: 1.056175]\n",
      "485 [D loss: 0.694778, acc.: 55.86%] [G loss: 1.034528]\n",
      "486 [D loss: 0.708007, acc.: 47.27%] [G loss: 1.049877]\n",
      "487 [D loss: 0.663404, acc.: 59.77%] [G loss: 1.032489]\n",
      "488 [D loss: 0.733009, acc.: 46.48%] [G loss: 1.051543]\n",
      "489 [D loss: 0.685979, acc.: 53.91%] [G loss: 1.068024]\n",
      "490 [D loss: 0.665345, acc.: 58.20%] [G loss: 1.128923]\n",
      "491 [D loss: 0.688437, acc.: 53.52%] [G loss: 1.130316]\n",
      "492 [D loss: 0.677943, acc.: 60.94%] [G loss: 1.072314]\n",
      "493 [D loss: 0.718013, acc.: 56.25%] [G loss: 1.074715]\n",
      "494 [D loss: 0.646545, acc.: 61.33%] [G loss: 1.108900]\n",
      "495 [D loss: 0.713404, acc.: 53.91%] [G loss: 1.067342]\n",
      "496 [D loss: 0.680993, acc.: 53.12%] [G loss: 1.137640]\n",
      "497 [D loss: 0.696288, acc.: 55.08%] [G loss: 1.069371]\n",
      "498 [D loss: 0.682462, acc.: 55.08%] [G loss: 1.132341]\n",
      "499 [D loss: 0.690382, acc.: 51.95%] [G loss: 1.114472]\n",
      "500 [D loss: 0.699371, acc.: 56.64%] [G loss: 1.166816]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=500, batch_size=128, sample_interval=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_ml",
   "language": "python",
   "name": "py_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
